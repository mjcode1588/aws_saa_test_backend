[
  {
    "Question_Number": "Q1",
    "Question_Description": "한 회사가 여러 대륙의 도시들에서 온도, 습도, 대기압 데이터를 수집하고 있습니다. 각 사이트에서 매일 수집하는 평균 데이터 볼륨은 500GB입니다. 각 사이트는 고속 인터넷 연결을 보유하고 있습니다. 회사는 이러한 모든 글로벌 사이트의 데이터를 단일 Amazon S3 버킷에 가능한 한 빠르게 집계하려고 합니다. 솔루션은 운영 복잡성을 최소화해야 합니다. 이러한 요구사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/84973-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 전 세계 지점에서 발생하는 대규모 데이터를 단일 Amazon S3 버킷으로 빠르고 간편하게 업로드하는 방법을 묻고 있습니다. S3 Transfer Acceleration을 사용하면 네트워크 지연을 줄이고, Multipart Upload로 대용량 데이터를 병렬 전송하여 업로드 속도를 높이는 최적의 해법을 구현할 수 있습니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.1",
      "3.4"
    ],
    "Keywords": [
      "글로벌 사이트",
      "500GB",
      "고속 인터넷",
      "운영 복잡성",
      "Amazon S3",
      "S3 Transfer Acceleration",
      "Multipart Upload"
    ],
    "Terms": [
      "S3 Transfer Acceleration",
      "Multipart Upload",
      "S3 Cross-Region Replication",
      "AWS Snowball Edge",
      "Amazon EBS",
      "EBS Snapshot"
    ],
    "SelectA": "대상 S3 버킷에서 S3 Transfer Acceleration을 활성화하고, Multipart Upload를 사용해 사이트 데이터를 직접 업로드합니다.",
    "SelectA_Commentary": "S3 Transfer Acceleration은 글로벌 Edge Location을 통해 빠른 업로드를 가능하게 하며, Multipart Upload와 결합하면 대용량 파일도 병렬로 효과적으로 업로드할 수 있어 요구사항을 가장 잘 충족합니다.",
    "SelectB": "가장 가까운 리전의 S3 버킷에 데이터를 업로드하고, S3 Cross-Region Replication으로 대상 S3 버킷에 복제한 후 원본 버킷에서 데이터를 제거합니다.",
    "SelectB_Commentary": "중간 S3 버킷을 활용한 복제 프로세스는 추가 단계가 많아 운영이 복잡해지며, 복제 지연이 발생해 데이터를 즉시 집계하기 어려워집니다.",
    "SelectC": "AWS Snowball Edge Storage Optimized 디바이스를 매일 예약하여 각 사이트에서 가장 가까운 리전으로 데이터를 전송하고, S3 Cross-Region Replication으로 대상 버킷에 복제합니다.",
    "SelectC_Commentary": "Snowball Edge는 물리적 장비 운송이 필요해 시간이 오래 걸리며 이미 고속 인터넷이 있는 환경에서는 오버엔지니어링으로 운영 복잡성이 커집니다.",
    "SelectD": "가장 가까운 리전의 Amazon EC2 인스턴스에 데이터를 업로드하고, Amazon EBS 볼륨에 저장합니다. 정기적으로 EBS 스냅샷을 생성해 대상 S3 버킷이 있는 리전으로 복사하고 필요 시 EBS 볼륨을 복원합니다.",
    "SelectD_Commentary": "EC2, EBS, 스냅샷 복사 등 단계가 많아 복잡하며, 바로 S3에 업로드하는 것보다 지연이 늘어나고 관리 부담이 커집니다."
  },
  {
    "Question_Number": "Q2",
    "Question_Description": "한 회사는 자체 애플리케이션의 로그 파일을 분석해야 합니다. 로그는 JSON 형식으로 Amazon S3 버킷에 저장되어 있습니다. 쿼리는 간단하며 필요할 때마다 실행될 예정입니다. 솔루션스 아키텍트는 기존 아키텍처에 최소한의 변경으로 분석을 수행해야 합니다. 가장 적은 운영 오버헤드로 이러한 요구 사항을 충족하려면 어떻게 해야 합니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/84848-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 Amazon S3에 저장된 JSON 로그 데이터를 최소한의 변경으로 즉시 분석해야 하는 시나리오입니다. Athena를 사용하면 추가 인프라 구성 없이 S3에 직접 쿼리를 실행할 수 있고, 서버리스 방식으로 운영 오버헤드가 매우 적습니다. 따라서 각 선택지 중 가장 간단하고 효율적인 해법을 제공하는 C가 정답입니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.3",
      "3.5"
    ],
    "Keywords": [
      "Amazon S3",
      "JSON 로그",
      "분석",
      "운영 오버헤드 최소화",
      "온디맨드 쿼리"
    ],
    "Terms": [
      "Amazon Redshift",
      "Amazon CloudWatch Logs",
      "Amazon Athena",
      "AWS Glue",
      "Apache Spark",
      "Amazon EMR",
      "SQL 쿼리",
      "JSON"
    ],
    "SelectA": "Amazon Redshift를 사용해 모든 데이터를 한 곳으로 로드하고, 필요할 때 SQL 쿼리를 실행합니다.",
    "SelectA_Commentary": "Redshift 클러스터 구성, 관리, 로드 작업이 필요해 운영 오버헤드가 높고 초기 설정이 복잡합니다.",
    "SelectB": "Amazon CloudWatch Logs에 로그를 저장하고, 콘솔에서 SQL 쿼리를 필요할 때 실행합니다.",
    "SelectB_Commentary": "CloudWatch Logs는 로그 수집 및 모니터링에 적합하지만, S3에 이미 저장된 JSON을 직접 분석하기엔 적합하지 않습니다.",
    "SelectC": "Amazon Athena를 사용해 Amazon S3에 직접 쿼리를 실행합니다.",
    "SelectC_Commentary": "서버리스 기반으로, 기존 데이터가 저장된 S3에 대해 바로 SQL 쿼리를 수행할 수 있어 설정과 운영이 간단하며, 필요할 때만 비용이 발생하는 가장 효율적인 방법입니다.",
    "SelectD": "AWS Glue로 로그를 카탈로그하고, Amazon EMR의 일시적 Apache Spark 클러스터로 필요 시 SQL 쿼리를 실행합니다.",
    "SelectD_Commentary": "EMR 클러스터를 설정하고 Glue 카탈로그와 연동하는 과정이 필요하며, Athena보다 운영 부담과 비용이 높습니다."
  },
  {
    "Question_Number": "Q3",
    "Question_Description": "한 회사가 여러 부서별로 다른 AWS 계정을 관리하기 위해 AWS Organizations를 사용하고 있습니다. 관리 계정에는 프로젝트 보고서가 들어 있는 Amazon S3 버킷이 있습니다. 회사는 이 S3 버킷에 오직 조직 내 계정(=AWS Organizations에 속한 계정)의 사용자만 접근할 수 있도록 제한하고자 합니다. 가장 적은 운영 오버헤드를 들이면서 이러한 요구 사항을 만족하는 솔루션은 무엇입니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/84838-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 여러 AWS 계정으로 구성된 조직에서 특정 S3 버킷을 오직 조직 내 계정만 접근하도록 설정하는 방법을 묻습니다. 조직 ID를 활용해 S3 버킷 정책에서 aws:PrincipalOrgID 키를 사용하면, 심플하고 효율적으로 조직 내 모든 계정 사용자에게만 접근을 허용할 수 있습니다. 다른 옵션들은 조직 구조 변경, CloudTrail 이벤트 모니터링, 사용자 태깅 등 추가 관리 작업이 많아 운영 오버헤드가 증가합니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1",
      "1.3"
    ],
    "Keywords": [
      "S3 버킷 접근 제한",
      "AWS Organizations",
      "aws:PrincipalOrgID",
      "조직 내 계정",
      "운영 오버헤드 최소화"
    ],
    "Terms": [
      "AWS Organizations",
      "Amazon S3",
      "aws:PrincipalOrgID",
      "aws:PrincipalOrgPaths",
      "AWS CloudTrail",
      "aws:PrincipalTag"
    ],
    "SelectA": "S3 버킷 정책에 aws:PrincipalOrgID 글로벌 컨디션 키를 조직 ID로 참조하도록 추가합니다.",
    "SelectA_Commentary": "조직 내 계정임을 쉽게 검증하므로 유지보수가 최소화되고 접근 제어가 간편합니다. 정답입니다.",
    "SelectB": "각 부서별로 조직 단위(OU)를 만들고, aws:PrincipalOrgPaths 글로벌 컨디션 키를 S3 버킷 정책에 추가합니다.",
    "SelectB_Commentary": "OU를 세분화하고 정책을 관리해야 하므로 추가 설정과 관리 비용이 늘어납니다.",
    "SelectC": "AWS CloudTrail로 CreateAccount, InviteAccountToOrganization, LeaveOrganization, RemoveAccountFromOrganization 이벤트를 모니터링하고 버킷 정책을 그때그때 업데이트합니다.",
    "SelectC_Commentary": "계정 변동이 발생할 때마다 직접 정책을 수정해야 하므로 운영이 복잡해집니다.",
    "SelectD": "S3 버킷 접근이 필요한 각 사용자를 태깅하고, aws:PrincipalTag 글로벌 컨디션 키를 S3 버킷 정책에 추가합니다.",
    "SelectD_Commentary": "필요 사용자마다 태그를 꾸준히 관리해야 하므로 계정이 늘어날수록 오버헤드가 커집니다."
  },
  {
    "Question_Number": "Q4",
    "Question_Description": "한 애플리케이션이 VPC 내의 Amazon EC2 인스턴스에서 실행 중입니다. 해당 애플리케이션은 Amazon S3 버킷에 저장된 로그를 처리합니다. EC2 인스턴스는 인터넷 연결 없이 S3 버킷에 접근해야 합니다. Amazon S3에 대한 프라이빗 네트워크 연결을 제공하는 솔루션은 무엇입니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/84980-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 VPC 내 EC2 인스턴스가 인터넷 없이 Amazon S3 버킷에 접근해야 하는 상황입니다. Gateway VPC endpoint를 사용하면 사설 경로를 통해 S3와 안전하게 통신할 수 있으므로 비용 부담도 적고 요구사항을 충족합니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1"
    ],
    "Keywords": [
      "EC2 인스턴스",
      "인터넷 없이",
      "프라이빗 네트워크 연결",
      "Amazon S3",
      "VPC Endpoint"
    ],
    "Terms": [
      "Amazon EC2",
      "VPC",
      "Amazon S3",
      "Gateway VPC endpoint",
      "CloudWatch Logs",
      "Instance profile",
      "Amazon API Gateway"
    ],
    "SelectA": "S3 버킷에 대한 Gateway VPC endpoint를 생성합니다.",
    "SelectA_Commentary": "Gateway VPC endpoint는 S3에 대한 사설 경로를 제공하여 인터넷 연결 없이도 버킷에 안전하게 액세스할 수 있습니다.",
    "SelectB": "Amazon CloudWatch Logs로 로그를 스트리밍한 뒤, 이를 S3 버킷에 내보냅니다.",
    "SelectB_Commentary": "CloudWatch Logs는 로그 집계에 유용하지만, S3와의 직접적인 사설 연결을 제공하지 않아 요구사항을 충족하기 어렵습니다.",
    "SelectC": "Amazon EC2 인스턴스에 Instance profile을 생성하여 S3 접근을 허용합니다.",
    "SelectC_Commentary": "Instance profile은 권한만 부여할 뿐, 인터넷 없이 S3 버킷에 연결할 프라이빗 경로를 제공하지 않습니다.",
    "SelectD": "Amazon API Gateway API를 사용해 S3 엔드포인트에 대한 프라이빗 링크를 만듭니다.",
    "SelectD_Commentary": "API Gateway는 S3에 대한 직접적이고 효율적인 사설 연결 방식이 아니므로 적절한 선택지가 아닙니다."
  },
  {
    "Question_Number": "Q5",
    "Question_Description": "회사는 단일 Amazon EC2 인스턴스를 사용하여 웹 애플리케이션을 AWS에서 호스팅하고 있으며, 사용자 업로드 문서를 Amazon EBS 볼륨에 저장하고 있습니다. 더 나은 확장성과 가용성을 위해 회사는 동일한 아키텍처를 복제하여 두 번째 EC2 인스턴스와 EBS 볼륨을 다른 Availability Zone에 생성하고, 둘 다 Application Load Balancer 뒤에 배치했습니다. 이 변경을 마친 후, 사용자가 웹사이트를 새로고침할 때마다 어느 순간에는 특정 문서 집합만 보이고, 다른 순간에는 다른 문서 집합만 보이지만 동시에 모든 문서를 볼 수는 없다고 보고했습니다. 사용자가 모든 문서를 한꺼번에 볼 수 있도록 하기 위해 솔루션스 아키텍트는 어떤 제안을 해야 합니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/84981-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 다중 AZ 환경에서 여러 EC2 인스턴스가 동일한 파일에 접근해야 할 때 공유 스토리지가 필요한 상황을 묻습니다. Amazon EFS를 사용하면 모든 인스턴스에서 실시간으로 동일한 데이터를 볼 수 있어 문제를 해결할 수 있습니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.2"
    ],
    "Keywords": [
      "다중 AZ",
      "공유 스토리지",
      "Amazon EBS",
      "Amazon EFS",
      "문서 접근",
      "가용성",
      "확장성"
    ],
    "Terms": [
      "Amazon EC2",
      "Amazon EBS",
      "Application Load Balancer",
      "Availability Zone",
      "Amazon EFS"
    ],
    "SelectA": "두 EBS 볼륨 모두 모든 문서를 포함하도록 데이터를 복사합니다.",
    "SelectA_Commentary": "각 볼륨의 데이터를 계속 동기화해야 하므로 운영이 복잡하며, 새 문서가 추가될 때마다 실시간 일관성을 보장하기 어렵습니다.",
    "SelectB": "Application Load Balancer가 문서를 갖고 있는 서버로 사용자를 보내도록 구성합니다.",
    "SelectB_Commentary": "사용자를 문서를 가진 서버로만 연결해도 두 서버의 문서가 각각 다르다면 모든 문서를 동시에 보는 문제는 해결되지 않습니다.",
    "SelectC": "두 EBS 볼륨의 데이터를 Amazon EFS로 복사하고, 애플리케이션이 새 문서를 Amazon EFS에 저장하도록 수정합니다.",
    "SelectC_Commentary": "Amazon EFS는 여러 AZ에서 동시에 접근할 수 있는 공유 파일 시스템이므로, 모든 인스턴스에서 동일한 데이터를 즉시 볼 수 있어 근본적인 문제를 해결하는 최적의 방법입니다.",
    "SelectD": "Application Load Balancer가 요청을 두 서버로 모두 보내도록 구성하고, 각 서버에서 적절한 문서를 반환합니다.",
    "SelectD_Commentary": "두 서버가 가진 문서를 합쳐 보여주려 해도 실시간 동기화 없이 서로 다른 볼륨에 분산된 데이터를 동시에 일관성 있게 제공하기는 어렵습니다."
  },
  {
    "Question_Number": "Q6",
    "Question_Description": "한 회사가 사내 NFS(Network File System)를 사용하여 대용량 동영상 파일을 저장하고 있습니다. 각 동영상 파일은 1MB부터 500GB까지 다양하며, 총 70TB의 스토리지가 있고 더 이상 증가하지 않습니다. 회사는 이 동영상 파일들을 가능한 한 빨리, 그리고 네트워크 대역폭 사용을 최소화하면서 Amazon S3로 마이그레이션하려고 합니다. 이러한 요구사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/84875-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 온프레미스 NFS 스토리지에 보관된 대용량 데이터를 짧은 시간 안에, 그리고 네트워크 사용량을 최소화해서 Amazon S3로 옮기는 방법을 묻습니다. 네트워크 활용도를 고려하면 물리적 장비를 통한 오프라인 전송이 유리하며, Snowball Edge를 통해 대규모 데이터를 효율적으로 마이그레이션할 수 있습니다. 직접 Network 연결 방식인 Direct Connect나 S3 File Gateway를 통한 온라인 전송은 대역폭을 많이 소모하거나 전송 시간이 길어질 수 있으므로, 오프라인 전송이 최적의 선택입니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.1",
      "3.5"
    ],
    "Keywords": [
      "NFS",
      "동영상 파일",
      "70TB",
      "Amazon S3",
      "마이그레이션",
      "네트워크 대역폭 최소화",
      "최대한 빠른 전송",
      "AWS Snowball Edge"
    ],
    "Terms": [
      "NFS",
      "Amazon S3",
      "AWS Snowball Edge",
      "IAM role",
      "AWS CLI",
      "S3 File Gateway",
      "AWS Direct Connect",
      "NFS file share",
      "on-premises"
    ],
    "SelectA": "S3 버킷을 생성합니다. S3 버킷에 쓸 수 있는 IAM role을 생성합니다. AWS CLI를 사용하여 모든 파일을 로컬에서 S3 버킷으로 복사합니다.",
    "SelectA_Commentary": "70TB 규모를 인터넷으로 직접 전송하면 대역폭 사용이 큽니다. 빠른 전송 방식으로 보기 어렵습니다.",
    "SelectB": "AWS Snowball Edge 작업을 생성합니다. 온프레미스에서 Snowball Edge 디바이스를 수령한 후 Snowball Edge 클라이언트를 사용하여 데이터를 디바이스로 옮깁니다. 이후 디바이스를 반환해 AWS에서 Amazon S3로 데이터를 가져오도록 합니다.",
    "SelectB_Commentary": "물리적인 장비를 사용하여 대규모 데이터를 오프라인으로 전송하므로 네트워크 대역폭 사용을 최소화하면서도 빠른 전송이 가능합니다. 정답입니다.",
    "SelectC": "온프레미스에 S3 File Gateway를 배포합니다. 공용 서비스 엔드포인트로 S3 File Gateway에 접속합니다. S3 버킷을 생성한 후, S3 File Gateway에 새 NFS file share를 생성하고 해당 공유를 S3 버킷에 연결합니다. 기존 NFS 파일 공유에서 S3 File Gateway로 데이터를 전송합니다.",
    "SelectC_Commentary": "인터넷을 통한 온라인 전송으로 70TB를 전송하는 데 오랜 시간이 걸리며, 네트워크 대역폭을 크게 소모합니다.",
    "SelectD": "온프레미스 네트워크와 AWS 간에 AWS Direct Connect 연결을 설정합니다. 온프레미스에 S3 File Gateway를 배포하고, public VIF를 생성해 S3 File Gateway와 연결합니다. S3 버킷을 생성한 뒤, S3 File Gateway에 새 NFS file share를 만들고 해당 공유를 S3 버킷에 연결합니다. 기존 NFS 파일 공유에서 S3 File Gateway로 데이터를 전송합니다.",
    "SelectD_Commentary": "Direct Connect를 활용해 전송 속도를 높일 수 있지만, 여전히 온라인 전송으로 70TB 전송 시 대역폭 사용이 상당하며 오프라인보다 시간이 더 오래 걸릴 수 있습니다."
  },
  {
    "Question_Number": "Q7",
    "Question_Description": "한 회사는 들어오는 메시지를 수집하는 애플리케이션을 운영하고 있습니다. 수십 개의 다른 애플리케이션과 마이크로서비스가 이 메시지들을 빠르게 소비합니다. 메시지의 양은 크게 변동하며 때때로 초당 100,000개로 갑자기 증가하기도 합니다. 회사는 솔루션을 느슨하게 결합하고 확장성을 높이기를 원합니다. 이러한 요구사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/84721-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 메시지의 폭발적인 증가에도 빠르게 확장하면서 마이크로서비스들이 동시에 소비할 수 있는 구조, 즉 느슨하게 결합된 아키텍처를 설계하는 방법을 묻습니다. Amazon SNS와 Amazon SQS 조합을 사용하면 게시된 메시지를 여러 큐로 분산하여 처리 가능하며, 높은 확장성을 확보할 수 있습니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1"
    ],
    "Keywords": [
      "메시지",
      "느슨한 결합",
      "확장성",
      "100,000건",
      "SNS",
      "SQS"
    ],
    "Terms": [
      "Amazon Kinesis Data Analytics",
      "Amazon Kinesis Data Streams",
      "Auto Scaling group",
      "Amazon DynamoDB",
      "Amazon SNS",
      "Amazon SQS",
      "AWS Lambda"
    ],
    "SelectA": "Amazon Kinesis Data Analytics에 메시지를 저장한 뒤, 컨슈머 애플리케이션들이 메시지를 읽고 처리하도록 구성합니다.",
    "SelectA_Commentary": "Kinesis Data Analytics는 실시간 분석 용도로 적합하지만, 메시지를 빠르게 분산/수신하는 데는 SNS+SQS 같은 단순 큐 방식이 더 적절합니다.",
    "SelectB": "Amazon EC2 Auto Scaling 그룹에서 애플리케이션을 배포하고 CPU 지표에 따라 EC2 인스턴스 수를 확대/축소합니다.",
    "SelectB_Commentary": "EC2 Auto Scaling만으로는 메시지 처리 로직을 분산하지 못해 느슨한 결합 구조를 확보하기 어렵고, 갑작스러운 트래픽 변화에도 유연성이 제한적입니다.",
    "SelectC": "단일 shard로 설정된 Amazon Kinesis Data Streams에 메시지를 기록하고, AWS Lambda로 전처리하여 Amazon DynamoDB에 저장합니다. 이후 컨슈머 애플리케이션들이 DynamoDB에서 메시지를 읽어 처리하도록 구성합니다.",
    "SelectC_Commentary": "단일 shard는 초당 처리량에 한계가 있어 100,000건 이상의 급증 상황에 대응하기 어렵습니다.",
    "SelectD": "Amazon SNS 토픽에 메시지를 게시하고, 여러 Amazon SQS 구독을 설정합니다. 컨슈머 애플리케이션들은 각 큐로부터 메시지를 받아 처리하도록 구성합니다.",
    "SelectD_Commentary": "SNS+SQS 구조는 높은 확장성과 느슨한 결합을 동시에 달성할 수 있어 급격한 메시지 증가에도 유연하게 대응할 수 있는 최적의 솔루션입니다."
  },
  {
    "Question_Number": "Q8",
    "Question_Description": "한 회사가 분산된 애플리케이션을 AWS로 마이그레이션하려고 합니다. 이 애플리케이션은 가변적인 워크로드를 처리합니다. 레거시 플랫폼은 여러 컴퓨트 노드에 걸쳐 작업을 조정하는 기본 서버로 구성됩니다. 회사는 복원력과 확장성을 최대화하는 솔루션으로 애플리케이션을 현대화하고자 합니다. 어떻게 설계해야 합니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/84679-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 레거시 환경에서 기본 서버가 여러 노드를 관리하던 구조를 AWS 서비스로 현대화해, 변동이 큰 워크로드를 효율적으로 처리하고 복원력을 극대화하는 방법을 묻습니다. Amazon SQS를 통해 작업을 큐에 넣고, 큐 크기에 따라 Auto Scaling 그룹의 EC2 인스턴스를 동적으로 확대·축소하는 방식이 가장 적절한 해결책입니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1",
      "2.2"
    ],
    "Keywords": [
      "분산된 애플리케이션",
      "가변적인 워크로드",
      "기본 서버",
      "여러 컴퓨트 노드",
      "복원력",
      "확장성",
      "Amazon SQS",
      "Amazon EC2",
      "Auto Scaling 그룹",
      "큐 크기 기반 스케일링"
    ],
    "Terms": [
      "Amazon Simple Queue Service (Amazon SQS)",
      "Amazon EC2",
      "Auto Scaling group",
      "EC2 Auto Scaling",
      "AWS CloudTrail",
      "Amazon EventBridge (Amazon CloudWatch Events)"
    ],
    "SelectA": "Amazon SQS 큐를 작업 전송 대상으로 구성합니다. Amazon EC2 인스턴스로 구성된 컴퓨트 노드를 Auto Scaling group으로 관리하고, EC2 Auto Scaling에서 예약 기반 스케일링을 구성합니다.",
    "SelectA_Commentary": "예약 스케일링은 실제 부하와 무관하게 정해진 시점에만 스케일링되어, 가변적인 워크로드를 대응하기엔 유연성이 부족합니다.",
    "SelectB": "Amazon SQS 큐를 작업 전송 대상으로 구성합니다. Amazon EC2 인스턴스로 구성된 컴퓨트 노드를 Auto Scaling group으로 관리하고, EC2 Auto Scaling에서 큐 크기에 따라 스케일링하도록 설정합니다.",
    "SelectB_Commentary": "큐의 길이에 따라 자동으로 인스턴스 수를 조절하는 유연한 아키텍처로, 가변적인 워크로드와 높은 복원성을 모두 만족하는 최적의 솔루션입니다.",
    "SelectC": "기본 서버와 컴퓨트 노드를 모두 Amazon EC2 인스턴스로 구성하여 Auto Scaling group으로 관리합니다. AWS CloudTrail을 작업 전송 대상으로 구성하고, EC2 Auto Scaling에서 기본 서버의 부하를 기준으로 스케일링합니다.",
    "SelectC_Commentary": "CloudTrail은 API 호출 기록 용도로, 작업 대기열로 쓰기에 적합하지 않습니다. 또한 기본 서버와 컴퓨트 노드를 같은 그룹으로 묶으면 계층 분리가 깨져 확장성이 떨어집니다.",
    "SelectD": "기본 서버와 컴퓨트 노드를 모두 Amazon EC2 인스턴스로 구성하여 Auto Scaling group으로 관리합니다. Amazon EventBridge(Amazon CloudWatch Events)를 작업 전송 대상으로 구성하고, EC2 Auto Scaling에서 컴퓨트 노드의 부하를 기준으로 스케일링합니다.",
    "SelectD_Commentary": "EventBridge는 이벤트 라우팅 서비스로, 작업 부하를 처리하기에는 적합하지 않습니다. 기본 서버와 컴퓨트 노드를 분리하지 않아 확장성과 복원성을 모두 극대화하기 어렵습니다."
  },
  {
    "Question_Number": "Q9",
    "Question_Description": "한 회사가 데이터 센터에서 SMB file server를 운영하고 있습니다. 이 file server는 대용량 파일을 저장하며, 생성 후 처음 며칠 동안 자주 액세스됩니다. 7일이 지나면 파일은 거의 액세스되지 않습니다. 전체 데이터 용량이 꾸준히 증가하여 회사의 스토리지 한계에 도달하고 있습니다. 솔루션스 아키텍트는 최근에 액세스된 파일에 대한 저지연 액세스를 유지하면서도 사용 가능한 스토리지를 확장해야 합니다. 또한, future storage issues를 피하기 위해 파일 lifecycle management도 제공해야 합니다. 이러한 요구사항을 만족하는 솔루션은 무엇입니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/84680-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 기존 온프레미스 SMB file server와 연동하면서 자주 액세스되는 파일에 대한 낮은 지연 시간을 유지하고, 오래된 파일을 자동으로 아카이빙해 비용을 절감하는 솔루션을 찾는 것입니다. Amazon S3 File Gateway를 통해 확장 가능하고 저비용의 클라우드 스토리지를 연동한 뒤, Lifecycle policy로 오래된 파일을 S3 Glacier Deep Archive로 옮겨 효율적인 파일 수명 관리를 달성할 수 있습니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.1"
    ],
    "Keywords": [
      "SMB file server",
      "저지연 액세스",
      "lifecycle management",
      "스토리지 용량 확장",
      "S3 Glacier Deep Archive"
    ],
    "Terms": [
      "AWS DataSync",
      "Amazon S3 File Gateway",
      "S3 Lifecycle policy",
      "S3 Glacier Deep Archive",
      "Amazon FSx for Windows File Server",
      "Amazon S3"
    ],
    "SelectA": "AWS DataSync를 사용하여 7일 이상 지난 데이터를 SMB file server에서 AWS로 복사합니다.",
    "SelectA_Commentary": "단순 복사만 제공하므로 파일을 자주 액세스해야 하는 시나리오에 대한 저지연 액세스 보장이 부족하고, Lifecycle policy 연동도 명시되지 않아 요구사항에 부합하지 않습니다.",
    "SelectB": "Amazon S3 File Gateway를 생성하여 회사의 스토리지를 확장합니다. 7일 후 데이터를 S3 Glacier Deep Archive로 전환하는 S3 Lifecycle policy를 만듭니다.",
    "SelectB_Commentary": "SMB file server와 투명하게 연동해 자주 액세스되는 파일에는 로컬 캐싱으로 저지연을 제공하고, 오래된 파일은 자동으로 저비용 스토리지로 옮겨 요구사항을 충족합니다.",
    "SelectC": "Amazon FSx for Windows File Server 파일 시스템을 생성하여 회사의 스토리지를 확장합니다.",
    "SelectC_Commentary": "Windows 기반 파일 서버를 간단히 확장하지만, 오래된 파일의 자동 아카이빙이나 비용 최적화 관리가 부족하므로 적합하지 않습니다.",
    "SelectD": "모든 사용자 컴퓨터에 유틸리티를 설치해 Amazon S3에 접근하게 합니다. 7일 후 데이터를 S3 Glacier Flexible Retrieval로 전환하는 S3 Lifecycle policy를 만듭니다.",
    "SelectD_Commentary": "각 사용자 측에서 별도 프로그램을 사용해야 하고, File Gateway처럼 SMB 프로토콜과 연동되지 않아 저지연 액세스 제공이 번거로우며 운영 복잡성이 큽니다."
  },
  {
    "Question_Number": "Q10",
    "Question_Description": "한 회사가 AWS에서 전자상거래 웹 애플리케이션을 구축하고 있습니다. 애플리케이션은 새 주문 정보를 처리하기 위해 Amazon API Gateway REST API로 보냅니다. 회사는 주문이 도착한 순서대로 처리되도록 보장하고자 합니다. 이 요구사항을 충족할 솔루션은 무엇입니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/84681-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 주문이 들어오는 순서대로 처리해야 하는 시나리오에서 적합한 메커니즘을 찾는 것입니다. Amazon SQS FIFO queue는 메시지의 순서를 보장하므로 요구사항을 충족합니다. API Gateway를 통해 메시지를 FIFO 큐로 보내고, AWS Lambda가 순차적으로 메시지를 처리하도록 설정하면 안정적이고 확장 가능한 구조를 구현할 수 있습니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1"
    ],
    "Keywords": [
      "Amazon API Gateway",
      "REST API",
      "Amazon SQS FIFO queue",
      "AWS Lambda",
      "주문 처리 순서 보장"
    ],
    "Terms": [
      "Amazon API Gateway",
      "REST API",
      "Amazon SNS",
      "AWS Lambda",
      "Amazon SQS FIFO queue",
      "Amazon SQS standard queue",
      "API Gateway authorizer"
    ],
    "SelectA": "애플리케이션에서 주문을 받을 때 Amazon API Gateway 통합을 사용하여 Amazon Simple Notification Service(Amazon SNS) 토픽에 메시지를 게시합니다. AWS Lambda 함수를 토픽 구독자로 설정해 주문을 처리합니다.",
    "SelectA_Commentary": "Amazon SNS는 메시지 브로드캐스트에 적합하며, 순서 보장은 제공하지 않으므로 요구사항과 맞지 않습니다.",
    "SelectB": "애플리케이션에서 주문을 받을 때 Amazon API Gateway 통합을 사용하여 Amazon Simple Queue Service(SQS) FIFO queue에 메시지를 보냅니다. 해당 SQS FIFO queue가 AWS Lambda 함수를 호출하도록 구성해 주문을 처리합니다.",
    "SelectB_Commentary": "FIFO 큐는 메시지 순서를 엄격하게 보장하므로 주문을 처리하는 순서를 유지해야 하는 상황에 최적의 선택입니다.",
    "SelectC": "API Gateway authorizer를 사용하여 애플리케이션이 한 주문을 처리하는 동안 모든 요청을 차단합니다.",
    "SelectC_Commentary": "전체 요청을 차단하는 방식은 순서 보장보다는 진입 자체를 제한하는 방법이며, 운영상 비효율적이고 요구사항을 충족하지 못합니다.",
    "SelectD": "애플리케이션에서 주문을 받을 때 Amazon API Gateway 통합을 사용하여 Amazon Simple Queue Service(SQS) standard queue로 메시지를 보냅니다. 해당 SQS standard queue가 AWS Lambda 함수를 호출하도록 구성해 주문을 처리합니다.",
    "SelectD_Commentary": "SQS standard queue는 높은 처리량을 제공하지만 메시지 순서를 보장하지 않습니다. FIFO 큐와 달리 순서 제어가 불가능합니다."
  },
  {
    "Question_Number": "Q11",
    "Question_Description": "한 회사는 Amazon EC2 인스턴스에서 애플리케이션을 운영하고 있으며, Amazon Aurora 데이터베이스를 사용하고 있습니다. EC2 인스턴스는 로컬 파일에 저장된 사용자 이름과 비밀번호를 이용하여 데이터베이스에 접속합니다. 회사는 자격 증명 관리에 대한 운영 오버헤드를 최소화하고 싶어 합니다. 이 목표를 달성하기 위해 Solutions Architect는 무엇을 해야 합니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/84682-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 로컬 파일에 저장된 데이터베이스 자격 증명을 안전하게 관리하고 자동으로 갱신할 방법을 찾는 보안 설계 문제입니다. AWS Secrets Manager는 자동 자격 증명 로테이션 기능을 제공하여 운영 오버헤드를 크게 줄여주므로 효과적인 솔루션입니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1",
      "1.3"
    ],
    "Keywords": [
      "Amazon EC2",
      "Amazon Aurora",
      "자격 증명 관리",
      "운영 오버헤드 최소화",
      "AWS Secrets Manager",
      "automatic rotation"
    ],
    "Terms": [
      "Amazon EC2",
      "Amazon Aurora",
      "AWS Secrets Manager",
      "AWS Systems Manager Parameter Store",
      "Amazon S3",
      "AWS Key Management Service (AWS KMS)",
      "Amazon Elastic Block Store (Amazon EBS)"
    ],
    "SelectA": "AWS Secrets Manager를 사용하고, automatic rotation을 활성화합니다.",
    "SelectA_Commentary": "AWS Secrets Manager는 관리형 비밀번호 로테이션을 지원하므로 자격 증명을 안전하게 보관하고 자동으로 갱신할 수 있어 운영 오버헤드를 최소화합니다.",
    "SelectB": "AWS Systems Manager Parameter Store를 사용하고, automatic rotation을 활성화합니다.",
    "SelectB_Commentary": "Parameter Store는 기본적으로 자동 로테이션을 제공하지 않으므로, 자체 로직이 필요해 관리 비용이 더 큽니다.",
    "SelectC": "AWS KMS 암호화 키로 암호화된 객체를 저장하는 Amazon S3 버킷을 생성하고, 자격 증명 파일을 마이그레이션하여 애플리케이션이 S3 버킷을 사용하도록 합니다.",
    "SelectC_Commentary": "S3 버킷에 자격 증명을 저장해도 자동 로테이션 기능이 없고, 애플리케이션 호출 방식이 복잡해져 운영 오버헤드를 줄이기 어렵습니다.",
    "SelectD": "각 Amazon EC2 인스턴스에 암호화된 Amazon EBS 볼륨을 생성하고 연결한 뒤, 자격 증명 파일을 옮기고 애플리케이션이 이를 사용하도록 합니다.",
    "SelectD_Commentary": "이 방식은 단순히 저장 매체를 암호화하는 것이므로 자동 자격 증명 갱신 기능이 없어 원하는 운영 간소화를 달성하기 어렵습니다."
  },
  {
    "Question_Number": "Q12",
    "Question_Description": "한 글로벌 회사가 Application Load Balancer(ALB) 뒤의 Amazon EC2 인스턴스에서 웹 애플리케이션을 호스팅하고 있습니다. 웹 애플리케이션은 정적 데이터와 동적 데이터를 모두 사용하며, 정적 데이터는 Amazon S3 버킷에 저장됩니다. 회사는 정적 데이터와 동적 데이터의 성능을 개선하고 지연 시간을 줄이고 싶어 합니다. 또한 회사는 Amazon Route 53에 등록된 자체 도메인 이름을 사용하고 있습니다. 이러한 요구 사항을 충족하기 위한 솔루션은 무엇입니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85010-exam-aws-certified-solut",
    "AnswerDescription": "이 문제에서는 정적 콘텐츠(S3)와 동적 콘텐츠(ALB) 양쪽 모두의 지연 시간을 줄이고 성능을 높이기 위한 최적의 분산 전략을 묻습니다. Amazon CloudFront는 글로벌 엣지를 활용하여 정적·동적 콘텐츠 모두의 전송 속도를 향상시킬 수 있고, Route 53으로 트래픽을 라우팅해 간단히 구성할 수 있습니다. AWS Global Accelerator는 주로 비HTTP 프로토콜, 혹은 정적 IP가 필요한 특정 사례에 더욱 적합합니다. 따라서 S3와 ALB를 동시에 Origin으로 사용하는 CloudFront 배포가 운영 복잡도와 성능 개선 면에서 최적의 해답입니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.1",
      "3.4"
    ],
    "Keywords": [
      "정적 데이터",
      "동적 데이터",
      "지연 시간 감소",
      "AWS Global Accelerator",
      "Amazon CloudFront",
      "Application Load Balancer",
      "Amazon S3"
    ],
    "Terms": [
      "Amazon Route 53",
      "Amazon EC2",
      "Amazon S3",
      "Application Load Balancer",
      "Amazon CloudFront",
      "AWS Global Accelerator"
    ],
    "SelectA": "Amazon CloudFront distribution을 생성하고 S3 버킷과 ALB를 각각 Origin으로 설정합니다. Route 53에서 이 CloudFront distribution으로 트래픽을 라우팅하도록 구성합니다.",
    "SelectA_Commentary": "정적·동적 콘텐츠를 동일한 CloudFront distribution에서 제공함으로써 네트워크 엣지에서 캐싱과 가속을 동시에 수행해 지연 시간을 효과적으로 줄입니다.",
    "SelectB": "ALB를 Origin으로 하는 Amazon CloudFront distribution을 생성합니다. AWS Global Accelerator standard accelerator를 생성하고, S3 버킷을 endpoint로 합니다. 그리고 Route 53에서 CloudFront distribution으로 트래픽을 라우팅합니다.",
    "SelectB_Commentary": "정적 콘텐츠를 Global Accelerator로, 동적 콘텐츠를 CloudFront로 분리하므로 구성 복잡도가 높아집니다. HTTP 환경에서 Global Accelerator를 꼭 써야 할 이유가 부족합니다.",
    "SelectC": "S3 버킷을 Origin으로 하는 Amazon CloudFront distribution을 생성합니다. ALB와 CloudFront distribution을 endpoint로 하는 AWS Global Accelerator standard accelerator를 생성합니다. Accelerator DNS에 연결된 커스텀 도메인을 만들어 웹 애플리케이션 엔드포인트로 사용합니다.",
    "SelectC_Commentary": "Global Accelerator와 CloudFront를 동시에 사용해 이중 구성을 구성하므로 운영 복잡도가 높아집니다. 필요한 요구사항을 초과해 복잡성을 증가시킵니다.",
    "SelectD": "ALB를 Origin으로 하는 Amazon CloudFront distribution을 생성합니다. AWS Global Accelerator standard accelerator를 생성하고 S3 버킷을 endpoint로 합니다. 두 개의 도메인 이름을 만들어 하나는 동적 콘텐츠용 CloudFront, 다른 하나는 정적 콘텐츠용 accelerator DNS에 매핑합니다.",
    "SelectD_Commentary": "정적·동적 콘텐츠를 각기 다른 경로로 분리하여 도메인까지 이원화합니다. 관리가 복잡해지고 CloudFront 단일 사용 시 얻을 수 있는 이점을 놓칩니다."
  },
  {
    "Question_Number": "Q13",
    "Question_Description": "한 회사가 AWS 인프라에 대해 매달 정기 유지 보수를 수행합니다. 이 유지 보수 기간 중, 회사는 여러 AWS Region에 걸쳐 있는 Amazon RDS for MySQL 데이터베이스의 자격 증명을 회전해야 합니다. 가장 적은 운영 오버헤드로 이 요구 사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/84728-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 여러 AWS Region에 분산된 Amazon RDS for MySQL 자격 증명을 매달 회전하는 방법을 묻습니다. 가장 간단하고 자동화된 방식으로 자격 증명을 안전하게 관리해야 하므로, AWS Secrets Manager의 자동 회전 기능을 활용하는 것이 최소의 운영 오버헤드를 제공합니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1",
      "1.3"
    ],
    "Keywords": [
      "정기 유지 보수",
      "Amazon RDS for MySQL",
      "자격 증명 회전",
      "여러 AWS Region",
      "운영 오버헤드 최소화",
      "AWS Secrets Manager"
    ],
    "Terms": [
      "Amazon RDS for MySQL",
      "AWS Secrets Manager",
      "AWS Systems Manager",
      "multi-Region replication",
      "Amazon S3",
      "server-side encryption (SSE)",
      "Amazon EventBridge (Amazon CloudWatch Events)",
      "AWS Lambda",
      "AWS Key Management Service (AWS KMS)",
      "Amazon DynamoDB global table"
    ],
    "SelectA": "AWS Secrets Manager에 자격 증명을 secrets로 저장합니다. 필요한 Region에 대해 multi-Region secret replication을 구성합니다. Secrets Manager를 통해 스케줄에 따라 secrets를 회전하도록 설정합니다.",
    "SelectA_Commentary": "AWS Secrets Manager는 RDS 자격 증명 회전에 특화된 자동화 기능과 multi-Region replication 기능을 제공하므로, 관리 부담이 최소화되는 최적의 솔루션입니다.",
    "SelectB": "AWS Systems Manager의 secure string 매개변수로 자격 증명을 저장합니다. 필요한 Region에 대해 multi-Region secret replication을 구성합니다. Systems Manager를 통해 스케줄에 따라 secrets를 회전하도록 설정합니다.",
    "SelectB_Commentary": "Parameter Store도 보안 저장을 지원하지만, RDS 자격 증명 회전에 대한 자동화 기능은 Secrets Manager만큼 완비되어 있지 않아 운영 편의성이 떨어집니다.",
    "SelectC": "서버 사이드 암호화(SSE)가 활성화된 Amazon S3 버킷에 자격 증명을 저장합니다. Amazon EventBridge(Amazon CloudWatch Events)를 사용해 AWS Lambda 함수를 호출하여 자격 증명을 회전합니다.",
    "SelectC_Commentary": "S3 버킷과 Lambda를 이용한 자체 회전 로직 구현은 운영 복잡도가 높고, 별도의 스크립팅과 관리가 필요합니다.",
    "SelectD": "AWS Key Management Service(AWS KMS) multi-Region 고객 관리형 키로 자격 증명을 암호화해서 Amazon DynamoDB 글로벌 테이블에 저장합니다. AWS Lambda 함수를 사용하여 DynamoDB에서 secrets를 가져오고 RDS API를 호출해 자격 증명을 회전합니다.",
    "SelectD_Commentary": "직접 암호화, DynamoDB 글로벌 테이블, Lambda를 결합한 방안은 구성 요소가 많아 운영 부담이 증가하며, 별도 로직 구현이 필요합니다."
  },
  {
    "Question_Number": "Q14",
    "Question_Description": "한 회사가 Application Load Balancer 뒤에서 Amazon EC2 인스턴스 기반의 전자상거래 애플리케이션을 운영하고 있습니다. 이 인스턴스들은 여러 가용 영역에 분산된 Amazon EC2 Auto Scaling 그룹에서 실행되며, CPU 사용률 지표를 기준으로 확장됩니다. 전자상거래 애플리케이션은 거래 데이터를 MySQL 8.0 데이터베이스(대형 EC2 인스턴스에 호스팅)로 저장하는데, 애플리케이션 부하가 증가함에 따라 데이터베이스 성능이 급격히 저하되고 있습니다. 애플리케이션은 쓰기 트랜잭션보다 읽기 요청이 더 많은 상황입니다. 회사는 예측하기 어려운 읽기 워크로드 수요를 자동으로 충족하고, 동시에 고가용성을 유지하기 위한 솔루션을 원합니다. 다음 중 어떤 솔루션이 이러한 요구 사항을 충족할까요?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85019-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 읽기 트래픽이 많은 MySQL 기반 전자상거래 애플리케이션에서 데이터베이스를 자동으로 확장하고 고가용성을 유지해야 하는 상황입니다. Amazon Aurora의 Multi-AZ 배포와 Aurora Auto Scaling 기능을 사용하면 요구 사항을 충족하면서 뛰어난 성능과 내결함성을 확보할 수 있습니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.3"
    ],
    "Keywords": [
      "Amazon EC2 Auto Scaling",
      "MySQL 8.0",
      "읽기 워크로드",
      "고가용성",
      "Amazon Aurora"
    ],
    "Terms": [
      "Amazon Redshift",
      "Amazon RDS",
      "Amazon Aurora",
      "Aurora Auto Scaling",
      "Aurora Replica",
      "Multi-AZ Deployment",
      "Amazon ElastiCache",
      "Memcached",
      "EC2 Spot Instances"
    ],
    "SelectA": "Amazon Redshift를 단일 노드로 구성하여 리더와 컴퓨팅 기능을 동시에 수행합니다.",
    "SelectA_Commentary": "Redshift는 주로 데이터 웨어하우징 및 분석(OLAP)용이며, 트랜잭션 데이터베이스 활용 및 자동 확장 요구 사항에 적합하지 않아 성능 저하가 발생할 수 있습니다.",
    "SelectB": "Amazon RDS를 Single-AZ 배포로 사용하고, Amazon RDS에서 다른 가용 영역에 읽기 전용 인스턴스를 추가하도록 구성합니다.",
    "SelectB_Commentary": "Single-AZ 배포는 가용 영역 장애 시 접속 불가 가능성이 있으며, 읽기 트래픽 폭주에 유연하게 대응하기에도 제한이 큽니다.",
    "SelectC": "Amazon Aurora를 Multi-AZ 배포로 구성하고, Aurora Replicas에 대해 Aurora Auto Scaling을 설정합니다.",
    "SelectC_Commentary": "Aurora는 MySQL 호환이 가능하며 Multi-AZ 환경으로 고가용성을 제공하고, 자동 확장을 통해 증가하는 읽기 요청에도 빠르게 대처할 수 있어 정답입니다.",
    "SelectD": "Amazon ElastiCache for Memcached를 EC2 Spot Instances와 함께 사용합니다.",
    "SelectD_Commentary": "Memcached는 읽기 캐싱에 도움이 될 수 있지만, 트랜잭션이 필요한 DB 자체를 대체하기 어렵고 EC2 Spot Instances는 예측 불가능성이 높아 핵심 DB로 적절치 않습니다."
  },
  {
    "Question_Number": "Q15",
    "Question_Description": "한 회사가 최근 AWS로 마이그레이션을 완료했고, 프로덕션 VPC 내부 및 외부로 흐르는 트래픽을 보호하기 위한 솔루션을 구현하려고 합니다. 이 회사는 온프레미스 데이터 센터에서 점검 서버를 운용하며 트래픽 흐름 분석과 트래픽 필터링을 수행해 왔습니다. 회사는 AWS Cloud에서도 동일한 기능을 갖추길 원합니다. 이러한 요구사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/84731-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 기존 온프레미스 점검 서버가 담당하던 트래픽 점검 및 필터링 기능을 AWS 환경에서 어떻게 구현할지를 묻습니다. VPC 내부와 외부 트래픽을 보안 정책에 따라 제어하고, 운영 방식이 간단하며 확장성이 있어야 합니다. AWS Network Firewall은 상태 기반 점검과 규칙 기반 필터링을 제공하여 이러한 요구사항을 충족합니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1",
      "1.2"
    ],
    "Keywords": [
      "트래픽 보호",
      "프로덕션 VPC",
      "트래픽 점검",
      "트래픽 필터링",
      "AWS Network Firewall"
    ],
    "Terms": [
      "VPC",
      "트래픽 인바운드/아웃바운드",
      "Amazon GuardDuty",
      "Traffic Mirroring",
      "AWS Network Firewall",
      "AWS Firewall Manager",
      "AWS Cloud",
      "트래픽 흐름 분석"
    ],
    "SelectA": "프로덕션 VPC에서 Amazon GuardDuty를 사용하여 트래픽 점검 및 트래픽 필터링을 수행합니다.",
    "SelectA_Commentary": "Amazon GuardDuty는 위협 탐지 서비스로서 자체 필터링 기능을 제공하지 않으므로 요구사항을 충족하지 못합니다.",
    "SelectB": "Traffic Mirroring을 사용하여 프로덕션 VPC의 트래픽을 미러링해 점검 및 필터링을 수행합니다.",
    "SelectB_Commentary": "Traffic Mirroring은 트래픽을 복사하여 분석 도구로 보내는 기능만 제공하며, 직접적인 필터링을 수행하지 않습니다.",
    "SelectC": "AWS Network Firewall을 사용하여 프로덕션 VPC를 위한 트래픽 점검 및 트래픽 필터링 규칙을 생성합니다.",
    "SelectC_Commentary": "AWS Network Firewall은 상태 기반 방화벽과 규칙 기반 필터링을 지원해 요구사항을 모두 충족하는 올바른 솔루션입니다.",
    "SelectD": "AWS Firewall Manager를 사용하여 프로덕션 VPC에 필요한 트래픽 점검 및 필터링 규칙을 생성합니다.",
    "SelectD_Commentary": "AWS Firewall Manager는 보안 규칙을 중앙에서 관리하는 서비스로, 트래픽 필터링 엔진을 자체 제공하지 않아 요구사항에 부합하지 않습니다."
  },
  {
    "Question_Number": "Q16",
    "Question_Description": "한 회사가 AWS에서 Data Lake를 운영하고 있습니다. 이 Data Lake는 Amazon S3와 Amazon RDS for PostgreSQL에 저장된 데이터를 포함합니다. 회사에서는 모든 Data Lake의 데이터 소스를 활용해 데이터 시각화가 가능한 보고 솔루션을 원합니다. 경영진만 모든 시각화 자료에 대해 완전한 접근 권한을 가져야 하며, 그 외 직원들은 제한된 접근 권한만 가져야 합니다. 이러한 요구사항을 충족시키는 솔루션은 무엇입니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/84732-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 데이터 레이크 내 여러 데이터 소스로부터 시각화 대시보드를 생성하고, 사용자별(특히 경영진과 일반 직원) 접근 권한을 구분하는 요구사항을 해결하는 방안을 찾는 것입니다. Amazon QuickSight는 다양한 데이터 소스를 연결하고, 사용자와 그룹별 권한관리를 통해 접근 통제 기능을 간편하게 설정할 수 있어 조건을 만족합니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.3"
    ],
    "Keywords": [
      "데이터 시각화",
      "제한된 접근 권한",
      "Amazon QuickSight",
      "경영진 전용"
    ],
    "Terms": [
      "Amazon QuickSight",
      "Amazon S3",
      "Amazon RDS for PostgreSQL",
      "AWS Glue",
      "Amazon Athena Federated Query",
      "Amazon Athena",
      "S3 bucket policies",
      "ETL job"
    ],
    "SelectA": "Amazon QuickSight에서 Analysis를 생성하고 모든 데이터 소스를 연결해 신규 데이터셋을 만듭니다. 대시보드를 게시하고 적절한 IAM 역할과 공유합니다.",
    "SelectA_Commentary": "IAM 역할 기준으로 공유는 가능하나, 세부 사용자/그룹별 접근 제한 설정이 까다롭습니다.",
    "SelectB": "Amazon QuickSight에서 Analysis를 생성하고 모든 데이터 소스를 연결해 신규 데이터셋을 만듭니다. 대시보드를 게시하고 적절한 사용자와 그룹과 공유합니다.",
    "SelectB_Commentary": "사용자와 그룹 기반으로 보다 세밀하고 직관적인 접근 제어가 가능해 요구사항을 충족하는 최적의 솔루션입니다.",
    "SelectC": "AWS Glue 테이블과 크롤러로 Amazon S3 데이터를 수집하고, AWS Glue ETL 작업을 통해 리포트를 생성하여 Amazon S3에 게시합니다. S3 버킷 정책으로 접근을 제한합니다.",
    "SelectC_Commentary": "ETL 기반 리포트 생성은 시각화 기능이 부족하며, 즉각적인 대시보드 공유에 대한 세밀한 권한 제어가 어렵습니다.",
    "SelectD": "AWS Glue 테이블과 크롤러로 Amazon S3 데이터를 수집하고, Amazon Athena Federated Query를 사용해 Amazon RDS for PostgreSQL 데이터를 조회합니다. Athena로 리포트를 생성하고 Amazon S3에 게시합니다. S3 버킷 정책으로 접근을 제한합니다.",
    "SelectD_Commentary": "표준 쿼리와 파일 형태로 결과를 제공하므로 실시간 대시보드 기능과 사용 권한 세분화 측면에서 QuickSight 대비 제한적입니다."
  },
  {
    "Question_Number": "Q17",
    "Question_Description": "한 회사에서 새로운 비즈니스 애플리케이션을 구현하고 있습니다. 애플리케이션은 두 개의 Amazon EC2 인스턴스에서 실행되며, 문서 저장소로 Amazon S3 버킷을 사용합니다. 솔루션스 아키텍트는 EC2 인스턴스가 S3 버킷에 접근할 수 있도록 보장해야 합니다. 이를 위해 무엇을 해야 합니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85032-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 Amazon EC2 인스턴스가 Amazon S3 버킷에 접근할 수 있도록 권한을 설정하는 방법에 관한 것입니다. 가장 안전하고 권장되는 방법은 IAM role을 생성해 인스턴스에 연결하는 것이며, 이를 통해 보안 자격 증명 없이도 S3에 안전하게 액세스 가능합니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1"
    ],
    "Keywords": [
      "비즈니스 애플리케이션",
      "EC2 인스턴스",
      "S3 버킷",
      "IAM Role",
      "문서 저장소"
    ],
    "Terms": [
      "Amazon EC2",
      "Amazon S3",
      "IAM role",
      "IAM policy",
      "IAM group",
      "IAM user"
    ],
    "SelectA": "S3 버킷에 대한 액세스를 부여하는 IAM role을 생성하고, 해당 role을 EC2 인스턴스에 연결합니다.",
    "SelectA_Commentary": "IAM role을 통해 EC2 인스턴스가 자격 증명 없이 안전하게 S3에 접근할 수 있으며, AWS 모범 사례에 부합하는 가장 적절한 솔루션입니다.",
    "SelectB": "S3 버킷에 대한 액세스를 부여하는 IAM policy를 생성하고, 이를 EC2 인스턴스에 직접 연결합니다.",
    "SelectB_Commentary": "IAM policy는 role이나 user 등에 적용해야 하며, 인스턴스에 직접 부착하는 방식은 존재하지 않으므로 올바르지 않습니다.",
    "SelectC": "S3 버킷에 대한 액세스를 부여하는 IAM group을 생성하고, 해당 group을 EC2 인스턴스에 연결합니다.",
    "SelectC_Commentary": "IAM group은 사용자 계정을 모아 권한을 부여하는 용도로, 인스턴스에 직접 적용할 수 없어 적절한 방법이 아닙니다.",
    "SelectD": "S3 버킷에 대한 액세스를 부여하는 IAM user를 생성하고, 이 user 계정을 EC2 인스턴스에 연결합니다.",
    "SelectD_Commentary": "EC2 인스턴스가 user 자격 증명을 직접 사용하도록 구성하는 것은 관리와 보안 면에서 권장되지 않는 방식입니다."
  },
  {
    "Question_Number": "Q18",
    "Question_Description": "애플리케이션 개발 팀이 대용량 이미지를 작은 압축 이미지로 변환하는 마이크로서비스를 설계하고 있습니다. 사용자가 웹 인터페이스를 통해 이미지를 업로드하면, 해당 이미지는 Amazon S3 버킷에 저장되고, AWS Lambda 함수를 통해 처리 및 압축된 후 별도의 S3 버킷에 압축된 형태로 저장되어야 합니다. 솔루션 아키텍트는 내구성 있고 무상태(stateless)인 구성 요소를 사용하여 이미지를 자동으로 처리할 수 있는 솔루션을 설계해야 합니다. 다음 중 어떤 조합을 구성하면 이 요구사항을 충족할 수 있습니까? (2개를 선택하세요)",
    "Answer": "A,B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85033-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 무상태이면서 내구성이 뛰어난 구조로 이미지를 자동 처리하는 방안을 묻습니다. S3로부터 업로드 이벤트를 Amazon SQS 큐로 전달하고, AWS Lambda가 큐 메시지를 트리거로 이미지를 압축 처리하는 방식이 가장 단순하고 안정적입니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1"
    ],
    "Keywords": [
      "마이크로서비스",
      "이미지 압축",
      "S3 버킷",
      "AWS Lambda",
      "무상태 컴포넌트",
      "자동 처리"
    ],
    "Terms": [
      "Amazon S3",
      "AWS Lambda",
      "Amazon Simple Queue Service (Amazon SQS)",
      "Amazon SNS",
      "Amazon EventBridge (Amazon CloudWatch Events)",
      "Amazon EC2"
    ],
    "SelectA": "Amazon Simple Queue Service(Amazon SQS) 큐를 생성합니다. Amazon S3 버킷이 이미지를 업로드할 때, 해당 S3 버킷에서 SQS 큐로 알림을 보내도록 구성합니다.",
    "SelectA_Commentary": "S3 업로드 이벤트를 SQS 큐에 전달하여 이벤트를 내구성 있게 보관하고, 무상태 구조를 유지하는 핵심 단계입니다. 정답에 필요한 요소입니다.",
    "SelectB": "AWS Lambda 함수를 Amazon SQS 큐를 호출 소스로 사용하도록 구성합니다. SQS 메시지가 정상 처리되면, 큐에서 메시지를 삭제합니다.",
    "SelectB_Commentary": "Lambda 함수를 SQS로부터 직접 트리거해 메시지가 처리될 때마다 자동으로 이미지를 압축 처리하고, 처리 후 메시지를 제거함으로써 중복 수행을 방지합니다. 정답에 필요한 요소입니다.",
    "SelectC": "AWS Lambda 함수를 S3 버킷의 신규 업로드를 모니터하도록 구성합니다. 업로드된 이미지가 감지되면, 파일 이름을 텍스트 파일(메모리)에 기록하고, 이 파일을 사용해 처리된 이미지를 추적합니다.",
    "SelectC_Commentary": "직접 S3 이벤트로 Lambda를 호출할 수 있지만, 문제에서 요구하는 내구성과 무상태 구조를 확보하기 위해서는 SQS를 통해 이벤트를 비동기로 분리하는 것이 적합합니다.",
    "SelectD": "Amazon EC2 인스턴스를 실행하여 Amazon SQS 큐를 모니터링합니다. 큐에 아이템이 추가될 때마다, EC2 인스턴스에서 파일 이름을 텍스트 파일로 기록하고 Lambda 함수를 호출합니다.",
    "SelectD_Commentary": "EC2 인스턴스를 별도로 운영해야 하므로 무상태 아키텍처 요구사항에 부합하지 않으며, 불필요한 운영 복잡성이 증가합니다.",
    "SelectE": "Amazon EventBridge(Amazon CloudWatch Events)를 구성하여 S3 버킷을 모니터합니다. 이미지가 업로드되면, Amazon SNS 주제로 알림을 전송하여 해당 이메일 구독자에게 알립니다.",
    "SelectE_Commentary": "SNS 알림을 이메일로 보내는 방식은 사람이 후속 작업을 진행해야 하므로 자동 처리 요건에 맞지 않습니다."
  },
  {
    "Question_Number": "Q20",
    "Question_Description": "회사는 동일한 AWS Region 내에서 대규모 프로덕션 데이터를 테스트 환경으로 복제하는 시간을 단축하고 싶어 합니다. 데이터는 Amazon EC2 인스턴스의 Amazon EBS 볼륨에 저장되어 있으며, 복제된 데이터가 변경되더라도 프로덕션 환경에 영향을 주어서는 안 됩니다. 또한 이 데이터를 사용하는 소프트웨어는 항상 높은 I/O 성능을 필요로 합니다. 솔루션 아키텍트는 프로덕션 데이터를 테스트 환경으로 최소한의 시간으로 복제해야 합니다. 어떤 솔루션이 이러한 요구 사항을 충족합니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85226-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 프로덕션 환경의 EBS 데이터를 신속하게 테스트 환경에 복사하면서도 높은 I/O 성능과 운영 분리를 달성해야 합니다. Fast Snapshot Restore를 사용하면 스냅샷에서 생성되는 볼륨이 즉시 최대 성능을 제공하므로 복제 시간을 크게 단축할 수 있습니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.1"
    ],
    "Keywords": [
      "프로덕션 데이터 복제",
      "테스트 환경",
      "Amazon EBS",
      "EBS Snapshot",
      "Fast Snapshot Restore",
      "고성능 I/O"
    ],
    "Terms": [
      "Amazon EC2",
      "Amazon EBS",
      "EBS Multi-Attach",
      "EBS Snapshot",
      "EC2 Instance Store",
      "Fast Snapshot Restore",
      "I/O 성능"
    ],
    "SelectA": "프로덕션 EBS 볼륨의 스냅샷을 생성한 후, 해당 스냅샷을 테스트 환경의 EC2 Instance Store 볼륨에 복원합니다.",
    "SelectA_Commentary": "Instance Store는 일시적 스토리지이며 스냅샷 복원 시간이 오래 걸릴 수 있어 운영 분리와 빠른 복제, 고성능 I/O 요구 사항에 모두 부합하기 어렵습니다.",
    "SelectB": "프로덕션 EBS 볼륨에 EBS Multi-Attach 기능을 구성하고 스냅샷을 생성합니다. 그 후 프로덕션 EBS 볼륨을 테스트 환경의 EC2 인스턴스에 연결합니다.",
    "SelectB_Commentary": "Multi-Attach로 같은 볼륨을 동시에 사용하면 프로덕션 데이터가 영향을 받을 가능성이 있으므로, 완전한 환경 분리를 보장하기 어렵습니다.",
    "SelectC": "프로덕션 EBS 볼륨의 스냅샷을 생성합니다. 새 EBS 볼륨을 만들고 초기화한 후, 프로덕션 EBS 스냅샷을 복원하기 전에 이를 테스트 환경의 EC2 인스턴스에 연결합니다.",
    "SelectC_Commentary": "일반적인 스냅샷 복원은 볼륨을 처음 사용할 때 데이터 블록을 로드하는 지연이 발생해 전체 복원 시간이 길어질 수 있습니다.",
    "SelectD": "프로덕션 EBS 볼륨의 스냅샷을 생성합니다. 해당 스냅샷에 EBS Fast Snapshot Restore 기능을 활성화한 뒤, 새 EBS 볼륨으로 복원하여 테스트 환경의 EC2 인스턴스에 연결합니다.",
    "SelectD_Commentary": "Fast Snapshot Restore를 활성화하면 새 볼륨이 생성 즉시 최대 성능을 제공하므로 복제 시간을 단축하고 고성능 I/O를 보장합니다."
  },
  {
    "Question_Number": "Q21",
    "Question_Description": "한 전자상거래(ecommerce) 회사가 AWS에서 하루에 하나의 특별 할인 상품(one-deal-a-day)을 제공하는 웹사이트를 론칭하려고 합니다. 매일 정확히 하나의 상품이 24시간 동안 판매됩니다. 이 회사는 피크 시간대에 밀리초(ms) 단위의 지연 시간으로 시간당 수백만 건의 요청을 처리할 수 있기를 바랍니다. 가장 적은 운영 오버헤드(operational overhead)로 이 요구사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85195-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 하루에 하나의 상품을 매우 짧은 지연 시간으로 대규모 트래픽에 대응해야 하는 시나리오입니다. 정적 콘텐츠는 Amazon S3와 CloudFront로 빠르고 확장 가능하게 제공할 수 있으며, 백엔드는 API Gateway와 Lambda 같은 서버리스로 구성해 자동 확장과 운영 단순화를 제공합니다. 데이터베이스 계층도 DynamoDB를 사용하여 높은 처리량과 낮은 지연 시간을 확보할 수 있어, 요구사항을 가장 효율적으로 만족할 수 있습니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.1",
      "3.2",
      "3.3",
      "3.4"
    ],
    "Keywords": [
      "하루에 하나의 특별 할인 상품",
      "24시간 판매",
      "수백만 건의 요청",
      "밀리초 단위 지연 시간",
      "운영 오버헤드 최소화"
    ],
    "Terms": [
      "Amazon S3",
      "Amazon CloudFront",
      "Amazon EC2",
      "Auto Scaling",
      "Application Load Balancer (ALB)",
      "Amazon EKS",
      "Kubernetes Cluster Autoscaler",
      "Amazon API Gateway",
      "AWS Lambda",
      "Amazon DynamoDB",
      "Amazon RDS for MySQL"
    ],
    "SelectA": "Use Amazon S3 to host the full website in different S3 buckets. Add Amazon CloudFront distributions. Set the S3 buckets as origins for the distributions. Store the order data in Amazon S3.",
    "SelectA_Commentary": "전체 웹사이트를 S3에서 호스팅하지만 동적 요청 처리와 데이터 관리가 부족합니다. 단순 파일 호스팅 용도로는 좋지만, 초당 대량 트랜잭션 처리를 위한 서버리스 백엔드 구성이 마련되어 있지 않습니다.",
    "SelectB": "Deploy the full website on Amazon EC2 instances that run in Auto Scaling groups across multiple Availability Zones. Add an Application Load Balancer (ALB) to distribute the website traffic. Add another ALB for the backend APIs. Store the data in Amazon RDS for MySQL.",
    "SelectB_Commentary": "EC2 인스턴스와 확장형 RDS 구성은 충분한 성능을 낼 수 있지만, 서버 운영과 Auto Scaling 관리 등 운영 오버헤드가 큽니다. 밀리초 단위 지연에 대응하기 위해서는 인프라 관리가 복잡해집니다.",
    "SelectC": "Migrate the full application to run in containers. Host the containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use the Kubernetes Cluster Autoscaler to increase and decrease the number of pods to process bursts in traffic. Store the data in Amazon RDS for MySQL.",
    "SelectC_Commentary": "EKS로 컨테이너를 자동 확장할 수 있지만, Kubernetes 관리와 클러스터 운영은 여전히 복잡합니다. 서버리스보다 운영 부담이 크며, 데이터베이스도 RDS로 유지 시 오버헤드가 적지 않습니다.",
    "SelectD": "Use an Amazon S3 bucket to host the website's static content. Deploy an Amazon CloudFront distribution. Set the S3 bucket as the origin. Use Amazon API Gateway and AWS Lambda functions for the backend APIs. Store the data in Amazon DynamoDB.",
    "SelectD_Commentary": "정적 콘텐츠는 S3와 CloudFront, 동적 처리는 API Gateway와 Lambda, 데이터는 DynamoDB에 저장하여 무한 확장성과 낮은 지연 시간을 확보할 수 있습니다. 운영 오버헤드를 최소화하며 고성능을 달성하는 최적의 서버리스 아키텍처입니다."
  },
  {
    "Question_Number": "Q22",
    "Question_Description": "한 Solutions Architect가 새로운 디지털 미디어 애플리케이션의 스토리지 아키텍처를 Amazon S3로 설계하고 있습니다. 매체 파일은 하나의 가용 영역 상실에도 견딜 수 있어야 하며, 파일들은 어떤 것은 자주 액세스되고 어떤 것은 거의 액세스되지 않을 수 있지만 그 패턴이 예측 불가능합니다. 이 때 파일을 저장하고 검색하는 데 드는 비용을 최소화해야 합니다. 이러한 요구 사항을 충족하는 스토리지 옵션은 무엇입니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/84943-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 예측하기 어려운 액세스 패턴을 가진 파일을 안정적으로 보관하고, 비용을 절감해야 하는 상황에서 올바른 S3 스토리지 클래스를 선택하는 것입니다. 가용 영역 상실에도 견딜 수 있어야 하므로 최소 3개의 Availability Zone에 데이터를 저장하는 클래스여야 하며, 접근 빈도의 예측이 어렵다면 S3 Intelligent-Tiering을 고려해야 합니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.1"
    ],
    "Keywords": [
      "디지털 미디어 애플리케이션",
      "가용 영역 상실",
      "예측 불가능한 액세스 패턴",
      "비용 최소화",
      "S3 Intelligent-Tiering"
    ],
    "Terms": [
      "Amazon S3",
      "S3 Standard",
      "S3 Intelligent-Tiering",
      "S3 Standard-Infrequent Access (S3 Standard-IA)",
      "S3 One Zone-Infrequent Access (S3 One Zone-IA)"
    ],
    "SelectA": "S3 Standard",
    "SelectA_Commentary": "S3 Standard는 다중 AZ 내구성을 제공하지만, 자주 액세스되지 않는 객체에도 동일 요율이 부과되어 비용 최적화 효과가 떨어집니다.",
    "SelectB": "S3 Intelligent-Tiering",
    "SelectB_Commentary": "자주 액세스되는 객체와 드물게 액세스되는 객체를 자동으로 계층화하고 비용을 절감하면서도 다중 AZ 내구성을 제공하는 최적의 솔루션입니다.",
    "SelectC": "S3 Standard-Infrequent Access (S3 Standard-IA)",
    "SelectC_Commentary": "다중 AZ 내구성을 제공하지만, 불규칙한 액세스에 적합하지 않고 검색 패턴이 불투명할 경우 비용이 더 들 수 있습니다.",
    "SelectD": "S3 One Zone-Infrequent Access (S3 One Zone-IA)",
    "SelectD_Commentary": "한 개의 AZ에만 데이터를 보관하여 가용 영역 상실에 대응할 수 없어 내구성 요구사항을 충족하지 못합니다."
  },
  {
    "Question_Number": "Q23",
    "Question_Description": "한 회사가 Amazon S3 Standard 스토리지를 사용하여 백업 파일을 저장하고 있습니다. 해당 파일들은 1개월 동안은 자주 액세스되지만 이후로는 거의 액세스되지 않습니다. 또한 회사는 이 파일들을 무기한 보관해야 합니다. 비용 효율성을 최대화하기 위해 사용할 수 있는 가장 적절한 스토리지 솔루션은 무엇입니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85092-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 1개월 후에 더 이상 조회되지 않는 백업 파일을 무기한 보관해야 하므로 장기 보관 및 비용 효율성에 중점을 두는 것이 핵심입니다. Amazon S3 Glacier Deep Archive는 매우 저렴한 비용으로 데이터를 보관할 수 있는 스토리지 클래스이므로, 1개월 후에는 이 클래스로 자동 전환되도록 S3 Lifecycle Policy를 설정하는 것이 가장 비용 효과적입니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.1"
    ],
    "Keywords": [
      "백업 파일",
      "무기한 보관",
      "비용 효율성",
      "1개월 후 비액세스",
      "S3 Glacier Deep Archive"
    ],
    "Terms": [
      "Amazon S3 Standard",
      "S3 Intelligent-Tiering",
      "S3 Glacier Deep Archive",
      "S3 Standard-IA",
      "S3 One Zone-IA",
      "S3 Lifecycle Configuration"
    ],
    "SelectA": "S3 Intelligent-Tiering을 구성하여 객체를 자동으로 마이그레이션합니다.",
    "SelectA_Commentary": "S3 Intelligent-Tiering은 엑세스 패턴이 불규칙할 때 유용하지만, 장기적으로 거의 액세스가 없는 백업 파일에는 Deep Archive만큼 저렴하지 않습니다.",
    "SelectB": "S3 Lifecycle 구성을 만들어, 객체를 1개월 후 S3 Glacier Deep Archive로 전환하도록 설정합니다.",
    "SelectB_Commentary": "백업 파일을 1개월 동안 S3 Standard에서 유지한 뒤, 거의 액세스가 없을 때 극도로 저렴한 Deep Archive로 자동 전환해 비용을 크게 절감하는 최적의 선택입니다.",
    "SelectC": "S3 Lifecycle 구성을 만들어, 객체를 1개월 후 S3 Standard-IA로 전환하도록 설정합니다.",
    "SelectC_Commentary": "S3 Standard-IA도 저렴하지만, 최장 유지 비용면에서 Deep Archive보다 비싸므로 장기 보관에는 부적합합니다.",
    "SelectD": "S3 Lifecycle 구성을 만들어, 객체를 1개월 후 S3 One Zone-Infrequent Access로 전환하도록 설정합니다.",
    "SelectD_Commentary": "One Zone-IA는 내구성 측면에서 여러 AZ를 활용하지 않으므로 백업 파일용으로 안전성이 떨어지며, 장기 보관에서는 Deep Archive만큼 비용 효율적이지 않습니다."
  },
  {
    "Question_Number": "Q24",
    "Question_Description": "한 회사가 최근 청구서를 확인하던 중 Amazon EC2 비용이 증가한 것을 발견했습니다. 청구 담당 부서에서는 몇 개의 EC2 인스턴스가 원치 않게 인스턴스 유형을 상향(Vertical Scaling)했다는 점을 파악했습니다. 솔루션스 아키텍트는 지난 2개월 간 EC2 비용을 비교하는 그래프를 생성하고, 이러한 상향 조정의 근본 원인을 찾기 위해 심층 분석을 수행해야 합니다. 가장 적은 운영 오버헤드로 정보를 생성하려면 어떻게 해야 합니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85038-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 EC2 인스턴스 유형 변경으로 인한 비용 증가의 원인을 파악하는 방법을 묻습니다. 가장 간단하면서 분석 기능이 뛰어난 Cost Explorer의 필터링 기능을 활용하면, 인스턴스 타입별로 지난 2개월간의 비용 변화를 직관적으로 분석할 수 있습니다. 이 방식이 추가적인 인프라 구성 없이 운영 부담을 최소화하는 데 최적입니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.2"
    ],
    "Keywords": [
      "인스턴스 유형",
      "EC2 비용",
      "비용 비교",
      "운영 오버헤드 최소화"
    ],
    "Terms": [
      "AWS Budgets",
      "Cost Explorer",
      "AWS Billing and Cost Management",
      "AWS Cost and Usage Reports",
      "Amazon QuickSight",
      "Amazon S3"
    ],
    "SelectA": "AWS Budgets를 사용하여 예산 보고서를 생성하고 인스턴스 유형을 기준으로 EC2 비용을 비교합니다.",
    "SelectA_Commentary": "AWS Budgets는 예산 모니터링 및 알림에 강점이 있으나, 세부적인 비용 분석 기능은 제한적입니다.",
    "SelectB": "Cost Explorer의 세분화된 필터링 기능을 사용하여 인스턴스 유형별 EC2 비용에 대해 심층 분석을 수행합니다.",
    "SelectB_Commentary": "Cost Explorer는 인스턴스 유형 별 비용을 직관적으로 비교할 수 있고, 추가 설정이 없어 운영 오버헤드를 크게 줄이는 최적의 선택입니다.",
    "SelectC": "AWS Billing and Cost Management 대시보드의 그래프를 사용하여 지난 2개월 간 인스턴스 유형별 EC2 비용을 비교합니다.",
    "SelectC_Commentary": "대시보드의 그래프 비교는 기본적인 정보를 제공하지만, 원하는 수준의 세밀한 필터링 및 분석 기능이 제한됩니다.",
    "SelectD": "AWS Cost and Usage Reports를 생성하여 Amazon S3 버킷에 전송하고, Amazon QuickSight로 S3를 소스로 사용하여 인스턴스 유형 기반 인터랙티브 그래프를 생성합니다.",
    "SelectD_Commentary": "이 방법은 강력한 시각화가 가능하지만, QuickSight와 S3 설정 등 추가 구성이 필요해 운영 오버헤드가 커집니다."
  },
  {
    "Question_Number": "Q25",
    "Question_Description": "한 회사가 애플리케이션을 설계하고 있습니다. 이 애플리케이션은 Amazon API Gateway를 통해 정보를 받아 Amazon Aurora PostgreSQL 데이터베이스에 저장하기 위해 AWS Lambda 함수를 사용합니다. 개념 증명 단계에서, 회사는 대규모 데이터를 데이터베이스에 로드하기 위해 Lambda 할당량을 크게 늘려야 했습니다. 솔루션스 아키텍트는 확장성을 높이고 구성 노력을 최소화할 수 있는 새로운 설계를 제안해야 합니다. 어떤 솔루션이 이 요구사항을 만족합니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85197-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 API Gateway로 받은 대규모 데이터를 Aurora PostgreSQL에 저장하는 과정에서 Lambda 한도 증가가 필요한 상황을 해결해야 합니다. 기본 Lambda 구조를 무리하게 확장하기보다, 두 개의 Lambda 함수를 두고 Amazon SQS로 분리하면 수신과 적재를 느슨하게 결합해 확장성 및 가용성을 높일 수 있습니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1"
    ],
    "Keywords": [
      "Lambda 확장성",
      "Aurora PostgreSQL",
      "대규모 데이터 로드",
      "구성 노력 최소화",
      "애플리케이션 설계"
    ],
    "Terms": [
      "AWS Lambda",
      "Amazon API Gateway",
      "Amazon Aurora PostgreSQL",
      "Amazon EC2",
      "Apache Tomcat",
      "JDBC",
      "Amazon DynamoDB",
      "DynamoDB Accelerator(DAX)",
      "Amazon SNS",
      "Amazon SQS"
    ],
    "SelectA": "AWS Lambda 함수 코드를 Amazon EC2 인스턴스에서 실행되는 Apache Tomcat 코드로 리팩터링합니다. 데이터베이스는 JDBC 드라이버를 사용해 연결합니다.",
    "SelectA_Commentary": "코드 리팩터링과 EC2 환경 구성에 많은 작업이 필요해 구성 노력이 크고, 서버 관리 부담이 높아집니다.",
    "SelectB": "Amazon Aurora에서 Amazon DynamoDB로 플랫폼을 변경하고, DynamoDB Accelerator(DAX) 클러스터를 프로비저닝합니다. DAX 클라이언트 SDK를 사용해 기존 DynamoDB API 호출을 DAX 클러스터로 지정합니다.",
    "SelectB_Commentary": "Aurora(관계형)에서 DynamoDB(비관계형)로 마이그레이션이 필요해 설계 변경 폭이 크고, SQL을 NoSQL로 바꾸는 데도 큰 노력이 들어갑니다.",
    "SelectC": "두 개의 Lambda 함수를 구성합니다. 하나는 정보를 수신하고, 다른 하나는 데이터를 데이터베이스에 로드합니다. Amazon Simple Notification Service(Amazon SNS)를 사용해 두 Lambda 함수를 통합합니다.",
    "SelectC_Commentary": "SNS 알림으로 데이터를 전달하지만, 대량 전송 시 동시에 많은 이벤트가 발생해 여전히 부하 제어가 어렵습니다.",
    "SelectD": "두 개의 Lambda 함수를 구성합니다. 하나는 정보를 수신하고, 다른 하나는 데이터를 데이터베이스에 로드합니다. Amazon Simple Queue Service(Amazon SQS) 큐를 사용해 두 Lambda 함수를 통합합니다.",
    "SelectD_Commentary": "SQS를 통해 비동기 큐 기반 구조로 확장성과 안정성을 확보하며, 대규모 데이터도 효율적으로 처리할 수 있는 가장 적합한 솔루션입니다."
  },
  {
    "Question_Number": "Q26",
    "Question_Description": "한 회사가 Amazon S3 버킷의 구성 변경 사항을 검토하여 무단으로 변경된 부분이 없는지 확인해야 합니다. 이를 달성하기 위해 Solutions Architect는 무엇을 해야 합니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/84940-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 S3 버킷 설정이 무단으로 변경되지 않도록 모니터링 및 감사가 가능한 솔루션을 찾는 상황입니다. AWS Config를 통해 S3 버킷 구성 변경 이력을 추적하고 평가 규칙을 적용하면 무단 변경 사항을 빠르게 식별하고 대응할 수 있어, 가장 적절한 해법입니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.3"
    ],
    "Keywords": [
      "Amazon S3",
      "구성 변경",
      "무단 변경 방지",
      "AWS Config"
    ],
    "Terms": [
      "AWS Config",
      "AWS Trusted Advisor",
      "Amazon Inspector",
      "Amazon S3 server access logging",
      "Amazon EventBridge (Amazon CloudWatch Events)"
    ],
    "SelectA": "AWS Config를 활성화하고 적절한 규칙을 설정합니다.",
    "SelectA_Commentary": "AWS Config는 리소스의 구성 상태를 지속적으로 모니터링하고 기록할 수 있어 무단 변경 사항을 자동으로 감지하고 보고할 수 있습니다.",
    "SelectB": "AWS Trusted Advisor를 활성화하고 적절한 체크를 설정합니다.",
    "SelectB_Commentary": "Trusted Advisor는 모범 사례 관점에서 권장 사항을 제시하지만, 실시간 구성 변경 추적이나 감시 기능은 제한적입니다.",
    "SelectC": "Amazon Inspector를 활성화하고 적절한 평가 템플릿을 설정합니다.",
    "SelectC_Commentary": "Amazon Inspector는 주로 EC2 인스턴스 및 애플리케이션 보안을 평가하는 도구로, S3 버킷 구성 변경 모니터링 용도와는 맞지 않습니다.",
    "SelectD": "Amazon S3 server access logging을 활성화하고, Amazon EventBridge를 구성합니다.",
    "SelectD_Commentary": "서버 액세스 로그와 EventBridge를 사용하면 액세스 및 이벤트 정보를 모니터링할 수 있으나, 직접적이고 체계적인 구성 변경 추적에는 AWS Config가 더 적합합니다."
  },
  {
    "Question_Number": "Q30",
    "Question_Description": "한 개발 팀이 Performance Insights가 활성화된 general purpose Amazon RDS for MySQL DB instance에서 매월 리소스를 많이 사용하는 테스트를 실행합니다. 이 테스트는 한 달에 한 번, 48시간 동안만 진행되며, 이 데이터베이스를 사용하는 유일한 프로세스입니다. 해당 팀은 DB 인스턴스의 컴퓨팅 및 메모리 사양은 유지하면서, 테스트를 실행하는 비용을 절감하고 싶어 합니다. 이러한 요구사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85030-exam-aws-certified-solut",
    "AnswerDescription": "테스트에만 사용되는 DB를 상시로 유지하면 비용이 많이 듭니다. 스냅샷을 생성 후 DB 인스턴스를 종료하면 월 대부분의 시간에 비용을 절약하면서 필요 시 동일 사양으로 복원할 수 있어 가장 효율적입니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.3"
    ],
    "Keywords": [
      "RDS",
      "MySQL",
      "Performance Insights",
      "스냅샷",
      "비용 절감"
    ],
    "Terms": [
      "Amazon RDS for MySQL",
      "Performance Insights",
      "Snapshot",
      "귀중(High) Compute",
      "메모리 사양",
      "DB instance"
    ],
    "SelectA": "테스트가 완료되면 DB 인스턴스를 중지합니다. 필요할 때 DB 인스턴스를 다시 시작합니다.",
    "SelectA_Commentary": "인스턴스를 중지해도 저장소 비용은 계속 들고, 7일 제한 등의 제약이 있어 월 1회 장기 중지 시 운용상 불편이 큽니다.",
    "SelectB": "DB 인스턴스를 사용하는 Auto Scaling policy를 적용하여, 테스트가 완료되면 자동으로 스케일 다운합니다.",
    "SelectB_Commentary": "Amazon RDS for MySQL에 직접적인 Auto Scaling 정책이 적용되지 않으며, 테스트 외 시간에도 RDS 인스턴스가 계속 동작해 비용을 절감하기 어렵습니다.",
    "SelectC": "테스트가 끝나면 스냅샷을 생성합니다. DB 인스턴스를 종료한 뒤, 필요할 때 해당 스냅샷을 복원합니다.",
    "SelectC_Commentary": "인스턴스를 완전히 종료해 사용 시간이 없을 때 인스턴스 비용이 들지 않아 가장 비용 효율적입니다. 스냅샷 복원으로 동일 사양을 빠르고 손쉽게 재생성 가능합니다.",
    "SelectD": "테스트가 완료되면 DB 인스턴스를 소용량 인스턴스로 변경합니다. 필요할 때 다시 원래 사양으로 변경합니다.",
    "SelectD_Commentary": "DB 인스턴스 스펙 변경은 원하는 컴퓨팅·메모리를 유지해야 한다는 요구사항과 어긋나며, 변경 과정에서도 추가 비용과 시간이 소요됩니다."
  },
  {
    "Question_Number": "Q31",
    "Question_Description": "한 회사가 AWS에서 웹 애플리케이션을 호스팅 중이며, 모든 Amazon EC2 인스턴스, Amazon RDS DB 인스턴스, Amazon Redshift 클러스터가 태그(Tag)로 구성되어 있는지 확인하고자 합니다. 이 확인 작업의 구성과 운영 부담을 최소화하려면 어떻게 해야 합니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85198-exam-aws-certified-solut",
    "AnswerDescription": "AWS 리소스의 태그 상태를 자동으로 검사하고 싶다면 AWS Config가 가장 간단하고 효율적인 방법입니다. 수동이나 자체 코드 대신 관리형 규칙을 사용하면 운영 부담이 크게 줄어듭니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.2",
      "4.3"
    ],
    "Keywords": [
      "Amazon EC2",
      "Amazon RDS",
      "Amazon Redshift",
      "태그",
      "운영 부담 최소화"
    ],
    "Terms": [
      "AWS Config",
      "Cost Explorer",
      "AWS Lambda",
      "CloudWatch"
    ],
    "SelectA": "AWS Config 규칙을 사용하여 태그가 올바르게 설정되지 않은 리소스를 정의하고 감지합니다.",
    "SelectA_Commentary": "AWS Config는 자동 규칙으로 미태그 자원을 식별하고 모니터링하므로 운영 부담을 줄이는 최적의 솔루션입니다.",
    "SelectB": "Cost Explorer를 사용해 태그가 잘못된 리소스를 표시하고 수동으로 태그를 구성합니다.",
    "SelectB_Commentary": "Cost Explorer로 확인은 가능하지만 태그 반영 과정이 전부 수동이므로 운영 부담이 높습니다.",
    "SelectC": "적절한 태그 할당을 확인하는 API 호출을 작성하고, 이를 EC2 인스턴스에서 주기적으로 실행합니다.",
    "SelectC_Commentary": "별도 코드 유지와 스케줄 관리가 필요해 운영 복잡성이 큽니다.",
    "SelectD": "API 호출을 작성해 태그 할당을 확인하고, AWS Lambda 함수를 CloudWatch로 스케줄링해 주기적으로 실행합니다.",
    "SelectD_Commentary": "Lambda를 통해 자동화 가능하지만 자체 코드 작성과 유지가 필요해 AWS Config보다 부담이 큽니다."
  },
  {
    "Question_Number": "Q32",
    "Question_Description": "한 개발 팀이 다른 팀이 접속할 웹사이트를 호스팅해야 합니다. 웹사이트의 콘텐츠는 HTML, CSS, client-side JavaScript, 그리고 images로 구성됩니다. 가장 비용 효율적인 웹사이트 호스팅 방법은 무엇입니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85199-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 정적 콘텐츠로만 구성된 웹사이트를 가장 저렴하고 간단하게 호스팅하는 방안을 묻습니다. Amazon S3의 정적 웹 호스팅 기능은 서버나 컨테이너에 비용이 들지 않으므로, 소규모 팀 환경에서 특히 비용 효율적입니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.1"
    ],
    "Keywords": [
      "비용 효율적인 웹사이트 호스팅",
      "정적 웹사이트",
      "Amazon S3",
      "HTML",
      "client-side JavaScript"
    ],
    "Terms": [
      "AWS Fargate",
      "Amazon S3",
      "Amazon EC2",
      "Application Load Balancer",
      "AWS Lambda",
      "Express.js",
      "HTML",
      "CSS",
      "client-side JavaScript",
      "images"
    ],
    "SelectA": "웹사이트를 컨테이너로 만든 뒤 AWS Fargate에서 호스팅합니다.",
    "SelectA_Commentary": "정적 웹사이트를 컨테이너로 운영하면 불필요한 운영 및 컴퓨팅 비용이 추가되어 최적의 비용 효율이 아닙니다.",
    "SelectB": "Amazon S3 버킷을 생성하고 그곳에서 웹사이트를 호스팅합니다.",
    "SelectB_Commentary": "HTML, CSS, client-side JavaScript 등의 정적 파일은 Amazon S3에서 매우低 운영 비용으로 손쉽게 제공할 수 있으므로 가장 비용 효율적입니다.",
    "SelectC": "웹 서버를 Amazon EC2 인스턴스에 배포하여 웹사이트를 호스팅합니다.",
    "SelectC_Commentary": "EC2 인스턴스를 임대하는 비용이 발생하며, 서버 유지 관리도 필요하므로 정적 사이트 호스팅으로는 효율이 떨어집니다.",
    "SelectD": "Application Load Balancer를 설정하고 AWS Lambda에서 Express.js 프레임워크를 사용하는 대상으로 연결합니다.",
    "SelectD_Commentary": "Lambda와 ALB 구성이 가능하긴 하나, 정적 콘텐츠 제공만을 위해서는 과도하며 복잡도와 비용이 증가합니다."
  },
  {
    "Question_Number": "Q33",
    "Question_Description": "한 회사가 AWS에서 온라인 마켓플레이스 웹 애플리케이션을 운영하고 있습니다. 피크 시간대에는 수십만 명의 사용자를 지원합니다. 이 회사는 여러 내부 애플리케이션에 수백만 건의 금융 거래 정보를 거의 실시간(near-real-time)으로 공유할 수 있는 확장 가능한 솔루션이 필요합니다. 또한 트랜잭션을 document database에 낮은 지연 시간으로 저장하기 전에 민감한 데이터를 제거해야 합니다. 이러한 요구사항을 충족하기 위해 솔루션스 아키텍트는 어떤 구성을 추천해야 합니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85201-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 수백만 건의 금융 거래를 거의 실시간으로 여러 내부 애플리케이션에 전달하고, 동시에 민감한 데이터를 제거하여 문서형 데이터베이스에 저장해야 하는 고속·확장성 아키텍처를 설계하는 상황입니다. Amazon Kinesis Data Streams와 AWS Lambda를 조합하면 실시간 처리가 가능하며, Lambda 함수로 민감한 데이터를 제거 후 Amazon DynamoDB에 저장할 수 있습니다. 다른 애플리케이션들은 Kinesis Data Streams로부터 직접 데이터를 구독함으로써 확장성을 유지하면서도 지연 시간을 최소화할 수 있습니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.3",
      "3.5"
    ],
    "Keywords": [
      "온라인 마켓플레이스",
      "near-real-time",
      "수백만 건의 금융 거래",
      "민감한 데이터 제거",
      "document database",
      "낮은 지연 시간"
    ],
    "Terms": [
      "Amazon DynamoDB",
      "DynamoDB Streams",
      "Amazon Kinesis Data Firehose",
      "Amazon Kinesis Data Streams",
      "AWS Lambda",
      "Amazon S3"
    ],
    "SelectA": "Amazon DynamoDB에 트랜잭션 데이터를 저장하고, DynamoDB에 쓰여질 때 민감한 데이터를 제거하도록 설정합니다. DynamoDB Streams를 사용하여 다른 애플리케이션과 데이터를 공유합니다.",
    "SelectA_Commentary": "DynamoDB에 쓰기 시점에서 자동 필터링 규칙을 적용하는 기능은 기본적으로 제공되지 않아 원하는 대로 민감 정보를 완전히 제거하기 어렵습니다.",
    "SelectB": "트랜잭션 데이터를 Amazon Kinesis Data Firehose로 스트리밍하여 Amazon DynamoDB와 Amazon S3에 저장합니다. AWS Lambda 통합으로 민감한 데이터를 제거합니다. 다른 애플리케이션들은 Amazon S3에 저장된 데이터를 사용합니다.",
    "SelectB_Commentary": "Kinesis Data Firehose는 DynamoDB를 직접 대상으로 지원하지 않으므로, 요구사항인 near-real-time DynamoDB 삽입이 어렵습니다.",
    "SelectC": "트랜잭션 데이터를 Amazon Kinesis Data Streams로 스트리밍합니다. AWS Lambda 통합을 통해 각 트랜잭션에서 민감한 데이터를 제거한 후 Amazon DynamoDB에 저장합니다. 다른 애플리케이션들은 Kinesis 스트림에서 데이터를 소비합니다.",
    "SelectC_Commentary": "Kinesis Data Streams와 Lambda의 조합은 최소 지연으로 대량 데이터를 처리하고 민감 정보를 필터링하기 적합하며 DynamoDB 저장으로 저지연 읽기도 가능합니다.",
    "SelectD": "배치된 트랜잭션 데이터를 Amazon S3에 파일 형태로 저장합니다. AWS Lambda로 각 파일을 처리하여 민감한 데이터를 제거한 다음, 파일을 업데이트하고 Amazon DynamoDB에 저장합니다. 다른 애플리케이션들은 S3에 저장된 파일을 사용합니다.",
    "SelectD_Commentary": "배치 파일 처리 방식이므로 실시간성이 떨어지고, 민감 정보 제거-재업로드 과정도 복잡해 요구사항과 맞지 않습니다."
  },
  {
    "Question_Number": "Q34",
    "Question_Description": "한 회사가 AWS에서 다중 계층 애플리케이션을 호스팅하고 있습니다. 컴플라이언스, 거버넌스, 감사, 보안 목적상 AWS 리소스에 대한 구성 변경 사항을 추적하고 이 리소스들에 대한 API 호출 이력을 기록해야 합니다. 이러한 요구 사항을 충족하기 위해 솔루션스 아키텍트는 무엇을 해야 합니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85202-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 AWS 리소스에 대한 구성 변경 사항과 API 호출 이력을 동시에 추적해야 하는 상황입니다. AWS Config는 리소스 변경 내역을 지속적으로 모니터링하고, AWS CloudTrail은 사용자 및 서비스 API 호출 정보를 기록하여 보안, 감사, 거버넌스 요구 사항을 모두 충족합니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.3"
    ],
    "Keywords": [
      "다중 계층 애플리케이션",
      "컴플라이언스",
      "구성 변경 사항",
      "API 호출 이력",
      "AWS Config",
      "AWS CloudTrail"
    ],
    "Terms": [
      "AWS Config",
      "AWS CloudTrail",
      "Amazon CloudWatch",
      "Configuration changes",
      "API calls",
      "Compliance",
      "Governance",
      "Auditing",
      "Security"
    ],
    "SelectA": "AWS CloudTrail을 사용하여 구성 변경 사항을 추적하고 AWS Config를 사용하여 API 호출을 기록합니다.",
    "SelectA_Commentary": "각 서비스의 역할이 반대로 설정되어 있어 요구 사항을 충족하지 못합니다.",
    "SelectB": "AWS Config를 사용하여 구성 변경 사항을 추적하고 AWS CloudTrail을 사용하여 API 호출을 기록합니다.",
    "SelectB_Commentary": "정답입니다. 각 서비스가 맡은 역할과 기능이 정확히 부합해 보안과 감사 요구 사항을 모두 충족합니다.",
    "SelectC": "AWS Config를 사용하여 구성 변경 사항을 추적하고 Amazon CloudWatch를 사용하여 API 호출을 기록합니다.",
    "SelectC_Commentary": "Amazon CloudWatch는 로그 모니터링을 주로 담당하며, API 호출 기록 기능은 CloudTrail이 제공하므로 적절하지 않습니다.",
    "SelectD": "AWS CloudTrail을 사용하여 구성 변경 사항을 추적하고 Amazon CloudWatch를 사용하여 API 호출을 기록합니다.",
    "SelectD_Commentary": "CloudTrail이 API 호출 로깅을 담당해야 하므로, 이 조합은 요구 사항을 충족하지 못합니다."
  },
  {
    "Question_Number": "Q36",
    "Question_Description": "한 회사가 AWS Cloud에서 애플리케이션을 구축하고 있습니다. 애플리케이션은 두 개의 AWS Region에 있는 Amazon S3 버킷에 데이터를 저장할 예정입니다. 회사는 모든 데이터를 AWS Key Management Service(AWS KMS)의 Customer managed key로 암호화해야 합니다. 또한 두 버킷에 있는 모든 데이터가 동일한 KMS key로 암호화 및 복호화되어야 하며, 두 Region 각각에 데이터와 해당 KMS key가 존재해야 합니다. 가장 적은 운영 오버헤드로 이러한 요구사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/84747-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 동일한 Customer managed key를 두 개의 Region에서 모두 활용해야 하는 조건을 만족하면서 운영 오버헤드를 최소화하는 KMS 암호화 방안을 묻습니다. Multi-Region KMS key를 사용해야 Region별 동일 키 운용이 가능합니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.3"
    ],
    "Keywords": [
      "AWS KMS",
      "Customer managed key",
      "S3 버킷",
      "다른 Region",
      "암호화"
    ],
    "Terms": [
      "Multi-Region KMS key",
      "SSE-KMS",
      "SSE-S3",
      "Client-side Encryption",
      "Server-side Encryption",
      "Replication"
    ],
    "SelectA": "각 Region에 S3 버킷을 생성하고, Amazon S3 managed encryption keys(SSE-S3)로 서버측 암호화를 활성화합니다. 그런 다음 두 버킷 간에 복제를 구성합니다.",
    "SelectA_Commentary": "SSE-S3는 고객이 관리하는 KMS key가 아니라서 요구사항을 충족하지 못합니다.",
    "SelectB": "Multi-Region KMS key로 Customer managed key를 생성합니다. 각 Region에 S3 버킷을 생성하고, 버킷 간 복제를 구성합니다. 애플리케이션은 이 KMS key를 사용해 클라이언트 측 암호화를 수행합니다.",
    "SelectB_Commentary": "Multi-Region KMS key를 사용해 두 Region에서 동일한 키를 운용할 수 있으므로 요구사항을 충족하며 운영이 간단합니다.",
    "SelectC": "각 Region에 Customer managed KMS key와 S3 버킷을 생성합니다. 버킷을 SSE-S3로 서버측 암호화하도록 설정합니다. 두 버킷 간 복제를 구성합니다.",
    "SelectC_Commentary": "KMS key는 생성했지만 실제 버킷 암호화는 SSE-S3를 사용하므로 Customer managed key 요구사항에 부합하지 않습니다.",
    "SelectD": "각 Region에 Customer managed KMS key와 S3 버킷을 생성합니다. 버킷을 AWS KMS keys(SSE-KMS)로 서버측 암호화하도록 설정합니다. 두 버킷 간 복제를 구성합니다.",
    "SelectD_Commentary": "일반적인 KMS key는 Region 간 공유가 불가능하므로 동일한 키로 암호화·복호화한다는 조건을 만족하기 어렵습니다."
  },
  {
    "Question_Number": "Q37",
    "Question_Description": "한 회사가 최근에 본인의 AWS account에서 Amazon EC2 인스턴스 위에 다양한 신규 워크로드를 시작했습니다. 회사는 이러한 인스턴스에 원격으로 안전하게 접속하고 관리하기 위한 전략이 필요합니다. 이 프로세스는 반복 가능해야 하고, 네이티브 AWS 서비스를 활용하며, AWS Well-Architected Framework를 준수해야 합니다. 가장 적은 운영 오버헤드로 이러한 요구사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85037-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 Amazon EC2 인스턴스에 대한 원격 액세스를 안전하고 반복 가능하게 설계하는 방법을 묻습니다. AWS Systems Manager Session Manager를 사용하면 인바운드 포트를 열 필요가 없고, SSH 키나 bastion host 관리 부담을 줄여 운영 오버헤드를 최소화할 수 있습니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1"
    ],
    "Keywords": [
      "Amazon EC2 인스턴스",
      "원격 액세스",
      "보안 관리",
      "반복 가능한 프로세스",
      "AWS Well-Architected Framework",
      "운영 오버헤드 최소화",
      "IAM role",
      "Session Manager"
    ],
    "Terms": [
      "Amazon EC2",
      "AWS account",
      "EC2 serial console",
      "IAM role",
      "AWS Systems Manager Session Manager",
      "SSH key pair",
      "bastion host",
      "AWS Site-to-Site VPN",
      "AWS Well-Architected Framework"
    ],
    "SelectA": "각 인스턴스의 EC2 serial console을 사용하여 직접 터미널 인터페이스에 접근해 관리합니다.",
    "SelectA_Commentary": "serial console은 긴급 상황에서 유용하지만 개별 인스턴스마다 직접 접근해야 해 대규모 운영에는 비효율적입니다.",
    "SelectB": "모든 기존 및 신규 인스턴스에 적절한 IAM role을 연결하고, AWS Systems Manager Session Manager로 원격 SSH 세션을 설정합니다.",
    "SelectB_Commentary": "Session Manager는 인바운드 포트나 SSH 키 관리가 필요 없어 운영 부담이 크지 않으며, 보안과 확장성을 동시에 만족합니다.",
    "SelectC": "관리용 SSH key pair를 생성하고 공용 키를 각각의 EC2 인스턴스에 로드합니다. 퍼블릭 서브넷에 bastion host를 두어 터널링 방식으로 인스턴스를 관리합니다.",
    "SelectC_Commentary": "bastion host 운영과 SSH 키 관리를 지속해야 하므로 운영 절차가 복잡하고 오버헤드가 높습니다.",
    "SelectD": "AWS Site-to-Site VPN 연결을 설정하고, 온프레미스 머신에서 SSH 키를 이용해 VPN 터널로 인스턴스에 직접 접속하도록 안내합니다.",
    "SelectD_Commentary": "VPN 구성과 SSH 키 관리, 별도의 네트워크 설정 등이 추가로 필요하여 운영 부담이 높아집니다."
  },
  {
    "Question_Number": "Q38",
    "Question_Description": "한 회사가 Amazon S3를 통해 정적 웹사이트를 호스팅하고, DNS로 Amazon Route 53을 사용하고 있습니다. 전 세계적으로 웹사이트 트래픽이 증가해 사용자 접속 시 지연 시간(latency)을 줄여야 합니다. 가장 비용 효율적인 해결책은 무엇입니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85238-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 전 세계 사용자에게 정적 웹사이트를 빠르게 제공하고 비용을 최소화하기 위한 방안을 묻습니다. Amazon CloudFront를 사용하면 글로벌 엣지 로케이션에 콘텐츠를 캐싱하여 지연 시간을 낮추고, 별도의 복잡한 복제나 추가 인프라가 없어 가장 비용 효율적으로 성능을 개선할 수 있습니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.4"
    ],
    "Keywords": [
      "정적 웹사이트",
      "지연 시간 감소",
      "비용 효율",
      "Amazon S3",
      "Amazon Route 53",
      "Amazon CloudFront"
    ],
    "Terms": [
      "Amazon S3",
      "Amazon Route 53",
      "AWS Global Accelerator",
      "Amazon CloudFront",
      "S3 Transfer Acceleration",
      "Geolocation Routing",
      "DNS",
      "Static Website"
    ],
    "SelectA": "웹사이트를 담고 있는 S3 버킷을 모든 AWS 리전에 복제합니다. Route 53 지리 위치(geolocation) 라우팅 규칙을 추가합니다.",
    "SelectA_Commentary": "모든 리전에 버킷 복제와 geolocation 라우팅 설정은 비용도 높고 구성도 복잡해져 비효율적입니다.",
    "SelectB": "AWS Global Accelerator를 프로비저닝하고 제공된 IP 주소를 해당 S3 버킷과 연결합니다. Route 53 레코드를 Accelerator IP로 수정합니다.",
    "SelectB_Commentary": "Global Accelerator는 추가 인프라와 비용이 늘어나며 CloudFront와 유사한 가속 기능을 중복으로 제공합니다.",
    "SelectC": "S3 버킷 앞에 Amazon CloudFront distribution을 추가하고, Route 53 레코드를 CloudFront distribution으로 수정합니다.",
    "SelectC_Commentary": "CloudFront의 글로벌 캐싱과 엣지 로케이션을 통해 지연 시간을 줄이고 운영 복잡성과 비용도 절감하는 최적의 솔루션입니다.",
    "SelectD": "S3 Transfer Acceleration을 활성화하고, Route 53 레코드를 새로운 엔드포인트로 수정합니다.",
    "SelectD_Commentary": "Transfer Acceleration은 주로 업로드 가속에 유리하며, 정적 웹사이트 전달 지연 시간 개선에는 제한적입니다."
  },
  {
    "Question_Number": "Q39",
    "Question_Description": "한 회사는 웹사이트에서 검색 가능한 아이템 저장소를 운영하고 있습니다. 이 데이터는 1천만 건이 넘는 레코드를 담은 Amazon RDS for MySQL 데이터베이스 테이블에 저장되어 있으며, General Purpose SSD 스토리지를 2TB 사용 중입니다. 회사 웹사이트를 통해 매일 수백만 건의 업데이트가 이루어지는데, 일부 insert 연산이 10초 이상 걸리는 현상을 발견했습니다. 데이터베이스 스토리지 성능이 문제로 확인되었습니다. 다음 중 이 성능 문제를 해결할 솔루션은 무엇입니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/84748-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 스토리지 I/O 성능이 병목이 되어 insert 연산이 지연되는 상황입니다. 높은 IOPS를 보장하는 Provisioned IOPS SSD로 전환하면 일관적이고 예측 가능한 스토리지 성능을 확보할 수 있습니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.1",
      "3.3"
    ],
    "Keywords": [
      "Amazon RDS for MySQL",
      "General Purpose SSD",
      "Provisioned IOPS SSD",
      "insert 연산 지연",
      "스토리지 성능 문제"
    ],
    "Terms": [
      "Amazon RDS for MySQL",
      "General Purpose SSD",
      "Provisioned IOPS SSD",
      "memory optimized instance class",
      "burstable performance instance class",
      "Multi-AZ RDS",
      "MySQL native asynchronous replication",
      "RDS read replicas"
    ],
    "SelectA": "스토리지 타입을 Provisioned IOPS SSD로 변경합니다.",
    "SelectA_Commentary": "Provisioned IOPS SSD는 높은 IOPS를 보장하여 저장 연산 지연을 줄이는 데 가장 효과적입니다.",
    "SelectB": "DB 인스턴스를 memory optimized instance class로 변경합니다.",
    "SelectB_Commentary": "메모리 증가로 캐싱 효과를 기대할 수 있지만, 스토리지 I/O 병목 자체를 해결하지 못하므로 근본적 대안이 아닙니다.",
    "SelectC": "DB 인스턴스를 burstable performance instance class로 변경합니다.",
    "SelectC_Commentary": "버스팅 기능은 주로 CPU 성능 확장에 유리하며, 스토리지 성능 문제 해결과는 직접적인 관련이 없습니다.",
    "SelectD": "Multi-AZ RDS read replicas를 MySQL native asynchronous replication으로 활성화합니다.",
    "SelectD_Commentary": "읽기 성능 확장에는 도움이 되지만, 기본 DB에 대한 쓰기(insert) 성능 개선에는 도움이 되지 않습니다."
  },
  {
    "Question_Number": "Q40",
    "Question_Description": "한 회사는 수천 대의 엣지 디바이스에서 매일 총 1TB의 상태 알림(status alerts)을 생성합니다. 각 알림은 약 2KB 정도의 크기입니다. 이제 솔루션스 아키텍트는 추후 분석을 위해 이러한 알림을 수집하고 저장하는 솔루션을 구축해야 합니다. 회사는 고가용성(highly available)을 원하면서도 비용을 최소화하고 추가 인프라 관리를 원치 않습니다. 또한 14일 동안은 데이터를 즉시 분석하기 위해 사용 가능해야 하며, 14일이 지난 데이터는 보관(archive)해야 합니다. 이 요구사항을 만족하면서 가장 운영 효율적인(MOST operationally efficient) 솔루션은 무엇입니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85204-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 대규모 엣지 디바이스에서 생성되는 상태 알림을 자동으로 수집하고, 14일 동안은 신속하게 분석하며, 이후에는 저비용 스토리지로 보관하는 방안을 찾는 것입니다. Kinesis Data Firehose를 사용하면 완전 관리형 스트리밍 서비스로 알림을 안전하게 S3에 저장하고, Lifecycle 정책을 통해 데이터를 자동으로 Glacier로 이전할 수 있어 운영 효율성과 비용 절감을 모두 달성할 수 있습니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.1",
      "2.2"
    ],
    "Keywords": [
      "엣지 디바이스",
      "알림 데이터",
      "고가용성",
      "코스트 최소화",
      "데이터 수집",
      "14일 보관",
      "장기 보관"
    ],
    "Terms": [
      "Amazon Kinesis Data Firehose",
      "Amazon S3",
      "S3 Lifecycle",
      "Amazon S3 Glacier",
      "Amazon EC2",
      "Elastic Load Balancer",
      "Amazon OpenSearch Service",
      "Amazon Simple Queue Service (Amazon SQS)"
    ],
    "SelectA": "Amazon Kinesis Data Firehose delivery stream을 생성하여 알림을 수집합니다. Kinesis Data Firehose 스트림에서 Amazon S3 버킷으로 알림을 전송하도록 구성합니다. 그리고 S3 Lifecycle 설정을 통해 14일 이후 데이터를 Amazon S3 Glacier로 이전하도록 구성합니다.",
    "SelectA_Commentary": "완전 관리형 스트리밍 서비스인 Kinesis Data Firehose와 S3 Lifecycle 설정만으로 구축 가능해 운영이 간소화되고, 비용도 절감됩니다.",
    "SelectB": "두 개의 가용 영역(Availability Zone)에 Amazon EC2 인스턴스를 띄우고, Elastic Load Balancer 뒤에 두어 알림을 수집합니다. EC2 인스턴스에 스크립트를 만들어 알림을 Amazon S3 버킷에 저장하도록 합니다. 14일 이후에는 S3 Lifecycle 정책을 통해 S3 Glacier로 이전합니다.",
    "SelectB_Commentary": "EC2 인스턴스 관리와 확장, 로드 밸런서 구성 등 추가 인프라가 필요해 운영 복잡도가 높아집니다.",
    "SelectC": "Amazon Kinesis Data Firehose delivery stream을 생성하여 알림을 수집합니다. Kinesis Data Firehose 스트림이 Amazon OpenSearch Service(이전 Amazon Elasticsearch Service) 클러스터로 알림을 전송하도록 구성합니다. OpenSearch Service 클러스터는 매일 수동 스냅샷을 찍고, 14일 이상 된 데이터는 클러스터에서 삭제합니다.",
    "SelectC_Commentary": "OpenSearch Service를 활용하면 검색과 분석이 쉽지만, 14일 이전 데이터 보관을 위해 수동으로 스냅샷을 관리해야 하므로 운영 부담이 큽니다.",
    "SelectD": "Amazon Simple Queue Service(Amazon SQS) 표준 큐를 생성하여 알림을 수집하고, 메시지 보존 기간을 14일로 설정합니다. 컨슈머는 SQS 큐를 폴링하면서 메시지의 연령을 확인해 필요 시 분석하고, 14일이 지난 메시지는 Amazon S3에 복사 후 큐에서 삭제합니다.",
    "SelectD_Commentary": "개발자가 메시지 연령 및 보관 처리를 직접 구현해야 하므로 코드와 인프라 관리가 까다롭습니다."
  },
  {
    "Question_Number": "Q41",
    "Question_Description": "어떤 회사의 애플리케이션은 여러 SaaS(Software-as-a-Service) 소스와 연동되어 데이터를 수집합니다. 회사는 Amazon EC2 인스턴스를 사용하여 데이터를 수신하고 Amazon S3 버킷으로 업로드한 뒤 분석에 활용합니다. 또한 같은 EC2 인스턴스가 업로드 완료 시점에 사용자에게 알림을 발송합니다. 현재 애플리케이션 성능이 저하되어 이를 최대한 개선하고자 합니다. 운영 오버헤드를 최소화하면서 이 요구사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85446-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 SaaS 소스로부터 데이터를 수집해 S3에 업로드하고, 업로드 후 사용자에게 알리는 과정을 성능 저하 없이 진행하려면 어떻게 할지 묻습니다. 이미 Amazon EC2에 구현된 워크로드를 Auto Scaling group으로 확장하고, 알림 로직을 S3 event notification+Amazon SNS로 분리해 병목을 최소화하는 것이 가장 단순하고 효과적인 방법입니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.2"
    ],
    "Keywords": [
      "EC2 인스턴스",
      "S3 버킷",
      "알림",
      "성능 향상",
      "운영 오버헤드",
      "Auto Scaling group",
      "S3 event notification",
      "Amazon SNS"
    ],
    "Terms": [
      "Amazon EC2",
      "Amazon S3",
      "Amazon SNS",
      "Auto Scaling group",
      "Amazon AppFlow",
      "Amazon EventBridge",
      "Docker container",
      "Amazon ECS",
      "Amazon CloudWatch Container Insights"
    ],
    "SelectA": "Amazon EC2 인스턴스가 확장 가능하도록 Auto Scaling group을 구성합니다. Amazon S3 버킷으로 업로드가 완료되면 Amazon S3 event notification을 통해 Amazon SNS 토픽으로 이벤트를 전송하도록 설정합니다.",
    "SelectA_Commentary": "기존 EC2 인스턴스를 유지하면서 확장성을 제공하고, 알림 단계를 S3 event notification으로 분리해 부하를 줄여 성능을 높이는 가장 간단하고 효과적인 솔루션입니다.",
    "SelectB": "각 SaaS 소스와 S3 버킷 간 데이터 전송을 위해 Amazon AppFlow 플로우를 생성합니다. S3 버킷으로 업로드가 완료되면 Amazon S3 event notification으로 Amazon SNS 토픽에 알림을 전송하도록 구성합니다.",
    "SelectB_Commentary": "Amazon AppFlow를 새로 구성하고 장애 시 처리를 고려해야 하므로 운영 오버헤드가 늘어납니다. 이미 있는 EC2 기반 워크로드 확장보다 설정과 관리가 복잡합니다.",
    "SelectC": "각 SaaS 소스별로 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 설정해 데이터를 S3 버킷으로 전송합니다. S3 업로드 완료 시점을 감지하도록 두 번째 EventBridge 규칙을 생성하고, Amazon SNS 토픽을 대상으로 설정합니다.",
    "SelectC_Commentary": "EventBridge 규칙을 소스별로 설정하고 업로드 후 알림을 위한 이중 규칙을 구성해야 하므로, 새로운 인프라 구성이 많아집니다. 운영 오버헤드가 증가합니다.",
    "SelectD": "Docker 컨테이너를 만들어 Amazon EC2 인스턴스 대신 사용하고, Amazon Elastic Container Service(Amazon ECS)에서 애플리케이션을 호스팅합니다. 업로드 완료 알림은 Amazon CloudWatch Container Insights로 Amazon SNS 토픽에 전달되도록 설정합니다.",
    "SelectD_Commentary": "기존 인스턴스를 폐기하고 컨테이너 기반으로 전환하며 추가 모니터링을 구성해야 하므로, 아키텍처 전체를 크게 변경하는 방안입니다. 요구사항 대비 과도하게 복잡합니다."
  },
  {
    "Question_Number": "Q42",
    "Question_Description": "한 회사가 Amazon EC2 인스턴스에서 고가용성 이미지 처리 애플리케이션을 단일 VPC 내부에서 운영하고 있습니다. EC2 인스턴스들은 여러 가용 영역에 걸쳐 다양한 서브넷에 배치되어 있으며, 서로 간에는 통신하지 않습니다. 그러나 모든 EC2 인스턴스는 단일 NAT Gateway를 통해 Amazon S3로부터 이미지를 다운로드하고, Amazon S3로 이미지를 업로드합니다. 회사는 발생하는 데이터 전송 요금에 대해 우려하고 있습니다. 가장 비용 효율적인 방식으로 리전 간 데이터 전송 요금을 회피하기 위해서는 어떤 방법을 사용해야 합니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85205-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 NAT Gateway를 통해 S3로 데이터를 전송할 때 발생하는 비용을 최소화하는 방법을 찾는 것입니다. Gateway VPC Endpoint를 사용하면 VPC와 S3 간 트래픽이 인터넷으로 나가지 않아 전송 요금이 발생하지 않습니다. 따라서 S3에 대한 접근에 있어 가장 비용 효율적인 방식입니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.4"
    ],
    "Keywords": [
      "데이터 전송 요금",
      "NAT Gateway",
      "Amazon S3",
      "Gateway VPC Endpoint"
    ],
    "Terms": [
      "Amazon EC2",
      "VPC",
      "NAT Gateway",
      "Gateway VPC Endpoint for Amazon S3",
      "EC2 Dedicated Host"
    ],
    "SelectA": "각 가용 영역마다 NAT Gateway를 생성합니다.",
    "SelectA_Commentary": "가용 영역마다 NAT Gateway를 배포하면 중복성과 가용성은 높아지지만, NAT Gateway 요금이 여러 개로 늘어나 오히려 비용이 더 증가합니다.",
    "SelectB": "NAT Gateway 대신 NAT Instance를 사용합니다.",
    "SelectB_Commentary": "NAT Instance가 NAT Gateway보다 저렴할 수 있지만, 여전히 인터넷 트래픽 경로가 필요해 데이터 전송 요금 자체를 없애지는 못합니다.",
    "SelectC": "Amazon S3용 Gateway VPC Endpoint를 배포합니다.",
    "SelectC_Commentary": "S3로의 데이터 전송이 인터넷 경로를 거치지 않고 내부 통신으로 처리되어 전송 요금이 발생하지 않으므로 가장 비용 효율적인 솔루션입니다.",
    "SelectD": "EC2 Dedicated Host를 프로비저닝합니다.",
    "SelectD_Commentary": "Dedicated Host는 물리 서버 전용 사용을 위한 옵션으로, 데이터 전송 요금과는 무관하여 비용 절감 효과가 거의 없습니다."
  },
  {
    "Question_Number": "Q43",
    "Question_Description": "한 회사에는 온프레미스 애플리케이션이 있으며 이 애플리케이션은 대규모 시간 민감형 데이터를 생성하여 Amazon S3로 백업합니다. 최근 애플리케이션이 확장되면서 내부 사용자들이 인터넷 대역폭 제약에 대해 불만을 제기하고 있습니다. 솔루션스 아키텍트는 인터넷 연결에 미치는 영향을 최소화하면서도 Amazon S3로 신속하게 백업할 수 있는 장기 솔루션을 설계해야 합니다. 이 요구사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85206-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 대규모 데이터를 S3로 꾸준히 백업하면서, 내부 네트워크 혼잡을 줄이고 백업 속도를 확보하는 방안을 찾는 것입니다. VPN은 여전히 공용 인터넷에 의존하고, Snowball 매일 주문은 효율이 떨어집니다. S3 서비스 제한 해제 역시 대역폭 문제를 해결하지 못합니다. 따라서 전용 네트워크 연결 방식인 AWS Direct Connect가 장기적이고 안정적인 솔루션이 됩니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.4"
    ],
    "Keywords": [
      "온프레미스 백업",
      "시간 민감 데이터",
      "인터넷 대역폭 제약",
      "Amazon S3",
      "AWS Direct Connect"
    ],
    "Terms": [
      "AWS VPN",
      "VPC Gateway Endpoint",
      "AWS Direct Connect",
      "AWS Snowball",
      "S3 Service Limits",
      "Internet Bandwidth"
    ],
    "SelectA": "AWS VPN 연결을 구축하고 모든 트래픽을 VPC Gateway Endpoint를 통해 프록시합니다.",
    "SelectA_Commentary": "VPN은 여전히 인터넷을 사용하여 내부 대역폭 문제를 해결하지 못합니다.",
    "SelectB": "새로운 AWS Direct Connect 연결을 구축하고 백업 트래픽을 이 연결로 전송합니다.",
    "SelectB_Commentary": "전용 회선을 통해 인터넷을 우회하여 빠르고 안정적으로 백업이 가능하므로 요구사항을 충족합니다.",
    "SelectC": "매일 AWS Snowball 디바이스를 주문하여 데이터를 Snowball에 적재 후 AWS로 반환합니다.",
    "SelectC_Commentary": "매일 디바이스를 교환하는 방식은 장기적인 운영 측면에서 비효율적입니다.",
    "SelectD": "AWS Management Console에서 지원 티켓을 제출하여 계정의 S3 서비스 제한을 제거해 달라고 요청합니다.",
    "SelectD_Commentary": "S3 제한 설정을 풀어도 인터넷 대역폭 문제는 해결되지 않습니다."
  },
  {
    "Question_Number": "Q44",
    "Question_Description": "한 회사가 중요한 데이터를 저장한 Amazon S3 버킷을 보유합니다. 회사는 해당 데이터를 실수로 삭제하는 상황을 막아야 합니다. 이를 달성하기 위해 솔루션스 아키텍트는 어떤 두 단계를 수행해야 합니까? (2개 선택)",
    "Answer": "A,B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/84750-exam-aws-certified-solut",
    "AnswerDescription": "Versioning 활성화와 MFA Delete를 통한 2중 안전장치가 실수로 인한 객체 삭제를 방지하는 핵심 전략입니다. 다른 설정들은 보안이나 관리에는 도움이 되지만 삭제 방지를 완벽히 대체하지 못합니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.2",
      "1.3"
    ],
    "Keywords": [
      "Amazon S3 버킷",
      "실수로 인한 삭제 방지",
      "Versioning",
      "MFA Delete",
      "중요 데이터 보호"
    ],
    "Terms": [
      "S3 Versioning",
      "MFA Delete",
      "S3 Bucket Policy",
      "Default Encryption",
      "Lifecycle Policy"
    ],
    "SelectA": "S3 버킷에서 Versioning을 활성화합니다.",
    "SelectA_Commentary": "Versioning을 통해 객체의 이전 버전을 보관해 간단히 복원할 수 있습니다.",
    "SelectB": "S3 버킷에서 MFA Delete를 활성화합니다.",
    "SelectB_Commentary": "삭제 시 추가 인증을 요구해 실수 혹은 무단 삭제를 예방합니다.",
    "SelectC": "S3 버킷에 Bucket Policy를 생성합니다.",
    "SelectC_Commentary": "접근을 세분화할 수 있으나 삭제 방지 기능 자체는 제공하지 않습니다.",
    "SelectD": "S3 버킷에서 기본 암호화를 활성화합니다.",
    "SelectD_Commentary": "데이터 암호화는 보안을 향상하지만 삭제 방지와 직접적 연관이 없습니다.",
    "SelectE": "S3 버킷의 객체에 대해 Lifecycle Policy를 생성합니다.",
    "SelectE_Commentary": "객체 이동 및 만료 관리를 위한 것이며, 실수 삭제 방지 목적과는 다릅니다."
  },
  {
    "Question_Number": "Q45",
    "Question_Description": "한 회사는 다음과 같은 데이터 인제스트 워크플로우를 운영하고 있습니다:\n• 새로운 데이터 전달에 대한 알림을 위한 Amazon Simple Notification Service(Amazon SNS) 토픽\n• 데이터를 처리하고 메타데이터를 기록하기 위한 AWS Lambda 함수\n\n회사는 네트워크 연결 문제로 인해 인제스트 워크플로우가 간헐적으로 실패하는 것을 관찰했습니다. 이러한 실패가 발생하면, 회사가 수동으로 작업을 다시 실행하지 않는 이상 해당 Lambda 함수는 해당 데이터를 처리하지 않습니다.\n앞으로 Lambda 함수가 모든 데이터를 누락 없이 인제스트하도록 보장하기 위해 솔루션스 아키텍트가 취해야 할 조치 조합은 무엇입니까? (2개를 고르시오.)",
    "Answer": "B,E",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85408-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 네트워크 장애로 인한 알림 누락을 방지하기 위해 SNS와 Lambda 사이에 SQS를 도입하고, Lambda가 SQS에서 메시지를 읽도록 설계해 데이터 유실 없이 안정적으로 처리하도록 하는 것입니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1",
      "2.2"
    ],
    "Keywords": [
      "데이터 인제스트",
      "네트워크 연결 문제",
      "Amazon SNS",
      "AWS Lambda",
      "Amazon SQS",
      "재시도",
      "큐 기반 아키텍처"
    ],
    "Terms": [
      "Amazon Simple Notification Service (Amazon SNS)",
      "AWS Lambda",
      "Amazon Simple Queue Service (Amazon SQS)",
      "Availability Zones",
      "CPU",
      "Memory",
      "Provisioned Throughput"
    ],
    "SelectA": "Lambda 함수를 여러 Availability Zone에 배포합니다.",
    "SelectA_Commentary": "Lambda는 이미 다중 AZ로 고가용성을 제공하므로 추가적인 다중 AZ 배포로 네트워크 연결 문제를 근본적으로 해결하기 어렵습니다.",
    "SelectB": "Amazon Simple Queue Service(Amazon SQS) 큐를 생성하고, 해당 큐를 SNS 토픽에 구독시킵니다.",
    "SelectB_Commentary": "SNS에서 온 메시지를 SQS에 저장함으로써 네트워크 장애 시에도 데이터가 큐에 적재되어 유실 없이 처리할 수 있습니다.",
    "SelectC": "Lambda 함수에 할당된 CPU와 메모리를 늘립니다.",
    "SelectC_Commentary": "CPU와 메모리를 늘려도 네트워크 연결 문제 자체를 해결할 수 없으므로 근본적인 대안이 아닙니다.",
    "SelectD": "Lambda 함수의 프로비저닝된 처리량을 늘립니다.",
    "SelectD_Commentary": "프로비저닝된 처리량은 Lambda를 더 자주 혹은 빠르게 실행하기 위한 방식이며, 네트워크 장애로 인한 실패에는 직접적인 해결 효과가 없습니다.",
    "SelectE": "Lambda 함수를 수정하여 Amazon SQS 큐에서 메시지를 읽도록 합니다.",
    "SelectE_Commentary": "Lambda를 SQS 트리거로 동작하도록 구성하면, 장애 발생 시에도 큐에 쌓인 데이터를 재시도할 수 있어 안정적인 데이터 처리 환경을 확보할 수 있습니다."
  },
  {
    "Question_Number": "Q46",
    "Question_Description": "한 회사가 매장에서 발생하는 이전 구매 기록을 기반으로 마케팅 서비스를 제공하는 애플리케이션을 운영하고 있습니다. 매장들은 SFTP를 통해 거래 데이터를 회사로 업로드하며, 업로드된 데이터는 분석 및 처리되어 새로운 마케팅 제안을 생성합니다. 일부 파일은 200GB를 초과할 수 있습니다. 최근, 몇몇 매장에서 포함되어서는 안 될 개인정보(PII)가 업로드된 사실이 확인되었습니다. 회사는 PII가 다시 업로드될 경우 관리자에게 알림을 보내고, 자동으로 조치가 수행되기를 원합니다. 가장 적은 개발 작업으로 이를 만족시키려면 어떻게 해야 합니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85264-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 외부 매장에서 업로드되는 대형 파일에 포함된 PII를 자동으로 탐지하고, 최소한의 개발 노력으로 보안 위협을 방지하는 방법을 묻습니다. Amazon Macie는 S3 객체를 자동으로 스캔하여 PII를 식별할 수 있고, SNS 알림을 통해 관리자가 빠르게 대응할 수 있습니다. 별도의 맞춤 알고리즘 설계 없이 간단히 설정 가능하므로 개발 부담이 적습니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.3"
    ],
    "Keywords": [
      "SFTP",
      "Amazon Macie",
      "PII",
      "자동화된 조치",
      "S3 Lifecycle"
    ],
    "Terms": [
      "SFTP",
      "Amazon S3",
      "Amazon Inspector",
      "Amazon Macie",
      "AWS Lambda",
      "Amazon Simple Notification Service (Amazon SNS)",
      "Amazon Simple Email Service (Amazon SES)",
      "S3 Lifecycle policy"
    ],
    "SelectA": "Amazon S3 버킷을 안전한 전송 지점으로 사용하고, Amazon Inspector로 버킷의 객체를 스캔합니다. PII가 포함된 객체가 발견되면 S3 Lifecycle policy로 해당 객체를 제거하도록 합니다.",
    "SelectA_Commentary": "Amazon Inspector는 취약점 스캐닝 서비스로 PII 탐지를 지원하지 않습니다. 요구사항에 부합하지 않습니다.",
    "SelectB": "Amazon S3 버킷을 안전한 전송 지점으로 사용하고, Amazon Macie로 버킷의 객체를 스캔합니다. PII가 포함되어 있으면 Amazon SNS로 관리자에게 알림을 보내고, 관리자가 해당 객체를 제거합니다.",
    "SelectB_Commentary": "Amazon Macie로 PII를 손쉽게 탐지하고 SNS 알림을 자동화하며, 추가 개발이 최소화됩니다. 자동화된 탐지와 알림으로 신속 대응이 가능합니다.",
    "SelectC": "AWS Lambda 함수에 맞춤형 스캐닝 알고리즘을 구현하고, 객체가 버킷에 업로드될 때 이를 트리거합니다. PII가 포함된 경우 Amazon SNS를 통해 관리자에게 알림을 보내 해당 객체를 제거합니다.",
    "SelectC_Commentary": "직접 스캐너를 개발해야 하므로 개발 노력이 많이 들며, 자동으로 제거가 이뤄지지 않습니다.",
    "SelectD": "AWS Lambda 함수에 맞춤형 스캐닝 알고리즘을 구현하고, 객체가 버킷에 업로드될 때 이를 트리거합니다. PII가 포함된 경우 Amazon SES로 관리자에게 알림을 보내고, S3 Lifecycle policy를 통해 해당 객체를 제거합니다.",
    "SelectD_Commentary": "맞춤 알고리즘 구현과 SES 연동이 필요해 개발 부담이 크며, Macie 활용 대비 간단하지 않습니다."
  },
  {
    "Question_Number": "Q47",
    "Question_Description": "한 회사가 다가오는 1주일간의 이벤트를 위해 특정 AWS Region 내의 세 Availability Zones에서 Amazon EC2 용량을 보장받아야 합니다. 어떤 방법을 사용해야 Amazon EC2 용량을 보장할 수 있습니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85529-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 짧은 기간(1주일) 동안 특정 AWS Region과 세 Availability Zones에서 확실하게 Amazon EC2 용량을 확보하는 방법을 묻습니다. Reserved Instances는 장기적으로 비용을 절감하는 옵션이지만, 즉각적인 용량 보장을 위해서는 On-Demand Capacity Reservation이 적합합니다. 원하는 Region과 Availability Zones를 지정해두면 해당 기간 동안 필요한 EC2 용량이 고정적으로 예약되어 이벤트 트래픽을 안정적으로 처리할 수 있습니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.2"
    ],
    "Keywords": [
      "Amazon EC2",
      "AWS Region",
      "Availability Zones",
      "On-Demand Capacity Reservation",
      "Reserved Instances"
    ],
    "Terms": [
      "Amazon EC2",
      "Availability Zone",
      "AWS Region",
      "On-Demand Capacity Reservation",
      "Reserved Instances"
    ],
    "SelectA": "해당 Region만 지정한 Reserved Instances를 구매합니다.",
    "SelectA_Commentary": "Reserved Instances는 장기 사용 시 비용 절감에 초점이 있어, 기간 한정 이벤트에서 특정 AZ 용량 보장을 확실히 제공하지 못합니다.",
    "SelectB": "해당 Region만 지정한 On-Demand Capacity Reservation을 생성합니다.",
    "SelectB_Commentary": "Region만 지정하면 특정 AZ별로 할당이 보장되지 않아 원하는 세 Availability Zones 모두에 대한 용량을 확실히 보장하기 어렵습니다.",
    "SelectC": "해당 Region과 세 Availability Zones를 지정한 Reserved Instances를 구매합니다.",
    "SelectC_Commentary": "Reserved Instances로 AZ까지 지정은 가능하나, 일반적으로 1주일과 같은 단기 이벤트에는 장기 계약인 Reserved Instances가 비효율적입니다.",
    "SelectD": "해당 Region과 세 Availability Zones를 지정한 On-Demand Capacity Reservation을 생성합니다.",
    "SelectD_Commentary": "정확히 필요한 기간과 AZ를 지정해 필요한 EC2 용량을 즉시 예약하고, 원하는 기간 동안 안정적으로 용량을 확보할 수 있는 최적의 방법입니다."
  },
  {
    "Question_Number": "Q48",
    "Question_Description": "한 회사의 웹사이트는 카탈로그를 Amazon EC2 instance store에 저장하고 있습니다. 회사는 해당 카탈로그를 고가용성으로 유지하고 내구성 높은 위치에 저장하고자 합니다. 이러한 요구 사항을 충족하기 위해 솔루션스 아키텍트는 어떤 조치를 취해야 합니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85119-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 EC2 instance store가 휘발성 스토리지이므로 카탈로그 데이터를 안정적이고 내구성 있는, 그리고 고가용성을 지원하는 스토리지로 이전해야 하는 시나리오입니다. 정답이 Amazon EFS인 이유는 EFS가 멀티 AZ 환경에서의 내구성과 고가용성을 보장하기 때문입니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.2"
    ],
    "Keywords": [
      "카탈로그",
      "고가용성",
      "내구성",
      "Amazon EFS"
    ],
    "Terms": [
      "Amazon EC2 instance store",
      "Amazon EFS",
      "Amazon ElastiCache for Redis",
      "Amazon S3 Glacier Deep Archive"
    ],
    "SelectA": "카탈로그를 Amazon ElastiCache for Redis로 이전합니다.",
    "SelectA_Commentary": "ElastiCache는 인메모리 캐시 서비스로, 영구 저장용도가 아니어서 카탈로그를 내구적으로 보존하기 어렵습니다.",
    "SelectB": "인스턴스 스토리지가 더 큰 EC2 인스턴스로 배포합니다.",
    "SelectB_Commentary": "인스턴스 스토리지는 EC2 인스턴스가 중단될 경우 데이터가 사라질 수 있는 휘발성 스토리지입니다.",
    "SelectC": "인스턴스 스토어의 카탈로그를 Amazon S3 Glacier Deep Archive로 이전합니다.",
    "SelectC_Commentary": "S3 Glacier Deep Archive는 장기 보관에 적합하지만, 즉시 접근이 어려워 고가용성 요구사항에 부합하지 않습니다.",
    "SelectD": "카탈로그를 Amazon Elastic File System(Amazon EFS) 파일 시스템으로 이전합니다.",
    "SelectD_Commentary": "EFS는 멀티 AZ를 통해 뛰어난 내구성과 고가용성을 제공하므로 카탈로그 저장에 가장 적합한 선택입니다."
  },
  {
    "Question_Number": "Q49",
    "Question_Description": "한 회사에서 매달 콜 녹취 파일을 저장하고 있습니다. 사용자들은 콜 후 1년 이내에는 이 파일들을 무작위로 자주 액세스하지만, 1년 이후에는 거의 액세스하지 않습니다. 회사는 1년 미만 된 파일들을 가능한 한 빠르게 조회하고 가져올 수 있도록 최적화하면서, 오래된 파일을 가져오는 데 지연이 있어도 괜찮은 솔루션을 원합니다. 어떤 솔루션이 이러한 요구 사항을 가장 비용 효율적으로 충족할 수 있습니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85211-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 1년 이내에는 자주 접근하고, 이후에는 드물게 접근하는 콜 녹취 파일의 저장 비용을 최적화하는 방법을 묻습니다. S3 Intelligent-Tiering을 활용하면 자동 티어 조정이 가능하고, 1년 뒤에는 S3 Glacier Flexible Retrieval로 전환하여 비용을 절감할 수 있습니다. 특히 필요 시 Amazon Athena나 S3 Glacier Select로 데이터 조회가 가능해 빠른 액세스와 저렴한 보관비용을 모두 만족합니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.1"
    ],
    "Keywords": [
      "콜 녹취 파일",
      "1년 이내 빈번한 조회",
      "1년 후 드문 조회",
      "비용 효율",
      "S3 Intelligent-Tiering",
      "S3 Glacier Flexible Retrieval",
      "Amazon Athena",
      "S3 Glacier Select"
    ],
    "Terms": [
      "S3 Glacier Instant Retrieval",
      "S3 Glacier Flexible Retrieval",
      "S3 Glacier Deep Archive",
      "S3 Intelligent-Tiering",
      "S3 Lifecycle",
      "Amazon Athena",
      "S3 Glacier Select",
      "Amazon RDS"
    ],
    "SelectA": "Amazon S3 Glacier Instant Retrieval에 각 파일을 태그와 함께 저장하고, 태그를 조회하여 파일을 S3 Glacier Instant Retrieval에서 가져옵니다.",
    "SelectA_Commentary": "Instant Retrieval은 검색 속도는 빠르지만 1년 이내 자주 조회되는 데이터에 대해서는 S3 Intelligent-Tiering보다 비용 효율이 떨어집니다.",
    "SelectB": "Amazon S3 Intelligent-Tiering에 각 파일을 저장합니다. 1년 후에는 S3 Lifecycle 정책을 사용하여 S3 Glacier Flexible Retrieval로 객체를 이동시킵니다. Amazon Athena로 S3에 있는 파일을 조회하고, S3 Glacier Select로 Glacier에 있는 파일을 조회합니다.",
    "SelectB_Commentary": "예측이 어려운 액세스 패턴에서 Intelligent-Tiering은 자동으로 비용 최적화 효과를 제공하며, 1년 후 Glacier Flexible Retrieval로 이동하여 가장 효율적입니다.",
    "SelectC": "Amazon S3 Standard 스토리지에 각 파일을 태그와 함께 저장하고, 각 파일의 검색 메타데이터를 Amazon S3 Standard에 별도로 보관합니다. 1년 후에는 S3 Lifecycle 정책을 통해 파일을 S3 Glacier Instant Retrieval로 이동합니다. Amazon S3에서 메타데이터를 검색해 파일을 조회 및 가져옵니다.",
    "SelectC_Commentary": "메타데이터를 이중 관리해야 해서 복잡도가 높고, Glacier Instant Retrieval은 Intelligent-Tiering에 비해 가격이 더 부담됩니다.",
    "SelectD": "Amazon S3 Standard에 각 파일을 저장합니다. 1년 후에는 S3 Glacier Deep Archive로 파일을 이동하도록 S3 Lifecycle 정책을 설정합니다. 검색 메타데이터를 Amazon RDS에 저장합니다. RDS에서 파일을 조회하고 S3 Glacier Deep Archive에서 파일을 가져옵니다.",
    "SelectD_Commentary": "Deep Archive는 보관 비용은 저렴하지만 검색 시 복원 시간이 길고, 별도의 DB를 이용해 메타데이터를 관리해야 해 운영 부담이 큽니다."
  },
  {
    "Question_Number": "Q51",
    "Question_Description": "한 회사가 REST API로 조회할 수 있는 주문 배송 통계 애플리케이션을 개발하고 있습니다. 회사는 매일 아침 정해진 시간에 주문 배송 통계를 추출하여, 데이터를 읽기 쉬운 HTML 형식으로 정리하고 여러 이메일 주소로 보고서를 전송하려고 합니다. 이러한 요구사항을 충족하기 위해 솔루션스 아키텍트는 어떤 단계를 조합해서 수행해야 합니까? (2개를 선택하세요.)",
    "Answer": "B,D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85557-exam-aws-certified-solut",
    "AnswerDescription": "매일 일정 시간에 REST API로부터 데이터를 조회하고, HTML로 변환 후 여러 이메일로 전송해야 합니다. 이를 위해 Amazon EventBridge를 사용해 스케줄링하고, AWS Lambda로 데이터를 불러온 뒤 Amazon SES로 HTML 형식 보고서를 전송하는 구성이 간단하고 효과적입니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1"
    ],
    "Keywords": [
      "주문 배송 통계",
      "HTML 보고서",
      "이메일 전송",
      "EventBridge",
      "Lambda",
      "SES"
    ],
    "Terms": [
      "Amazon Kinesis Data Firehose",
      "Amazon Simple Email Service (Amazon SES)",
      "Amazon EventBridge (Amazon CloudWatch Events)",
      "AWS Glue",
      "AWS Lambda",
      "Amazon S3",
      "Amazon Simple Notification Service (Amazon SNS)"
    ],
    "SelectA": "애플리케이션에서 Amazon Kinesis Data Firehose로 데이터를 전송하도록 구성합니다.",
    "SelectA_Commentary": "Kinesis Data Firehose는 실시간 스트리밍 데이터를 저장·변환할 때 유용하지만, 매일 아침 정기 보고서 전송에는 적합하지 않습니다.",
    "SelectB": "Amazon Simple Email Service(Amazon SES)를 사용하여 데이터를 HTML로 형식화하고 이메일로 보고서를 전송합니다.",
    "SelectB_Commentary": "HTML 형식 보고서 전송을 간편하게 구현할 수 있는 핵심 구성요소이며, 여러 이메일 주소로 동시 전송도 간단합니다. (정답)",
    "SelectC": "Amazon EventBridge(Amazon CloudWatch Events) 스케줄 이벤트를 생성하여 AWS Glue 작업이 애플리케이션의 API에서 데이터를 조회하도록 합니다.",
    "SelectC_Commentary": "AWS Glue는 주로 데이터베이스나 S3 크롤링 및 ETL에 활용됩니다. 단순 API 조회에는 Lambda가 더 간편합니다.",
    "SelectD": "Amazon EventBridge(Amazon CloudWatch Events) 스케줄 이벤트를 생성하여 AWS Lambda 함수를 호출해 애플리케이션의 API에서 데이터를 조회합니다.",
    "SelectD_Commentary": "Lambda를 사용하면 매일 특정 시간에 API에서 데이터를 가져와 HTML 보고서 생성을 위한 준비를 쉽게 할 수 있습니다. (정답)",
    "SelectE": "애플리케이션 데이터를 Amazon S3에 저장합니다. S3 이벤트 대상로 Amazon SNS 토픽을 생성하여 이메일로 보고서를 전송합니다.",
    "SelectE_Commentary": "S3 이벤트 트리거나 SNS만으로는 HTML 형식 변환과 스케줄링 시간이 맞춰진 전송을 구현하기 어렵습니다."
  },
  {
    "Question_Number": "Q52",
    "Question_Description": "한 회사가 온프레미스 애플리케이션을 AWS로 마이그레이션하려고 합니다. 이 애플리케이션은 수십 GB부터 수백 TB까지 다양한 크기의 출력 파일을 생성합니다. 애플리케이션 데이터는 표준 파일 시스템 구조로 저장되어야 합니다. 회사는 자동 확장, 고가용성, 최소한의 운영 오버헤드를 요구하는 솔루션을 찾고 있습니다. 이러한 요구사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85265-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 온프레미스 애플리케이션을 AWS로 이전하면서 수십 GB~수백 TB 규모의 데이터를 표준 파일 시스템 형태로 보관해야 하는 상황입니다. Amazon EFS는 파일 시스템 구조를 제공하면서 자동 확장 및 Multi-AZ 고가용성을 지원하므로, 운영 부담을 줄이기에 가장 적합합니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1",
      "2.2"
    ],
    "Keywords": [
      "마이그레이션",
      "표준 파일 시스템",
      "자동 확장",
      "고가용성",
      "운영 오버헤드 최소화"
    ],
    "Terms": [
      "Amazon ECS",
      "Amazon EKS",
      "Amazon EC2",
      "Amazon EBS",
      "Amazon EFS",
      "Auto Scaling group",
      "Multi-AZ"
    ],
    "SelectA": "애플리케이션을 Amazon ECS 컨테이너로 마이그레이션하고, Amazon S3를 스토리지로 사용합니다.",
    "SelectA_Commentary": "S3는 객체 스토리지로, 표준 파일 시스템 요구사항을 충족하지 못합니다.",
    "SelectB": "애플리케이션을 Amazon EKS 컨테이너로 마이그레이션하고, Amazon EBS를 스토리지로 사용합니다.",
    "SelectB_Commentary": "EBS는 EC2 인스턴스에 종속적이며, 대규모 자동 확장이나 Multi-AZ 가용성 보장 측면에서 제한적입니다.",
    "SelectC": "애플리케이션을 Multi-AZ Auto Scaling 그룹의 Amazon EC2 인스턴스에서 실행하고, Amazon EFS를 스토리지로 사용합니다.",
    "SelectC_Commentary": "표준 파일 시스템 구조, Multi-AZ 고가용성, 자동 확장을 모두 지원하므로 요구사항에 부합하는 최적의 솔루션입니다.",
    "SelectD": "애플리케이션을 Multi-AZ Auto Scaling 그룹의 Amazon EC2 인스턴스에서 실행하고, Amazon EBS를 스토리지로 사용합니다.",
    "SelectD_Commentary": "EBS는 인스턴스 단위로 볼륨을 관리하므로 확장성과 고가용성 면에서 EFS보다 제한적입니다."
  },
  {
    "Question_Number": "Q53",
    "Question_Description": "한 회사가 회계 데이터를 Amazon S3에 저장해야 합니다. 이 데이터는 1년 동안 즉시 접근 가능해야 하며, 이후 9년 동안은 장기 보관해야 합니다. 또한 회사 내 관리자 계정과 root 계정을 포함하여 10년의 전체 보존 기간 동안 아무도 데이터를 삭제할 수 없어야 합니다. 그리고 이 데이터는 최대한 높은 내구성을 가지는 스토리지에 저장되어야 합니다. 이러한 요구 사항을 만족하는 솔루션은 무엇입니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85532-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 회계 데이터에 대한 장기 보존 정책과 삭제 방지 요구사항을 충족해야 하므로, S3 Object Lock의 compliance mode와 Lifecycle 정책을 활용하는 방법이 핵심입니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.3"
    ],
    "Keywords": [
      "회계 데이터 저장",
      "10년 보존",
      "즉시 접근 1년",
      "장기 보관 9년",
      "S3 Object Lock",
      "compliance mode",
      "S3 Glacier Deep Archive"
    ],
    "Terms": [
      "Amazon S3",
      "S3 Glacier",
      "S3 Intelligent-Tiering",
      "S3 Glacier Deep Archive",
      "S3 Object Lock",
      "compliance mode",
      "governance mode",
      "S3 Lifecycle policy",
      "IAM policy"
    ],
    "SelectA": "S3 Glacier에 10년 전체 기간 동안 레코드를 저장합니다. 10년 동안 레코드 삭제를 거부하도록 하는 access control policy를 사용합니다.",
    "SelectA_Commentary": "S3 Glacier만 사용하고 policy로만 삭제를 막는 방식은 root나 관리자 권한을 완전히 제한할 수 없고, 1년간 즉시 접근 요건에도 부적합합니다.",
    "SelectB": "S3 Intelligent-Tiering을 사용하여 레코드를 저장합니다. IAM policy로 10년 동안 삭제를 거부한 뒤, 10년 후 정책을 변경합니다.",
    "SelectB_Commentary": "IAM policy만으로는 최상위 권한 계정의 삭제 방지를 완벽히 보장하기 어렵고, 별도의 장기 보관 방식도 고려되지 않아 요구사항에 부합하지 않습니다.",
    "SelectC": "1년 후 S3 Standard에서 S3 Glacier Deep Archive로 전환하는 S3 Lifecycle policy를 설정하고, S3 Object Lock의 compliance mode를 10년간 적용합니다.",
    "SelectC_Commentary": "compliance mode는 root나 관리자도 삭제 못 하게 하며, 1년 즉시 접근 후 9년 장기 보관 요구사항을 모두 만족시키는 올바른 솔루션입니다.",
    "SelectD": "1년 후 S3 Standard에서 S3 One Zone-Infrequent Access로 전환하는 S3 Lifecycle policy를 사용하고, S3 Object Lock의 governance mode를 10년간 적용합니다.",
    "SelectD_Commentary": "governance mode는 적절한 권한을 가진 사용자가 삭제를 해제할 수 있어, root 계정 등도 삭제를 막지 못하는 위험이 있어 요구사항에 부합하지 않습니다."
  },
  {
    "Question_Number": "Q54",
    "Question_Description": "한 회사가 AWS에서 여러 Windows 워크로드를 운영하고 있습니다. 회사의 직원들은 두 개의 Amazon EC2 인스턴스에 호스팅된 Windows file shares를 사용합니다. 해당 file share들은 서로 간 데이터를 동기화하며 중복 사본을 유지하고 있습니다. 회사는 현재 사용자가 파일에 접근하는 방식을 유지하면서, 고가용성(High Availability)과 내구성(Durability)을 갖춘 스토리지 솔루션을 원합니다. 이를 만족하기 위한 솔루션은 무엇입니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85574-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 Windows 환경 특유의 SMB 기반 파일 공유 방식을 유지하면서, 고가용성과 내구성을 높이는 스토리지를 선택하는 것입니다. Amazon FSx for Windows File Server는 Windows 네이티브 프로토콜을 완벽히 지원하므로 요구사항을 모두 충족합니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.2"
    ],
    "Keywords": [
      "Windows file shares",
      "고가용성",
      "내구성",
      "FSx for Windows File Server",
      "Multi-AZ"
    ],
    "Terms": [
      "Amazon EC2",
      "Windows file shares",
      "Amazon S3",
      "IAM",
      "S3 File Gateway",
      "Amazon FSx for Windows File Server",
      "Multi-AZ",
      "Amazon EFS"
    ],
    "SelectA": "모든 데이터를 Amazon S3로 마이그레이션하고 사용자가 파일에 접근할 수 있도록 IAM 인증을 설정합니다.",
    "SelectA_Commentary": "S3는 Windows의 SMB 프로토콜을 그대로 지원하지 않으므로 사용자 측 접근 방식을 유지하기 어렵습니다.",
    "SelectB": "Amazon S3 File Gateway를 설정하고, 기존 EC2 인스턴스에 S3 File Gateway를 마운트합니다.",
    "SelectB_Commentary": "S3 File Gateway는 파일 공유를 위한 일부 기능을 제공하지만, Windows ACL 등 완전한 Windows 파일 공유 호환성을 보장하지 못합니다.",
    "SelectC": "Amazon FSx for Windows File Server 환경을 Multi-AZ로 확장하고, 모든 데이터를 FSx for Windows File Server로 마이그레이션합니다.",
    "SelectC_Commentary": "Amazon FSx for Windows File Server는 Windows 네이티브 파일 시스템과 호환되어 고가용성과 내구성을 제공하므로 요구사항에 가장 적합한 솔루션입니다.",
    "SelectD": "Amazon Elastic File System(Amazon EFS)에 Multi-AZ 구성을 추가하고, 모든 데이터를 EFS로 마이그레이션합니다.",
    "SelectD_Commentary": "Amazon EFS는 Linux 기반 파일 시스템으로, Windows 파일 공유 방식(SMB)에 대한 지원이 불가능하므로 적합하지 않습니다."
  },
  {
    "Question_Number": "Q55",
    "Question_Description": "한 솔루션스 아키텍트가 여러 서브넷을 포함하는 VPC 아키텍처를 설계하고 있습니다. 이 아키텍처는 Amazon EC2 인스턴스와 Amazon RDS DB 인스턴스를 사용하며, 두 개의 가용 영역(Availability Zone)에 걸쳐 총 6개의 서브넷으로 구성됩니다. 각 가용 영역에는 Public Subnet, Private Subnet, Database용 전용 Subnet이 각각 존재합니다. Private Subnet에서 실행 중인 EC2 인스턴스만이 RDS Database에 액세스할 수 있어야 합니다. 이 요구 사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85409-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 서로 다른 서브넷 간 트래픽을 제어하여 Private Subnet에서만 RDS DB에 접근하도록 하는 방법을 묻습니다. 보안 그룹(Security Group)은 기본적으로 허용 규칙만 설정 가능하며, 원하는 소스(Private Subnet의 인스턴스)에 대해서만 Inbound 허용 규칙을 생성하여 트래픽을 제한하는 것이 핵심입니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1"
    ],
    "Keywords": [
      "VPC 아키텍처",
      "Amazon EC2",
      "Amazon RDS",
      "서브넷 구성",
      "Private Subnet",
      "데이터베이스 접근 제어"
    ],
    "Terms": [
      "VPC",
      "EC2",
      "RDS",
      "Security Group",
      "Route Table",
      "CIDR block",
      "VPC Peering"
    ],
    "SelectA": "Public Subnet의 CIDR 블록으로 가는 라우트를 제외한 새로운 Route Table을 생성하고, Database Subnet에 연결합니다.",
    "SelectA_Commentary": "Route Table만으로는 Database Subnet의 Inbound 흐름 제어를 완전히 해결하지 못하므로 적절한 접근 제한에 부족합니다.",
    "SelectB": "Public Subnet에 할당된 인스턴스의 Security Group으로부터 들어오는 트래픽을 거부하는 Security Group을 생성하고, DB 인스턴스에 할당합니다.",
    "SelectB_Commentary": "Security Group에서는 거부(Deny) 규칙이 불가능하므로 이 방법은 구현할 수 없습니다.",
    "SelectC": "Private Subnet에 할당된 인스턴스의 Security Group으로부터 들어오는 트래픽을 허용하는 Security Group을 생성하고, DB 인스턴스에 할당합니다.",
    "SelectC_Commentary": "Security Group은 기본적으로 허용 규칙만 설정 가능하므로, Private Subnet 인스턴스만 접근 허용 규칙을 두면 요구 사항을 충족합니다.",
    "SelectD": "Public Subnet과 Private Subnet 간 새로운 Peering Connection을 생성하고, Private Subnet과 Database Subnet 간 별도의 Peering Connection을 생성합니다.",
    "SelectD_Commentary": "VPC Peering은 주로 서로 다른 VPC 간 트래픽을 연결하기 위한 것이며, 같은 VPC 내 Subnet 간 트래픽 제어에는 적합하지 않습니다."
  },
  {
    "Question_Number": "Q56",
    "Question_Description": "한 회사가 Amazon Route 53을 통해 도메인 이름을 등록했습니다. 회사는 ca-central-1 리전에서 Amazon API Gateway를 퍼블릭 인터페이스로 사용하여 백엔드 마이크로서비스 API를 제공하고 있으며, 서드파티 서비스들이 보안 연결을 통해 API를 소비하고 있습니다. 회사는 서드파티 서비스가 HTTPS를 사용할 수 있도록, 회사의 도메인 이름과 해당 인증서를 사용해 API Gateway URL을 구성하고자 합니다. 이 요구사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85266-exam-aws-certified-solut",
    "AnswerDescription": "API Gateway에 커스텀 도메인 이름과 HTTPS를 적용하려면, 동일한 리전 내 ACM 인증서와 Regional 엔드포인트를 사용해야 합니다. 그 후 Route 53 레코드를 통해 트래픽을 해당 도메인으로 라우팅하면 HTTPS 연결이 완성됩니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1",
      "1.2"
    ],
    "Keywords": [
      "API Gateway",
      "도메인 이름",
      "ACM 인증서",
      "HTTPS",
      "Regional 엔드포인트"
    ],
    "Terms": [
      "Amazon Route 53",
      "Amazon API Gateway",
      "ca-central-1 Region",
      "AWS Certificate Manager (ACM)",
      "Route 53 DNS records",
      "Regional API Gateway endpoint",
      "Stage variables",
      "Public certificate",
      "HTTPS",
      "A record",
      "Alias record"
    ],
    "SelectA": "API Gateway에서 Name=\"Endpoint-URL\"와 Value=\"Company Domain Name\"을 갖는 Stage Variable을 생성하여 기본 URL을 덮어씁니다. 회사 도메인 이름에 대한 Public Certificate를 AWS Certificate Manager (ACM)에 Import합니다.",
    "SelectA_Commentary": "Stage Variable만으로 API Gateway에 커스텀 도메인을 설정할 수 없으므로 올바른 설정 방식이 아닙니다.",
    "SelectB": "회사 도메인 이름을 사용해 Route 53 DNS 레코드를 생성합니다. Alias 레코드를 Regional API Gateway 스테이지 엔드포인트로 지정합니다. 회사 도메인 이름 관련 Public Certificate를 us-east-1 리전의 AWS Certificate Manager (ACM)에 Import합니다.",
    "SelectB_Commentary": "Regional 엔드포인트이지만 인증서를 us-east-1 리전에 배포하면 ca-central-1 리전의 API Gateway와 정상 연동이 어려워 잘못된 접근입니다.",
    "SelectC": "Regional API Gateway 엔드포인트를 생성합니다. API Gateway 엔드포인트를 회사 도메인 이름과 연결합니다. 동일 리전에서 회사 도메인 이름에 대한 Public Certificate를 AWS Certificate Manager (ACM)에 Import하고, 이를 API Gateway 엔드포인트에 연결합니다. Route 53을 구성하여 트래픽을 API Gateway 엔드포인트로 라우팅합니다.",
    "SelectC_Commentary": "동일 리전의 ACM 인증서와 Regional 엔드포인트가 필요하며, Route 53을 통해 트래픽을 연결하는 설정으로 요구사항을 충족합니다.",
    "SelectD": "Regional API Gateway 엔드포인트를 생성합니다. 회사 도메인 이름과 엔드포인트를 연결합니다. 회사 도메인 이름에 대한 Public Certificate를 us-east-1 리전의 AWS Certificate Manager (ACM)에 Import하고, 이를 API Gateway에 연결합니다. 회사 도메인 이름으로 A 레코드를 생성하여 해당 도메인으로 지정합니다.",
    "SelectD_Commentary": "인증서를 API Gateway 엔드포인트와 동일한 리전에 배포해야 하는데, us-east-1에 Import하면 ca-central-1 엔드포인트와 호환되지 않습니다."
  },
  {
    "Question_Number": "Q57",
    "Question_Description": "한 회사에서 인기가 높은 소셜 미디어 웹사이트를 운영 중이며, 사용자가 다른 사용자와 공유할 이미지를 업로드할 수 있는 기능을 제공합니다. 회사는 이러한 이미지에 부적절한 콘텐츠가 포함되지 않도록 사전에 확인하고자 합니다. 또한, 개발 노력을 최소화할 수 있는 솔루션을 원합니다. 이를 충족하는 방법으로 어떤 것을 선택해야 합니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85452-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 사용자가 업로드하는 사진을 자동으로 점검해 부적절한 콘텐츠를 걸러내는 요구 사항에 관한 것입니다. Amazon Rekognition은 이미지를 기반으로 한 콘텐츠 모더레이션 기능을 이미 갖추고 있어 개발 부담을 최소화하며 정확도를 높일 수 있는 최적의 서비스입니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.2"
    ],
    "Keywords": [
      "소셜 미디어",
      "이미지 업로드",
      "부적절한 콘텐츠",
      "개발 노력 최소화",
      "Amazon Rekognition"
    ],
    "Terms": [
      "Amazon Comprehend",
      "Amazon Rekognition",
      "Amazon SageMaker",
      "AWS Fargate",
      "Ground Truth",
      "Human Review"
    ],
    "SelectA": "Amazon Comprehend를 사용해 부적절한 내용을 감지하고, 신뢰도가 낮은 예측에 대해서는 인적 리뷰를 진행",
    "SelectA_Commentary": "Comprehend는 주로 텍스트 분석에 특화되어 있으므로 이미지 부적절성 검출에는 적합하지 않습니다.",
    "SelectB": "Amazon Rekognition을 사용해 부적절한 내용을 감지하고, 신뢰도가 낮은 예측에 대해서는 인적 리뷰를 진행",
    "SelectB_Commentary": "이미지 분석에 최적화된 Amazon Rekognition을 사용하면 최소한의 개발 노력으로 정확하고 빠른 콘텐츠 모더레이션을 구현할 수 있는 최적의 선택입니다.",
    "SelectC": "Amazon SageMaker로 부적절한 내용을 감지하고, Ground Truth를 사용해 신뢰도가 낮은 예측에 라벨을 지정",
    "SelectC_Commentary": "직접 모델을 개발하고 학습해야 하므로 개발 과정이 복잡하고 시간과 비용이 많이 듭니다.",
    "SelectD": "AWS Fargate에 맞춤형 머신 러닝 모델을 배포해 부적절한 내용을 감지하고, Ground Truth를 사용해 신뢰도가 낮은 예측에 라벨을 지정",
    "SelectD_Commentary": "커스텀 모델을 배포해 운영하려면 상당한 개발 및 유지보수 노력이 필요하며, 요구 사항인 개발 노력 최소화에 부적합합니다."
  },
  {
    "Question_Number": "Q58",
    "Question_Description": "한 회사가 컨테이너로 크리티컬 애플리케이션을 구동하여 확장성과 가용성 요구사항을 충족하고자 합니다. 회사는 크리티컬 애플리케이션 유지보수에만 집중하기를 원하며, 컨테이너화된 워크로드가 동작하는 기본 인프라를 프로비저닝하고 관리하는 책임을 지고 싶어 하지 않습니다. 이 요구사항을 충족하기 위해 솔루션스 아키텍트는 어떤 조치를 취해야 합니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85453-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 확장성과 고가용성을 위해 컨테이너화된 애플리케이션을 운영하면서, 인프라 관리까지 맡지 않으려는 상황에 대한 해결책을 찾는 것입니다. AWS Fargate는 서버리스 방식으로 기본 인프라 관리를 자동화하고, 애플리케이션 유지보수에 집중할 수 있도록 지원하기 때문에 적합합니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1",
      "2.2"
    ],
    "Keywords": [
      "크리티컬 애플리케이션",
      "컨테이너",
      "확장성",
      "가용성",
      "서버리스",
      "AWS Fargate"
    ],
    "Terms": [
      "AWS Fargate",
      "Amazon ECS",
      "Amazon EC2",
      "Docker",
      "Amazon Machine Image (AMI)"
    ],
    "SelectA": "Amazon EC2 인스턴스를 사용하고, 인스턴스에 Docker를 설치합니다.",
    "SelectA_Commentary": "서버 설정 및 Docker 설치·관리가 필요하여 인프라 관리를 전담해야 하므로 요구사항에 부적합합니다.",
    "SelectB": "Amazon ECS를 Amazon EC2 worker 노드에서 사용합니다.",
    "SelectB_Commentary": "EC2 노드를 직접 관리해야 하므로 인프라 관리 부담이 그대로 남아있어 요구사항에 맞지 않습니다.",
    "SelectC": "Amazon ECS를 AWS Fargate에서 사용합니다.",
    "SelectC_Commentary": "서버리스 환경으로 인프라 관리가 자동화되어 크리티컬 애플리케이션 유지보수에만 집중할 수 있는 최적의 해법입니다.",
    "SelectD": "Amazon ECS 최적화 Amazon Machine Image (AMI)를 사용하여 Amazon EC2 인스턴스에서 구동합니다.",
    "SelectD_Commentary": "AMI를 사용해도 EC2 인스턴스 프로비저닝·관리가 필요해 요구사항을 충분히 만족시키지 못합니다."
  },
  {
    "Question_Number": "Q59",
    "Question_Description": "한 회사가 300개 이상의 글로벌 웹사이트와 애플리케이션을 운영하고 있습니다. 이 회사는 매일 30TB 이상의 clickstream 데이터를 분석할 플랫폼이 필요합니다. 솔루션스 아키텍트는 이러한 clickstream 데이터를 전송하고 처리하기 위해 무엇을 해야 합니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85793-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 대규모(30TB 이상) clickstream 데이터를 매일 빠르고 안정적으로 수집, 전송, 처리, 분석해야 하는 시나리오입니다. Amazon Kinesis Data Streams와 Kinesis Data Firehose를 사용하면 실시간 스트리밍 데이터 수집 및 저장소 전송이 용이하고, Amazon Redshift를 통해 대규모 데이터 웨어하우징과 분석을 수행할 수 있습니다. 이는 고성능 아키텍처 설계를 위한 핵심 전략입니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.3",
      "3.5"
    ],
    "Keywords": [
      "글로벌 웹사이트",
      "애플리케이션",
      "clickstream 데이터",
      "데이터 분석",
      "Amazon Kinesis Data Streams",
      "Amazon Kinesis Data Firehose",
      "Amazon S3",
      "Amazon Redshift"
    ],
    "Terms": [
      "AWS Data Pipeline",
      "Amazon EMR",
      "Auto Scaling group",
      "Amazon EC2",
      "Amazon CloudFront",
      "AWS Lambda",
      "Amazon Kinesis Data Streams",
      "Amazon Kinesis Data Firehose",
      "Amazon S3 data lake",
      "Amazon Redshift"
    ],
    "SelectA": "AWS Data Pipeline을 설계하여 Amazon S3 버킷에 데이터를 보관하고 Amazon EMR 클러스터에서 분석을 진행합니다.",
    "SelectA_Commentary": "AWS Data Pipeline과 Amazon EMR은 배치 기반 분석에는 적합하지만, 대규모 실시간 clickstream 처리를 위한 전송 및 스트리밍 측면에서 적절하지 않습니다.",
    "SelectB": "Amazon EC2 인스턴스로 구성된 Auto Scaling group에서 데이터를 처리하고 Amazon S3 data lake로 전송한 뒤 Amazon Redshift로 분석합니다.",
    "SelectB_Commentary": "EC2를 통한 직접 처리는 확장성 및 실시간 처리 측면에서 추가 설정이 복잡하고 대규모 스트리밍 처리 효율이 떨어질 수 있습니다.",
    "SelectC": "데이터를 Amazon CloudFront에 캐싱하고 Amazon S3 버킷에 저장합니다. 객체가 S3 버킷에 올라오면 AWS Lambda 함수를 실행해 분석용으로 처리합니다.",
    "SelectC_Commentary": "CloudFront 캐싱은 주로 콘텐츠 전달을 위한 기능이며, 매일 30TB 이상의 실시간 clickstream 데이터를 효과적으로 스트리밍 처리하기에는 적합하지 않습니다.",
    "SelectD": "Amazon Kinesis Data Streams에서 데이터를 수집하고 Amazon Kinesis Data Firehose를 사용해 데이터를 Amazon S3 data lake로 전송합니다. 데이터를 Amazon Redshift에 로드하여 분석을 수행합니다.",
    "SelectD_Commentary": "대규모 실시간 스트리밍 처리에 특화된 Amazon Kinesis와 확장성이 뛰어난 Kinesis Data Firehose 전송, 그리고 분석을 위한 Amazon Redshift 결합은 가장 효율적인 솔루션입니다."
  },
  {
    "Question_Number": "Q61",
    "Question_Description": "한 회사가 AWS에서 2티어 웹 애플리케이션을 개발 중입니다. 이 회사의 개발자들은 백엔드 Amazon RDS 데이터베이스와 직접 연결되는 Amazon EC2 인스턴스에 애플리케이션을 배포했습니다. 회사는 애플리케이션에 데이터베이스 자격 증명을 하드코딩하면 안 되며, 정기적으로 데이터베이스 자격 증명을 자동으로 로테이션하는 솔루션을 구현해야 합니다. 가장 적은 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85580-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 애플리케이션 코드에 하드코딩 없이 데이터베이스 자격 증명을 안전하게 관리하고, 정기적으로 인증 정보를 갱신하기 위한 접근 방안을 묻습니다. AWS Secrets Manager는 자격 증명 보안을 자동화하여 처리하며, 자동 로테이션까지 제공해 운영 오버헤드를 크게 줄일 수 있으므로 가장 적합한 솔루션입니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1",
      "1.3"
    ],
    "Keywords": [
      "데이터베이스 자격 증명",
      "자동 로테이션",
      "Amazon EC2",
      "Amazon RDS",
      "운영 오버헤드 최소화",
      "AWS Secrets Manager"
    ],
    "Terms": [
      "Amazon EC2",
      "Amazon RDS",
      "AWS Lambda",
      "Amazon EventBridge (Amazon CloudWatch Events)",
      "Amazon S3",
      "AWS Secrets Manager",
      "AWS Systems Manager Parameter Store",
      "자동 로테이션 (Automatic Rotation)",
      "인스턴스 메타데이터"
    ],
    "SelectA": "데이터베이스 자격 증명을 인스턴스 메타데이터에 저장합니다. Amazon EventBridge(Amazon CloudWatch Events) 규칙을 사용해 예약된 AWS Lambda 함수를 실행하여 RDS 자격 증명과 인스턴스 메타데이터를 동시에 업데이트합니다.",
    "SelectA_Commentary": "인스턴스 메타데이터에 자격 증명을 저장하는 것은 보안상 위험이 높습니다. 또한 EventBridge와 Lambda를 사용해 수동으로 로테이션 로직을 구현해야 하므로 운영 부담이 큽니다.",
    "SelectB": "암호화된 Amazon S3 버킷의 설정 파일에 데이터베이스 자격 증명을 저장합니다. Amazon EventBridge(Amazon CloudWatch Events) 규칙을 사용해 예약된 AWS Lambda 함수를 실행하여 RDS 자격 증명과 설정 파일 내 자격 증명을 동시에 업데이트합니다. S3 Versioning으로 이전 버전 복구 기능을 보장합니다.",
    "SelectB_Commentary": "S3에 직접 자격 증명을 저장하면 자동 로테이션을 위한 로직을 추가로 개발해야 하므로 운영 오버헤드가 큽니다. Versioning으로 이전 상태 복구가 가능하지만, 별도의 구성과 유지 비용이 발생합니다.",
    "SelectC": "데이터베이스 자격 증명을 AWS Secrets Manager의 시크릿으로 저장합니다. 시크릿 자동 로테이션을 활성화합니다. EC2 역할에 해당 시크릿에 대한 액세스 권한을 부여합니다.",
    "SelectC_Commentary": "AWS Secrets Manager는 자동으로 RDS 자격 증명을 로테이션하며, EC2 역할에 권한만 부여하면 되므로 운영 오버헤드가 매우 적습니다. 보안 및 편의성 모두를 만족하는 솔루션입니다.",
    "SelectD": "데이터베이스 자격 증명을 AWS Systems Manager Parameter Store의 암호화된 파라미터로 저장합니다. 암호화된 파라미터에 대한 자동 로테이션을 활성화합니다. EC2 역할에 해당 파라미터들에 대한 액세스 권한을 부여합니다.",
    "SelectD_Commentary": "Parameter Store는 기본적으로 자격 증명 자동 로테이션 기능을 제공하지 않거나 제한적이므로, Secrets Manager처럼 간편하고 완전한 자동 로테이션을 지원하지 않아 운영 부담이 늘어납니다."
  },
  {
    "Question_Number": "Q62",
    "Question_Description": "한 회사가 새로운 Public Web Application을 AWS에 배포하려고 합니다. 이 Application은 Application Load Balancer(ALB) 뒤에서 동작합니다. 외부 Certificate Authority(CA)가 발급한 SSL/TLS Certificate를 사용하여 엣지에서 암호화해야 하며, 해당 Certificate는 만료 전에 매년 교체(Rotate)되어야 합니다. 이러한 요구사항을 충족하기 위해서는 어떻게 해야 합니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85524-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 외부 CA가 발급한 SSL/TLS 인증서를 ALB에 적용해야 하며, 자동 갱신이 불가능하다는 점이 핵심입니다. AWS가 직접 관리하지 않는 제3자 인증서는 만료 알림을 받은 뒤 수동으로 교체해야 하므로, EventBridge를 통한 알림 후 수동 갱신이 유일한 실현 방법입니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1",
      "1.2"
    ],
    "Keywords": [
      "Application Load Balancer(ALB)",
      "SSL/TLS Certificate",
      "외부 Certificate Authority(CA)",
      "만료 전 교체",
      "ACM Import",
      "Amazon EventBridge"
    ],
    "Terms": [
      "Application Load Balancer(ALB)",
      "SSL/TLS Certificate",
      "External Certificate Authority(CA)",
      "AWS Certificate Manager(ACM)",
      "ACM Private Certificate Authority",
      "Amazon EventBridge(Amazon CloudWatch Events)",
      "Managed Renewal"
    ],
    "SelectA": "AWS Certificate Manager(ACM)을 사용하여 SSL/TLS Certificate를 발급받습니다. 그리고 이 인증서를 ALB에 적용합니다. 이후 Managed Renewal 기능을 통해 자동으로 인증서를 교체합니다.",
    "SelectA_Commentary": "AWS 자체 인증서를 활용하므로 외부 CA 요구사항을 충족하지 못하며, 3rd party 인증서는 자동 갱신 대상이 아닙니다.",
    "SelectB": "AWS Certificate Manager(ACM)을 사용하여 SSL/TLS Certificate를 발급받고, 키 재질을 가져옵니다. 그리고 이 인증서를 ALB에 적용합니다. 이후 Managed Renewal 기능을 통해 자동으로 인증서를 교체합니다.",
    "SelectB_Commentary": "역시 ACM이 발급한 인증서이므로, 외부 CA에서 발급된 인증서를 자동 갱신하는 방식이 아닙니다.",
    "SelectC": "AWS Certificate Manager(ACM) Private Certificate Authority를 사용하여 루트 CA로부터 SSL/TLS Certificate를 발급받습니다. 그리고 이 인증서를 ALB에 적용합니다. 이후 Managed Renewal 기능을 통해 인증서를 자동 교체합니다.",
    "SelectC_Commentary": "ACM Private CA를 사용하면 내부 CA가 되므로, 외부 CA를 요구하는 시나리오와 맞지 않습니다.",
    "SelectD": "AWS Certificate Manager(ACM)에 외부 SSL/TLS Certificate를 Import합니다. ALB에 해당 인증서를 적용합니다. 그리고 Amazon EventBridge(Amazon CloudWatch Events)를 통해 인증서 만료가 가까워지면 알림을 받고 수동으로 인증서를 교체합니다.",
    "SelectD_Commentary": "외부 CA에서 발급된 인증서는 AWS가 자동 갱신할 수 없습니다. EventBridge 알림으로 만료 시점을 파악하여 직접 교체하는 방식이 유일한 방법이므로 정답입니다."
  },
  {
    "Question_Number": "Q63",
    "Question_Description": "회사는 AWS에서 인프라를 운영하고 있으며, 문서 관리 애플리케이션을 사용 중인 700,000명의 사용자 기반을 보유하고 있습니다. 회사는 큰 용량의 .pdf 파일을 .jpg 이미지 파일로 변환하는 제품을 만들 계획입니다. .pdf 파일은 평균적으로 5MB 정도의 크기를 갖습니다. 회사는 원본 파일과 변환된 파일을 모두 저장해야 합니다. 한 Solutions Architect는 빠르게 증가하는 수요를 수용할 수 있는 확장 가능한 솔루션을 설계해야 합니다. 가장 비용 효율적인 솔루션은 무엇입니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85795-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 많은 사용자의 대규모 PDF 파일을 JPG로 변환하고, 원본과 변환본을 모두 저장해야 하는 상황에서 가장 비용 효율적이면서도 확장성이 높은 방법을 묻습니다. Amazon S3에 업로드 시 S3 PUT event로 트리거되는 AWS Lambda 함수를 통해 파일을 변환하고 S3에 재저장하는 방식이 서버 관리 부담 없이 비용 또한 절감할 수 있어 정답이 됩니다. DynamoDB나 Elastic Beanstalk 등을 사용하는 방식은 파일 크기와 비용 효율성 측면에서 적합하지 않습니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.1",
      "4.2"
    ],
    "Keywords": [
      "PDF 파일 변환",
      "비용 효율성",
      "대규모 사용자",
      "Amazon S3",
      "AWS Lambda",
      "S3 PUT event",
      "원본 파일 저장",
      "확장 가능한 솔루션"
    ],
    "Terms": [
      "Amazon S3",
      "AWS Lambda",
      "S3 PUT event",
      "DynamoDB",
      "DynamoDB Streams",
      "AWS Elastic Beanstalk",
      "Amazon EC2",
      "Amazon EBS",
      "Amazon EFS",
      "Auto Scaling"
    ],
    "SelectA": "Amazon S3에 .pdf 파일을 저장합니다. S3 PUT event를 통해 파일이 업로드될 때마다 AWS Lambda 함수를 호출하여 .jpg로 변환하고, 변환된 파일을 다시 Amazon S3에 저장합니다.",
    "SelectA_Commentary": "Lambda와 S3를 사용하면 파일 변환을 이벤트 기반으로 처리하고, 서버less 구조로 관리 부담 없이 비용 효율적인 스토리지와 컴퓨팅 환경을 확보할 수 있습니다.",
    "SelectB": "DynamoDB에 .pdf 파일을 저장합니다. DynamoDB Streams 기능을 사용해 AWS Lambda 함수를 호출하여 파일을 .jpg로 변환하고, 변환된 파일을 다시 DynamoDB에 저장합니다.",
    "SelectB_Commentary": "DynamoDB는 대규모 파일 저장에 적합하지 않으며, 파일을 직접 저장하면 비용이 크게 증가할 수 있습니다.",
    "SelectC": "AWS Elastic Beanstalk 애플리케이션(EC2 인스턴스, Amazon EBS 스토리지, Auto Scaling 포함)에 .pdf 파일을 업로드합니다. EC2 인스턴스에서 프로그램을 사용해 .jpg로 변환한 뒤, .pdf와 .jpg 파일을 EBS에 저장합니다.",
    "SelectC_Commentary": "EC2와 EBS를 이용하면 서버와 스토리지를 직접 관리해야 하며, 확장성과 비용 효율성 측면에서 Lambda+S3 조합보다 부담이 큽니다.",
    "SelectD": "AWS Elastic Beanstalk 애플리케이션(EC2 인스턴스, Amazon EFS 스토리지, Auto Scaling 포함)에 .pdf 파일을 업로드합니다. EC2 인스턴스에서 프로그램을 사용해 .jpg로 변환한 뒤, .pdf와 .jpg 파일을 EBS에 저장합니다.",
    "SelectD_Commentary": "Elastic Beanstalk와 EFS, EBS를 혼합 사용하면 관리와 운영 비용이 늘어나며, 서버리스 아키텍처 대비 확장성과 비용 최적화 측면에서 비효율적입니다."
  },
  {
    "Question_Number": "Q64",
    "Question_Description": "한 회사는 온프레미스에서 Windows 파일 서버를 통해 5TB 이상의 파일 데이터를 보유하고 있습니다. 사용자와 애플리케이션은 매일 해당 데이터에 액세스합니다. 회사는 Windows 워크로드를 AWS로 이전하고 있으며, 이전 과정 중에도 AWS와 온프레미스 환경 모두에서 최소 지연으로 파일 스토리지에 접근해야 합니다. 운영 오버헤드를 최소화하고 기존 파일 액세스 방식을 크게 변경하지 않는 솔루션이 필요합니다. 이 회사는 AWS와의 연결을 위해 AWS Site-to-Site VPN을 사용 중입니다. 이러한 요구사항을 만족하기 위해 솔루션스 아키텍트는 무엇을 해야 합니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85173-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 온프레미스 Windows 파일 서버를 AWS로 마이그레이션하면서도 기존 SMB 기반 파일 액세스 방식을 그대로 유지하고, 지연을 최소화하려는 상황입니다. Amazon FSx for Windows File Server와 FSx File Gateway를 결합하면 클라우드와 온프레미스에서 동일한 파일 서버 인터페이스를 사용하도록 구성할 수 있어, 운영 오버헤드와 애플리케이션 변경을 최소화합니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.1"
    ],
    "Keywords": [
      "Windows 파일 서버",
      "온프레미스",
      "Amazon FSx for Windows File Server",
      "FSx File Gateway",
      "파일 액세스 패턴",
      "운영 오버헤드 최소화"
    ],
    "Terms": [
      "Amazon FSx for Windows File Server",
      "Amazon S3 File Gateway",
      "FSx File Gateway",
      "VPN",
      "SMB 프로토콜"
    ],
    "SelectA": "Amazon FSx for Windows File Server를 AWS에 배포 및 구성합니다. 온프레미스 파일 데이터를 FSx로 이전하고, 워크로드를 FSx 서비스로 재구성하여 사용합니다.",
    "SelectA_Commentary": "온프레미스에서 직접 FSx로 접속하므로 VPN 대역폭과 지연이 문제가 될 수 있고, 온프레미스에 대한 캐시나 게이트웨이가 없어 기존 패턴을 유지하기 어렵습니다.",
    "SelectB": "온프레미스에 Amazon S3 File Gateway를 배포 및 구성합니다. 온프레미스 파일 데이터를 S3 File Gateway로 이전하고, 온프레미스와 클라우드 워크로드 모두에서 S3 File Gateway를 사용하도록 재구성합니다.",
    "SelectB_Commentary": "S3를 사용하는 파일 게이트웨이는 SMB가 아닌 객체 스토리지 인터페이스를 사용하므로, Windows 파일 서버 패턴과 맞지 않아 큰 변경이 필요합니다.",
    "SelectC": "온프레미스에 Amazon S3 File Gateway를 배포 및 구성합니다. 온프레미스 파일 데이터를 Amazon S3로 이전합니다. 각 워크로드의 위치에 따라 Amazon S3 또는 S3 File Gateway를 직접 사용하도록 재구성합니다.",
    "SelectC_Commentary": "이 역시 S3 기반 접근이므로 파일 서버 방식(SMB)과 달라 애플리케이션과 사용자 측면에서 많은 변경이 필요합니다.",
    "SelectD": "AWS에 Amazon FSx for Windows File Server를 배포 및 구성합니다. 온프레미스에 Amazon FSx File Gateway를 배포 및 구성합니다. 온프레미스 파일 데이터를 FSx File Gateway로 마이그레이션하고, 클라우드 워크로드는 FSx를, 온프레미스 워크로드는 FSx File Gateway를 사용하도록 설정합니다.",
    "SelectD_Commentary": "FSx File Gateway를 통해 온프레미스 환경에서도 로컬 SMB 형태로 접근이 가능하며, 클라우드에서는 FSx를 직접 활용하므로 최소 변화를 유지하면서 지연을 줄일 수 있는 최적의 솔루션입니다."
  },
  {
    "Question_Number": "Q65",
    "Question_Description": "한 병원이 Amazon API Gateway와 AWS Lambda를 사용하여 RESTful API를 최근에 배포했습니다. 이 병원은 PDF 형식과 JPEG 형식의 보고서를 업로드하기 위해 API Gateway와 Lambda를 사용하고 있습니다. 병원은 보고서 내의 Protected Health Information(PHI)을 식별하기 위해 Lambda 코드를 수정해야 합니다. 가장 낮은 운영 오버헤드로 이 요구 사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85367-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 의료 보고서에서 PHI를 식별하기 위한 가장 간단하고 직관적인 접근 방식을 찾는 것입니다. 문서에서 텍스트를 정확히 추출하고, 의료 특화 엔티티 식별에 최적화된 서비스를 활용해야 운영 오버헤드를 줄일 수 있습니다. Amazon Textract는 PDF·JPEG 등 다양한 문서 포맷에서 텍스트를 추출하고, Amazon Comprehend Medical은 의료 텍스트에 대한 PHI 식별 기능을 완전관리형으로 제공합니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.3"
    ],
    "Keywords": [
      "PHI 식별",
      "Amazon Textract",
      "Amazon Comprehend Medical",
      "PDF/JPEG 보고서",
      "운영 오버헤드 최소화"
    ],
    "Terms": [
      "Amazon API Gateway",
      "AWS Lambda",
      "RESTful API",
      "PDF",
      "JPEG",
      "Protected Health Information(PHI)",
      "Python libraries",
      "Amazon Textract",
      "Amazon SageMaker",
      "Amazon Comprehend Medical",
      "Amazon Rekognition"
    ],
    "SelectA": "기존 Python 라이브러리를 사용하여 보고서에서 텍스트를 추출하고, 추출된 텍스트에서 PHI를 식별합니다.",
    "SelectA_Commentary": "직접 라이브러리를 선택·구현·유지보수해야 하므로 추가 코드 작업과 관리 부담이 커져, 운영 오버헤드가 높을 수 있습니다.",
    "SelectB": "Amazon Textract를 사용하여 보고서에서 텍스트를 추출합니다. Amazon SageMaker를 사용하여 추출된 텍스트에서 PHI를 식별합니다.",
    "SelectB_Commentary": "Textract로 텍스트 추출은 편리하지만, SageMaker로 PHI 식별 모델을 직접 구축·운영해야 하므로 관리 부담이 큽니다.",
    "SelectC": "Amazon Textract를 사용하여 보고서에서 텍스트를 추출합니다. Amazon Comprehend Medical을 사용하여 추출된 텍스트에서 PHI를 식별합니다.",
    "SelectC_Commentary": "문서 텍스트 추출과 의료 특화 PHI 식별에 최적화된 완전관리형 서비스들을 연계하여, 유지보수가 간단하고 운영 오버헤드가 최소화됩니다.",
    "SelectD": "Amazon Rekognition을 사용하여 보고서에서 텍스트를 추출합니다. Amazon Comprehend Medical을 사용하여 추출된 텍스트에서 PHI를 식별합니다.",
    "SelectD_Commentary": "Rekognition의 OCR 기능은 주로 이미지 분석에 초점이 맞춰져 있어, 문서 텍스트 추출에는 Textract 대비 적합성이 낮습니다."
  },
  {
    "Question_Number": "Q66",
    "Question_Description": "어떤 회사는 대략 5MB 크기의 파일을 대량으로 생성하는 애플리케이션을 운영하고 있습니다. 이 파일들은 Amazon S3에 저장되며, 회사 정책상 4년 동안 삭제할 수 없습니다. 이 파일들은 재생산이 어려운 중요한 비즈니스 데이터가 포함되어 있어 항상 즉시 액세스할 수 있어야 합니다. 객체 생성 후 첫 30일 동안은 자주 액세스되지만, 그 이후에는 드물게 액세스됩니다. 이러한 요구사항을 만족하면서 가장 비용 효율적인 스토리지 솔루션은 무엇입니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85310-exam-aws-certified-solut",
    "AnswerDescription": "이 문제의 핵심은 일정 기간 자주 액세스된 후 드물게 참조되는 데이터를 비용 효율적으로 장기 보관하면서도, ‘언제든 즉시 액세스해야 한다’는 요구사항을 충족하는 방식입니다. S3 Glacier는 저렴하지만 즉시 액세스가 불가능하므로 부적합합니다. S3 One Zone-IA는 멀티 AZ 저장을 제공하지 않아 중요 데이터에 대한 내구성과 가용성을 모두 보장하기 어렵습니다. 반면 S3 Standard-IA는 멀티 AZ를 유지하며 즉시 액세스가 가능하고 드문 액세스 시 더 낮은 요금을 제공합니다. 따라서 30일 후 S3 Standard-IA로 전환하고, 4년 후 제거하는 것이 최적의 선택입니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.1"
    ],
    "Keywords": [
      "파일 즉시 액세스",
      "4년 보관",
      "30일 이후 드문 액세스",
      "비용 효율성"
    ],
    "Terms": [
      "Amazon S3",
      "S3 Standard",
      "S3 Standard-Infrequent Access (S3 Standard-IA)",
      "S3 One Zone-Infrequent Access (S3 One Zone-IA)",
      "S3 Glacier",
      "Lifecycle policy"
    ],
    "SelectA": "객체 생성 30일 후 S3 Standard에서 S3 Glacier로 이동하고, 4년 후 삭제",
    "SelectA_Commentary": "S3 Glacier로 이동 시 저렴하지만 즉시 액세스가 불가능하여 요구사항에 부합하지 않습니다.",
    "SelectB": "객체 생성 30일 후 S3 Standard에서 S3 One Zone-IA로 이동하고, 4년 후 삭제",
    "SelectB_Commentary": "One Zone-IA는 멀티 AZ가 아니므로 중요한 데이터에 대한 내구성 및 가용성 면에서 위험도가 높습니다.",
    "SelectC": "객체 생성 30일 후 S3 Standard에서 S3 Standard-IA로 이동하고, 4년 후 삭제",
    "SelectC_Commentary": "중요 데이터를 멀티 AZ에 안전하게 보관하면서도 접근이 적은 시점부터 더 낮은 요금으로 즉시 액세스를 지원해 가장 적합합니다.",
    "SelectD": "객체 생성 30일 후 S3 Standard에서 S3 Standard-IA로 이동하고, 4년 후 S3 Glacier로 이동",
    "SelectD_Commentary": "어차피 4년 후에 삭제할 파일을 Glacier로 옮기는 것은 불필요한 단계를 거치게 되어 비용과 운영 복잡성을 증가시킵니다."
  },
  {
    "Question_Number": "Q67",
    "Question_Description": "한 회사가 여러 Amazon EC2 인스턴스에서 애플리케이션을 호스팅하고 있습니다. 이 애플리케이션은 Amazon SQS 큐에서 메시지를 받아 처리한 뒤, Amazon RDS 테이블에 기록하고, 큐에서 메시지를 삭제합니다. 가끔 Amazon RDS 테이블에 중복된 레코드가 발견되지만, Amazon SQS 큐에는 중복 메시지가 존재하지 않습니다. 메시지가 한 번만 처리되도록 보장하려면 솔루션스 아키텍트는 어떻게 해야 합니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85583-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 Amazon SQS 큐를 사용하는 다중 소비자 환경에서 중복 처리를 방지하는 방법을 묻습니다. 메시지를 처리하는 동안 다른 인스턴스가 해당 메시지를 다시 가져가지 못하도록 적절한 Visibility Timeout을 설정해주는 것이 핵심입니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1"
    ],
    "Keywords": [
      "메시지 중복 방지",
      "Visibility Timeout",
      "Amazon SQS",
      "Amazon EC2",
      "Amazon RDS"
    ],
    "Terms": [
      "Amazon EC2",
      "Amazon RDS",
      "Amazon SQS",
      "CreateQueue API",
      "AddPermission API",
      "ReceiveMessage API",
      "ChangeMessageVisibility API",
      "Visibility Timeout"
    ],
    "SelectA": "CreateQueue API 호출을 사용하여 새 큐를 생성합니다.",
    "SelectA_Commentary": "새 큐를 만든다고 기존 중복 문제가 해결되지는 않아 오답입니다.",
    "SelectB": "AddPermission API 호출을 사용하여 적절한 권한을 추가합니다.",
    "SelectB_Commentary": "권한 설정은 보안 및 접근 제어 목적이며, 메시지 중복 처리 문제를 해결하지 못합니다.",
    "SelectC": "ReceiveMessage API 호출을 사용하여 적절한 대기 시간을 설정합니다.",
    "SelectC_Commentary": "대기 시간(WaitTimeSeconds)은 메시지 수신 방식에 영향을 주지만, 중복 처리를 방지하려면 Visibility Timeout 조정이 필요하므로 오답입니다.",
    "SelectD": "ChangeMessageVisibility API 호출을 사용하여 visibility timeout을 늘립니다.",
    "SelectD_Commentary": "메시지가 처리 중일 때 충분히 숨겨 다른 인스턴스에서 재처리되지 않도록 visibility timeout을 늘려야 하므로 정답입니다."
  },
  {
    "Question_Number": "Q68",
    "Question_Description": "한 솔루션스 아키텍트가 회사의 온프레미스 인프라를 AWS로 확장하기 위한 새로운 하이브리드 아키텍처를 설계하고 있습니다. 회사는 AWS Region으로 가는 고가용성 및 일관된 저지연 연결을 요구합니다. 또한 총 비용을 최소화하고자 하며, 기본 연결(Direct Connect)이 실패했을 때는 더 느린 트래픽 전송을 수용할 수 있습니다. 이러한 요구사항을 충족하려면 어떤 구성을 해야 합니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85593-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 온프레미스 인프라와 AWS를 연결하는 하이브리드 아키텍처에서 고가용성과 낮은 지연을 유지하면서도 장애 시 비용을 아끼고 느린 트래픽을 수용할 수 있는 대안을 묻습니다. 주 연결로 Direct Connect를 사용하고, 백업으로 VPN connection을 두어 장애 시에도 서비스 연속성과 비용 효율성을 모두 달성하는 구성이 최적입니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.2"
    ],
    "Keywords": [
      "하이브리드 아키텍처",
      "고가용성",
      "저지연",
      "비용 최소화",
      "AWS Direct Connect",
      "VPN connection"
    ],
    "Terms": [
      "AWS Direct Connect",
      "VPN connection",
      "AWS Region",
      "Direct Connect failover attribute"
    ],
    "SelectA": "AWS Direct Connect를 Region에 프로비저닝합니다. 기본 Direct Connect 장애에 대비해 VPN connection을 백업으로 구성합니다.",
    "SelectA_Commentary": "Direct Connect로 일관된 저지연을 보장하고, 장애 시 VPN으로 전환하여 비용 부담 없이 고가용성을 유지할 수 있는 최적 구성입니다.",
    "SelectB": "Region에 대한 private connectivity를 위해 VPN tunnel connection을 프로비저닝합니다. 기본 VPN 연결이 실패할 경우를 대비해 추가 VPN tunnel을 구성합니다.",
    "SelectB_Commentary": "VPN만 두 개 구성하면 일관된 저지연을 보장하기 어려워 회사의 요구사항을 충족하기 힘듭니다.",
    "SelectC": "Region에 대한 AWS Direct Connect 연결을 프로비저닝합니다. 기본 Direct Connect 장애에 대비해 동일 Region에 두 번째 Direct Connect 연결을 구성합니다.",
    "SelectC_Commentary": "Direct Connect를 이중화하면 고가용성이지만, 비용이 높고 장애 시에도 VPN 같은 저비용 대안이 없어 요구사항과 맞지 않습니다.",
    "SelectD": "Region에 AWS Direct Connect를 프로비저닝합니다. 기본 Direct Connect 장애 시 AWS CLI의 Direct Connect failover attribute를 사용해 자동으로 백업 연결을 생성하도록 구성합니다.",
    "SelectD_Commentary": "AWS CLI만으로 자동 백업 연결을 즉시 생성하는 기능은 없으며, 별도의 물리적 이중화가 필요하므로 적절한 솔루션이 아닙니다."
  },
  {
    "Question_Number": "Q69",
    "Question_Description": "한 회사가 비즈니스 크리티컬 웹 애플리케이션을 Amazon EC2 인스턴스에서 Application Load Balancer 뒤에서 운영 중입니다. 이 EC2 인스턴스들은 Auto Scaling group으로 구성되어 있습니다. 애플리케이션은 단일 Availability Zone에 배포된 Amazon Aurora PostgreSQL 데이터베이스를 사용하고 있습니다. 회사는 최소한의 다운타임과 데이터 손실로 애플리케이션을 고가용성으로 운영하기를 원합니다. 이 요구사항을 최소한의 운영 노력으로 충족하는 솔루션은 무엇입니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85594-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 웹 애플리케이션과 데이터베이스 모두에서 단일 AZ 문제 발생 시에도 운영이 중단되지 않는 구성을 요구합니다. 다중 AZ를 사용하는 Auto Scaling group과 Multi-AZ로 구성된 Amazon Aurora PostgreSQL, 그리고 Amazon RDS Proxy를 사용하면 다운타임과 데이터 손실을 최소화하면서 자동으로 장애조치 되어 고가용성을 달성할 수 있습니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.2"
    ],
    "Keywords": [
      "고가용성",
      "최소 다운타임",
      "최소 데이터 손실",
      "Auto Scaling group",
      "Multi-AZ",
      "Amazon RDS Proxy"
    ],
    "Terms": [
      "Amazon EC2",
      "Application Load Balancer",
      "Auto Scaling group",
      "Amazon Aurora PostgreSQL",
      "Multi-AZ",
      "Amazon RDS Proxy",
      "Amazon Route 53",
      "AWS Lambda",
      "Amazon S3",
      "S3 Event Notifications"
    ],
    "SelectA": "EC2 인스턴스들을 서로 다른 AWS Region에 배치합니다. Amazon Route 53 헬스 체크를 사용해 트래픽을 리다이렉션합니다. Aurora PostgreSQL Cross-Region Replication을 사용합니다.",
    "SelectA_Commentary": "멀티 리전 구성과 Cross-Region Replication은 설정과 운영이 복잡하며, 단일 AZ 장애 대비보다는 과도한 솔루션입니다.",
    "SelectB": "Auto Scaling group을 다중 Availability Zone에서 동작하도록 구성합니다. 데이터베이스를 Multi-AZ로 구성합니다. 데이터베이스용 Amazon RDS Proxy 인스턴스를 설정합니다.",
    "SelectB_Commentary": "다중 AZ와 Multi-AZ 구성, 그리고 RDS Proxy를 통해 다운타임 시에도 즉시 대체 인스턴스로 전환 가능하여 고가용성과 최소 데이터 손실을 구현합니다.",
    "SelectC": "Auto Scaling group을 단일 Availability Zone에서 사용하도록 설정합니다. 데이터베이스 스냅샷을 시간 단위로 생성합니다. 장애 시 스냅샷에서 복구합니다.",
    "SelectC_Commentary": "스냅샷 복구는 시간이 오래 걸리며, 데이터 손실 가능성도 있어 비즈니스 크리티컬 환경에는 적합하지 않습니다.",
    "SelectD": "Auto Scaling group을 여러 AWS Region에서 동작하도록 구성합니다. 애플리케이션에서 생성되는 데이터를 Amazon S3로 쓰고, S3 Event Notifications로 AWS Lambda 함수를 트리거하여 데이터베이스에 기록합니다.",
    "SelectD_Commentary": "멀티 리전 구성과 Lambda 트리거로 데이터베이스 동기화를 구성하는 것은 운영 복잡도가 높으며, 다운타임은 줄여도 즉각적인 DB 장애 조치는 어렵습니다."
  },
  {
    "Question_Number": "Q70",
    "Question_Description": "회사의 HTTP 애플리케이션이 Network Load Balancer(NLB) 뒤에 위치해 있으며, 이 NLB의 대상 그룹은 여러 Amazon EC2 인스턴스가 포함된 Amazon EC2 Auto Scaling 그룹으로 구성되어 웹 서비스를 실행하고 있습니다. 회사는 애플리케이션에서 발생하는 HTTP 오류를 NLB가 감지하지 못하고 있으며, 이 오류가 발생할 때마다 웹 서비스를 실행하는 EC2 인스턴스를 수동으로 재시작해야 하는 상황입니다. 회사는 커스텀 스크립트나 코드를 작성하지 않고도 애플리케이션의 가용성을 높이고자 합니다. 이러한 요구 사항을 만족하기 위해서는 어떤 조치를 취해야 합니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85734-exam-aws-certified-solut",
    "AnswerDescription": "NLB는 주로 TCP 또는 제한적인 계층 4 수준의 헬스 체크를 지원하여 특정 HTTP URL 상태 확인에는 적합하지 않습니다. ALB는 계층 7에서 HTTP 상태 코드를 확인할 수 있어, 애플리케이션 레벨의 오류를 감지하고 문제가 있는 인스턴스를 자동으로 교체하도록 설정할 수 있습니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.2"
    ],
    "Keywords": [
      "HTTP 애플리케이션",
      "NLB",
      "EC2 Auto Scaling",
      "가용성 개선",
      "HTTP 오류 감지"
    ],
    "Terms": [
      "Network Load Balancer(NLB)",
      "Application Load Balancer(ALB)",
      "Amazon EC2",
      "Amazon EC2 Auto Scaling",
      "HTTP Health Check",
      "Amazon CloudWatch"
    ],
    "SelectA": "NLB에서 HTTP 헬스 체크를 활성화하고 회사 애플리케이션의 URL을 입력합니다.",
    "SelectA_Commentary": "NLB는 기본적으로 HTTP/HTTPS 헬스 체크를 지원하지만, 특정 URL 기반의 정교한 헬스 체크 기능이 제한적이어서 문제 해결에 적합하지 않습니다.",
    "SelectB": "EC2 인스턴스에 cron 작업을 추가하여 로컬 애플리케이션 로그를 분마다 확인하고, HTTP 오류가 관찰되면 애플리케이션을 재시작합니다.",
    "SelectB_Commentary": "이는 커스텀 스크립트 작성과 설정이 필요하여 운영 복잡도가 높습니다. 자동 확장 및 교체가 이루어지지 않아 가용성 향상에 한계가 있습니다.",
    "SelectC": "NLB를 Application Load Balancer(ALB)로 교체하고, 회사 애플리케이션의 URL을 제공하여 HTTP 헬스 체크를 활성화합니다. Auto Scaling 액션을 구성하여 비정상 인스턴스를 자동으로 교체합니다.",
    "SelectC_Commentary": "ALB는 계층 7 기반 헬스 체크를 지원하여 HTTP 오류를 세밀하게 감지할 수 있으며, Auto Scaling과 연동해 불량 인스턴스를 자동으로 교체하여 가용성을 크게 향상시키는 최적의 해법입니다.",
    "SelectD": "NLB에 대한 UnhealthyHostCount 지표를 모니터링하는 Amazon CloudWatch Alarm을 생성합니다. 경보가 ALARM 상태가 되면 비정상 인스턴스를 교체하도록 Auto Scaling 액션을 구성합니다.",
    "SelectD_Commentary": "UnhealthyHostCount는 TCP 수준 헬스 체크만 감지하기 때문에 HTTP 레이어에서의 세부적인 오류 식별이 어렵습니다. 근본 해결책이 되지 못합니다."
  },
  {
    "Question_Number": "Q71",
    "Question_Description": "한 회사는 Amazon DynamoDB를 사용하여 고객 정보를 저장하는 쇼핑 애플리케이션을 운영하고 있습니다. 데이터가 손상될 경우를 대비해, 솔루션스 아키텍트는 복구 시점 목표(RPO)를 15분, 복구 시간 목표(RTO)를 1시간으로 충족하는 솔루션을 설계해야 합니다. 이러한 요구사항을 충족하기 위해 솔루션스 아키텍트는 무엇을 권장해야 합니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85603-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 짧은 RPO와 RTO를 만족하기 위해 DynamoDB 데이터를 빠르고 정확하게 백업하고 복원하는 방안을 찾는 것입니다. Point-in-time recovery(PITR)는 DynamoDB에서 연속 백업을 제공해 원하는 시점으로 바로 복원이 가능하므로 15분 이내의 RPO와 1시간 내 RTO를 충족하기에 적합합니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.2"
    ],
    "Keywords": [
      "DynamoDB",
      "RPO 15분",
      "RTO 1시간",
      "Point-in-time recovery",
      "백업 및 복원"
    ],
    "Terms": [
      "Amazon DynamoDB",
      "RPO",
      "RTO",
      "DynamoDB Global Tables",
      "DynamoDB Point-in-time recovery",
      "Amazon S3 Glacier",
      "Amazon EBS Snapshot"
    ],
    "SelectA": "DynamoDB Global Tables를 구성합니다. RPO 복구 시, 애플리케이션을 다른 AWS 리전으로 지정합니다.",
    "SelectA_Commentary": "Global Tables는 리전 간 데이터 동기화에 유용하지만, 데이터 손상이 여러 리전에 그대로 복제될 수 있어 백업/복원 시점 제어가 어렵습니다.",
    "SelectB": "DynamoDB Point-in-time recovery를 구성합니다. RPO 복구 시, 원하는 시점으로 복원합니다.",
    "SelectB_Commentary": "PITR을 통해 최대 35일 이내 어떤 시점으로든 손쉽게 복원할 수 있어, 15분 RPO와 1시간 RTO 목표를 만족합니다.",
    "SelectC": "DynamoDB 데이터를 매일 Amazon S3 Glacier로 내보냅니다. RPO 복구 시, S3 Glacier로부터 DynamoDB로 데이터를 가져옵니다.",
    "SelectC_Commentary": "하루에 한 번 백업은 15분 RPO 요구사항을 충족하지 못하고 S3 Glacier 복원 속도도 느려 RTO 충족이 어렵습니다.",
    "SelectD": "Amazon EBS 스냅샷을 15분 간격으로 예약 실행합니다. RPO 복구 시, 이 EBS 스냅샷을 사용해 DynamoDB 테이블을 복원합니다.",
    "SelectD_Commentary": "DynamoDB는 서버리스 서비스이므로 EBS 스냅샷 기반 복원이 불가능하며, 해당 접근 방식은 적용할 수 없습니다."
  },
  {
    "Question_Number": "Q72",
    "Question_Description": "한 회사가 사진 처리 애플리케이션을 운영하는데, 동일한 AWS Region에 위치한 Amazon S3 버킷들로부터 자주 이미지를 업로드하고 다운로드해야 합니다. 솔루션스 아키텍트는 데이터 전송 비용이 증가하고 있음을 확인했고, 이를 절감할 수 있는 솔루션을 구현해야 합니다. 어떤 접근 방식이 이 요구 사항을 충족할 수 있습니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85604-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 같은 Region 내에서 빈번하게 발생하는 S3 트래픽을 어떻게 내부 경로로 라우팅해 데이터 전송 비용을 줄일지 묻습니다. S3 VPC gateway endpoint를 사용하면 인터넷을 거치지 않아 비용이 크게 절감됩니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.4"
    ],
    "Keywords": [
      "동일한 AWS Region",
      "데이터 전송 비용",
      "Amazon S3 버킷",
      "S3 VPC gateway endpoint"
    ],
    "Terms": [
      "Amazon S3",
      "S3 VPC gateway endpoint",
      "NAT gateway",
      "Internet gateway",
      "Amazon API Gateway",
      "endpoint policy",
      "퍼블릭 서브넷",
      "VPC"
    ],
    "SelectA": "퍼블릭 서브넷에 Amazon API Gateway를 배포하고, 라우트 테이블을 조정하여 S3 호출을 이를 통해 라우팅합니다.",
    "SelectA_Commentary": "API Gateway로 트래픽을 우회하면 내부 통신을 활용하지 못하고 외부 흐름을 유발하므로, 비용 절감에 효과적이지 않습니다.",
    "SelectB": "퍼블릭 서브넷에 NAT gateway를 배포하고, S3 버킷 액세스를 허용하는 endpoint policy를 연결합니다.",
    "SelectB_Commentary": "NAT gateway를 경유하면 인터넷 트래픽으로 처리되어 추가 전송 비용이 발생하므로, VPC endpoint보다 비용 효율성이 떨어집니다.",
    "SelectC": "애플리케이션을 퍼블릭 서브넷에 배포하고, Internet gateway를 통해 S3 버킷에 접근하도록 라우팅을 허용합니다.",
    "SelectC_Commentary": "Internet gateway를 통한 액세스는 인터넷을 거치는 방식이므로, 전송 비용 절감 효과가 충분히 크지 않습니다.",
    "SelectD": "VPC에 S3 VPC gateway endpoint를 배포하고, S3 버킷 액세스를 허용하는 endpoint policy를 연결합니다.",
    "SelectD_Commentary": "S3 VPC gateway endpoint를 이용하면 VPC 내부 트래픽으로 연결되어 데이터 전송 비용을 절감하고, endpoint policy로 세부 권한 제어도 가능합니다."
  },
  {
    "Question_Number": "Q73",
    "Question_Description": "한 회사가 최근 Amazon EC2의 private subnet에 Linux 기반 애플리케이션 인스턴스를, VPC의 public subnet에 Linux 기반 bastion host를 런칭했습니다. 이 솔루션스 아키텍트는 온프레미스 네트워크에서 회사의 인터넷 연결을 통해 bastion host로, 그리고 애플리케이션 서버들로 연결해야 합니다. 이 솔루션스 아키텍트는 모든 EC2 인스턴스의 보안 그룹이 해당 액세스를 허용하도록 해야 합니다. 이러한 요구사항을 충족하기 위해 솔루션스 아키텍트가 취해야 하는 단계의 조합은 무엇입니까? (2개를 선택하십시오.)",
    "Answer": "C,D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85613-exam-aws-certified-solut",
    "AnswerDescription": "온프레미스 외부 IP에서 bastion host로 접속하고, 그 bastion host의 private IP만 애플리케이션 서버에 SSH를 허용하도록 보안 그룹을 구성해야 합니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1"
    ],
    "Keywords": [
      "bastion host",
      "private subnet",
      "public subnet",
      "on-premises network",
      "security group",
      "접근 허용",
      "external IP range",
      "private IP",
      "SSH"
    ],
    "Terms": [
      "Amazon EC2",
      "bastion host",
      "security group",
      "private subnet",
      "public subnet",
      "SSH",
      "inbound access",
      "IP range"
    ],
    "SelectA": "현재 bastion host의 보안 그룹을 애플리케이션 인스턴스에서만 들어오는 액세스를 허용하도록 교체하십시오.",
    "SelectA_Commentary": "bastion host가 온프레미스에서 접근 가능해야 하므로 잘못된 구성입니다.",
    "SelectB": "현재 bastion host의 보안 그룹을 회사 내부 IP 범위에서만 들어오는 액세스를 허용하도록 교체하십시오.",
    "SelectB_Commentary": "온프레미스 연결이지만 외부 인터넷 IP를 통한 연결도 필요한 경우가 많으므로 제한적입니다.",
    "SelectC": "현재 bastion host의 보안 그룹을 회사의 외부 IP 범위에서만 들어오는 액세스를 허용하도록 교체하십시오.",
    "SelectC_Commentary": "bastion host에 외부 IP(온프레미스)에서 SSH 접근을 허용해야 하므로 옳은 선택입니다.",
    "SelectD": "현재 애플리케이션 인스턴스의 보안 그룹을 bastion host의 private IP 주소에서만 SSH로 들어오는 액세스를 허용하도록 교체하십시오.",
    "SelectD_Commentary": "같은 VPC 내에서 bastion host는 private IP로 애플리케이션 서버에 연결하므로 옳은 선택입니다.",
    "SelectE": "현재 애플리케이션 인스턴스의 보안 그룹을 bastion host의 public IP 주소에서만 SSH로 들어오는 액세스를 허용하도록 교체하십시오.",
    "SelectE_Commentary": "동일 VPC에서는 private IP를 활용하므로 public IP 접근은 비효율적입니다."
  },
  {
    "Question_Number": "Q74",
    "Question_Description": "한 솔루션스 아키텍트가 2티어 웹 애플리케이션을 설계하고 있습니다. 웹 계층은 public subnet에 위치한 Amazon EC2에 호스팅되어 있으며, 데이터베이스 계층은 Microsoft SQL Server가 구동되는 Amazon EC2가 private subnet에 위치합니다. 회사에서는 보안을 매우 중요하게 생각합니다. 이러한 상황에서 security group을 어떻게 구성해야 할까요? (정답은 2개를 고르십시오.)",
    "Answer": "A,C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85346-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 웹 계층(프론트엔드)과 데이터베이스 계층(백엔드)을 각각 다른 subnet에 두고, 서로 다른 security group을 올바르게 구성하여 보안을 강화하는 방법을 묻습니다. 웹 계층에서는 HTTPS(443)로 외부에서 접근 가능해야 하고, 데이터베이스 계층에서는 오직 웹 계층 security group에서 오는 1433 포트 트래픽만 허용해야 합니다. 이를 통해 공용 인터넷에서 직접 접근이 불가능하도록 하면서도 애플리케이션이 정상적으로 동작하게끔 보안 구성을 설정합니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1",
      "1.2"
    ],
    "Keywords": [
      "2티어 웹 애플리케이션",
      "Amazon EC2",
      "public subnet",
      "private subnet",
      "Microsoft SQL Server",
      "security group",
      "포트 443",
      "포트 1433"
    ],
    "Terms": [
      "Amazon EC2",
      "Microsoft SQL Server",
      "public subnet",
      "private subnet",
      "security group",
      "inbound traffic",
      "outbound traffic",
      "port 443",
      "port 1433",
      "0.0.0.0/0"
    ],
    "SelectA": "웹 계층 security group을 0.0.0.0/0에서 포트 443으로의 인바운드 트래픽을 허용하도록 설정합니다.",
    "SelectA_Commentary": "HTTPS 접속을 위해 외부로부터 443 포트를 열어둬야 하므로 필수적인 설정입니다. 따라서 정답에 해당합니다.",
    "SelectB": "웹 계층 security group을 0.0.0.0/0에서 포트 443으로의 아웃바운드 트래픽을 허용하도록 설정합니다.",
    "SelectB_Commentary": "일반적으로 웹 서버에서의 아웃바운드 443은 서버가 외부로 접속할 때 필요한 규칙이지만, 질문에서 요구하는 필수적인 보안 구성 사항은 아닙니다.",
    "SelectC": "데이터베이스 계층 security group에서 웹 계층 security group으로부터 포트 1433 인바운드 트래픽을 허용하도록 설정합니다.",
    "SelectC_Commentary": "웹 계층(프론트엔드)에서 백엔드 DB(Microsoft SQL Server)에 연결하기 위한 포트를 열어주어야 하므로 정답입니다.",
    "SelectD": "데이터베이스 계층 security group에서 웹 계층 security group으로 포트 443과 1433에 대한 아웃바운드 트래픽을 허용하도록 설정합니다.",
    "SelectD_Commentary": "DB 계층에서 웹 계층으로 443이나 1433을 보낼 필요는 없습니다. 주로 웹 계층 → DB 계층으로 연결이 필요하므로 불필요한 설정입니다.",
    "SelectE": "데이터베이스 계층 security group에서 웹 계층 security group으로부터 포트 443과 1433 인바운드 트래픽을 허용하도록 설정합니다.",
    "SelectE_Commentary": "DB 계층에는 1433만 열면 되고 443은 필요하지 않으므로 과도한 허용 규칙입니다."
  },
  {
    "Question_Number": "Q75",
    "Question_Description": "한 회사가 애플리케이션의 성능을 개선하기 위해 온프레미스 환경에서 AWS Cloud로 다중 계층(multi-tiered) 애플리케이션을 이전하려고 합니다. 이 애플리케이션은 RESTful 서비스를 통해 서로 통신하는 여러 계층으로 구성되며, 한 계층이 과부하되면 트랜잭션이 누락되는 문제가 발생합니다. 솔루션 아키텍트는 이 문제를 해결하고 애플리케이션을 모던화할 방안을 설계해야 합니다. 이러한 요구사항을 만족하면서 가장 운영 효율적인 솔루션은 무엇입니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86120-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 과부하로 인해 트랜잭션이 누락되는 전통적 계층 구조를 클라우드 환경에서 현대화하는 시나리오입니다. 병목 해소와 성능 개선을 위해 서버리스 구조(AWS Lambda)와 메시징(Amazon SQS)을 활용해 느슨하게 결합된 확장형 아키텍처를 구성하는 것이 핵심입니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1"
    ],
    "Keywords": [
      "다중 계층 애플리케이션",
      "성능 개선",
      "RESTful 서비스",
      "모던화",
      "Amazon API Gateway",
      "AWS Lambda",
      "Amazon SQS"
    ],
    "Terms": [
      "AWS Cloud",
      "Amazon API Gateway",
      "AWS Lambda",
      "Amazon Simple Queue Service (Amazon SQS)",
      "Amazon CloudWatch",
      "Amazon Simple Notification Service (Amazon SNS)",
      "Amazon EC2",
      "Auto Scaling group"
    ],
    "SelectA": "Amazon API Gateway를 사용하여 트랜잭션을 AWS Lambda 함수(애플리케이션 계층)로 직접 연결하고, 애플리케이션 서비스 간 통신 계층으로 Amazon SQS를 사용합니다.",
    "SelectA_Commentary": "서버리스(Lambda)와 SQS를 활용한 메시지 처리로 계층 간 과부하를 방지하고 완전관리형 환경을 통해 운영 부담을 크게 줄이는 최적의 솔루션입니다.",
    "SelectB": "Amazon CloudWatch 지표를 사용해 애플리케이션 성능 기록을 분석한 뒤, 문제 발생 시점의 서버 피크 사용량에 맞추어 Amazon EC2 인스턴스 크기를 늘립니다.",
    "SelectB_Commentary": "서버 사양 증설은 트래픽 급증에 대비하지만, 과부하 발생 시점을 근본적으로 해결하지 못하고 운영 복잡도가 상대적으로 높아집니다.",
    "SelectC": "Amazon EC2 Auto Scaling 그룹으로 동작하는 애플리케이션 서버 간 메시징에 Amazon SNS를 사용하고, Amazon CloudWatch로 SNS 대기열 길이를 모니터링해 필요 시 확장합니다.",
    "SelectC_Commentary": "SNS는 주로 브로드캐스트 성격의 알림에 적합하며, SQS만큼 강력한 큐잉 기능을 제공하지 않아 트랜잭션 누락 문제를 완전히 해결하기 어렵습니다.",
    "SelectD": "Amazon EC2 Auto Scaling 그룹으로 동작하는 애플리케이션 서버 간 메시징에 Amazon SQS를 사용하고, Amazon CloudWatch로 SQS 대기열 길이를 모니터링한 뒤 통신 장애가 감지되면 확장합니다.",
    "SelectD_Commentary": "대기열 모니터링을 통한 확장은 가능하지만, 애플리케이션 계층을 서버리스로 전환하지 않아 민첩성과 운영 효율성이 A에 비해 떨어집니다."
  },
  {
    "Question_Number": "Q76",
    "Question_Description": "한 회사는 단일 공장 안에 있는 여러 기계로부터 매일 10TB의 계측 데이터를 받습니다. 이 데이터는 공장 내부 온프레미스 데이터 센터의 SAN(Storage Area Network)에 JSON 파일 형태로 저장되어 있습니다. 회사는 이 데이터를 Amazon S3로 전송하여 여러 추가 시스템이 거의 실시간 분석을 수행하도록 하길 원합니다. 데이터는 민감 정보이므로 안전한 전송이 필수적입니다. 이러한 요구사항을 만족하는 가장 신뢰도 높은 데이터 전송 솔루션은 무엇입니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85801-exam-aws-certified-solut",
    "AnswerDescription": "이 문제는 공장 내 SAN에 저장된 대규모 JSON 파일을 Amazon S3로 빠르고 안전하게 전송하는 방법을 묻습니다. AWS DataSync에 AWS Direct Connect를 사용하면 전용 네트워크로 안정성과 보안을 모두 충족합니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.4"
    ],
    "Keywords": [
      "AWS DataSync",
      "AWS Direct Connect",
      "JSON 파일",
      "10TB",
      "SAN",
      "민감 데이터",
      "Amazon S3"
    ],
    "Terms": [
      "AWS DataSync",
      "AWS Direct Connect",
      "AWS Database Migration Service (AWS DMS)",
      "SAN(Storage Area Network)",
      "JSON",
      "on-premises data center",
      "public internet"
    ],
    "SelectA": "AWS DataSync over public internet",
    "SelectA_Commentary": "인터넷 기반 전송은 지연 및 보안 이슈가 발생할 수 있어 신뢰도가 떨어집니다.",
    "SelectB": "AWS DataSync over AWS Direct Connect",
    "SelectB_Commentary": "전용 네트워크 연결을 사용해 안정적이고 안전한 전송이 가능하므로 대규모 민감 데이터 전송에 최적입니다.",
    "SelectC": "AWS Database Migration Service (AWS DMS) over public internet",
    "SelectC_Commentary": "DMS는 주로 데이터베이스 마이그레이션용으로 JSON 파일 전송에 적합하지 않으며, 공용 인터넷은 신뢰성이 낮습니다.",
    "SelectD": "AWS Database Migration Service (AWS DMS) over AWS Direct Connect",
    "SelectD_Commentary": "DMS 자체는 데이터베이스 마이그레이션에 집중된 서비스이므로 JSON 파일 전송 요구사항에는 부적절합니다."
  },
  {
    "Question_Number": "Q77",
    "Question_Description": "한 회사가 애플리케이션에 대해 실시간 데이터 수집 아키텍처를 구성해야 합니다. 회사는 API가 필요하고, 스트리밍되는 동안 데이터를 변환하는 프로세스가 필요하며, 데이터를 저장할 스토리지 솔루션이 필요합니다. 가장 적은 운영 오버헤드로 이 요구사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85740-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 애플리케이션에서 실시간으로 유입되는 데이터를 가져와 변환하고 저장하기 위한 아키텍처 구성에 대한 것입니다. Amazon API Gateway와 Amazon Kinesis Data Streams, Kinesis Data Firehose, AWS Lambda를 결합하여 데이터를 실시간으로 처리하고 Amazon S3에 저장하면 운영 부담을 크게 줄일 수 있습니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.5"
    ],
    "Keywords": [
      "실시간 데이터 스트리밍",
      "API",
      "데이터 변환",
      "저장",
      "Kinesis Data Firehose",
      "Lambda",
      "Amazon S3",
      "운영 오버헤드 최소화"
    ],
    "Terms": [
      "Amazon EC2",
      "Amazon API Gateway",
      "Amazon Kinesis Data Streams",
      "Amazon Kinesis Data Firehose",
      "AWS Lambda",
      "Amazon S3",
      "AWS Glue"
    ],
    "SelectA": "Amazon EC2 인스턴스를 배포하여 API를 호스팅하고, 데이터를 Amazon Kinesis data stream으로 전송합니다. Amazon Kinesis Data Firehose를 생성하여 해당 data stream을 데이터 소스로 사용합니다. AWS Lambda 함수를 사용해 데이터를 변환하고, 변환된 데이터를 Kinesis Data Firehose를 통해 Amazon S3로 전송합니다.",
    "SelectA_Commentary": "별도의 EC2 배포가 필요하므로 운영 오버헤드가 증가합니다. 전체 흐름은 유사하나 API를 EC2에서 호스팅해야 하므로 관리가 복잡해집니다.",
    "SelectB": "Amazon EC2 인스턴스를 배포하여 API를 호스팅하고 AWS Glue로 데이터를 전송합니다. EC2 인스턴스에서 소스/대상 확인을 해제합니다. AWS Glue를 사용하여 데이터를 변환하고 Amazon S3로 전송합니다.",
    "SelectB_Commentary": "EC2와 AWS Glue를 결합한 방식으로 실시간 스트리밍에는 적합하지 않으며, 운영 오버헤드가 큽니다.",
    "SelectC": "Amazon API Gateway API를 구성하여 데이터를 Amazon Kinesis data stream으로 전송합니다. Amazon Kinesis Data Firehose를 생성하여 해당 data stream을 데이터 소스로 사용합니다. AWS Lambda 함수를 사용해 데이터를 변환하고, Kinesis Data Firehose를 통해 Amazon S3로 데이터를 전송합니다.",
    "SelectC_Commentary": "API Gateway와 Kinesis, Lambda, Firehose를 연계하여 실시간 데이터를 처리하고 S3에 저장하며, EC2가 필요 없어 운영 오버헤드를 최소화하는 최적의 솔루션입니다.",
    "SelectD": "Amazon API Gateway API를 구성하여 데이터를 AWS Glue로 전송합니다. AWS Lambda 함수를 사용하여 데이터를 변환합니다. AWS Glue를 사용하여 데이터를 Amazon S3로 전송합니다.",
    "SelectD_Commentary": "AWS Glue는 주로 배치 데이터 처리에 사용되며, API Gateway를 통한 실시간 스트리밍 처리와는 맞지 않아 비효율적입니다."
  },
  {
    "Question_Number": "Q78",
    "Question_Description": "한 회사가 Amazon DynamoDB 테이블에 사용자 트랜잭션 데이터를 보관해야 합니다. 이 회사는 데이터를 7년 동안 유지해야 합니다. 이 요구사항을 충족하는 가장 운영 효율적인 솔루션은 무엇입니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85742-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 DynamoDB에 저장된 데이터를 7년간 유지하면서 운영 복잡도를 최소화하는 백업 전략을 찾는 것입니다. 단순한 PITR(Point-in-time recovery)로는 최대 35일까지 복원이 가능해 장기 보관 요건을 충족하지 못합니다. On-demand 백업과 S3 라이프사이클 등 수동 관리 방식은 오퍼레이션 부담이 큽니다. 반면 AWS Backup을 사용하면 백업 일정 관리와 보존 정책을 한 번에 지정할 수 있어 가장 운영 효율적인 솔루션이 됩니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.2"
    ],
    "Keywords": [
      "Amazon DynamoDB",
      "7년 보관",
      "운영 효율성",
      "백업 일정",
      "AWS Backup"
    ],
    "Terms": [
      "Amazon DynamoDB",
      "Point-in-time recovery (PITR)",
      "On-demand backup",
      "AWS Backup",
      "Amazon S3",
      "S3 Lifecycle",
      "Amazon EventBridge",
      "AWS Lambda"
    ],
    "SelectA": "DynamoDB 테이블에 대해 point-in-time recovery를 사용하여 연속 백업을 수행합니다.",
    "SelectA_Commentary": "PITR은 최대 35일까지 복원이 가능하므로 7년 보관을 충족하지 않습니다.",
    "SelectB": "AWS Backup을 사용하여 백업 일정과 테이블의 보존 정책을 생성합니다.",
    "SelectB_Commentary": "AWS Backup으로 장기 백업 일정 및 보존 정책을 중앙에서 자동 관리할 수 있어 운영 효율성이 매우 높습니다.",
    "SelectC": "DynamoDB 콘솔에서 테이블의 온디맨드 백업을 생성하고, 이를 Amazon S3 버킷에 저장합니다. S3 Lifecycle 구성으로 관리합니다.",
    "SelectC_Commentary": "수동으로 백업 스케줄을 설정해야 하므로 장기적으로 운영 부담이 큽니다.",
    "SelectD": "Amazon EventBridge 규칙로 AWS Lambda 함수를 호출하여 테이블을 백업하고, Amazon S3 버킷에 저장합니다. 이후 S3 Lifecycle을 설정합니다.",
    "SelectD_Commentary": "별도의 Lambda 및 EventBridge 설정이 필요해 운영 절차가 복잡하기 때문에 가장 효율적인 방법이 아닙니다."
  },
  {
    "Question_Number": "Q79",
    "Question_Description": "한 회사가 Amazon DynamoDB 테이블을 사용하여 데이터 저장을 계획하고 있습니다. 회사는 비용 최적화에 대해 우려하고 있습니다. 이 테이블은 대부분의 아침 시간대에는 사용되지 않을 것입니다. 저녁 시간대에는 읽기 및 쓰기 트래픽이 종종 예측하기 힘들 것입니다. 트래픽 급증이 발생하면 매우 빠르게 일어납니다. 솔루션스 아키텍트는 어떤 방안을 권장해야 합니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85743-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 일정하지 않고 예측하기 어려운 트래픽 패턴과 비용 최적화를 동시에 해결해야 합니다. on-demand capacity mode를 사용하면 사용량 급증 시 자동으로 확장되고, 사용한 만큼만 비용을 지불해 무활성 시간대 비용을 절감할 수 있습니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.3"
    ],
    "Keywords": [
      "비용 최적화",
      "아침 시간대 미사용",
      "저녁 예측 불가능 트래픽",
      "트래픽 급증",
      "on-demand capacity mode"
    ],
    "Terms": [
      "Amazon DynamoDB",
      "on-demand capacity mode",
      "provisioned capacity",
      "auto scaling",
      "global table",
      "global secondary index"
    ],
    "SelectA": "Amazon DynamoDB 테이블을 on-demand capacity mode로 생성하십시오.",
    "SelectA_Commentary": "필요할 때 자동으로 확장되며, 사용량이 적을 때는 비용도 줄어듭니다. 트래픽 급변 상황에서도 프로비저닝 설정 없이 즉시 대응 가능해 비용 최적화에 가장 적합합니다.",
    "SelectB": "글로벌 보조 인덱스(global secondary index)가 포함된 DynamoDB 테이블을 생성하십시오.",
    "SelectB_Commentary": "글로벌 보조 인덱스는 조회 패턴 확장에 유용하지만 예측 불가능한 트래픽과 비용 최적화 문제 자체를 해결해 주지는 못합니다.",
    "SelectC": "프로비저닝된 용량(provisioned capacity)과 auto scaling이 설정된 DynamoDB 테이블을 생성하십시오.",
    "SelectC_Commentary": "auto scaling은 트래픽 변화에 대응할 수 있지만 예측치 설정이 필요하며, 급격한 급증에 대한 즉각 대응과 비사용 시간대 비용 절감에서 on-demand만큼 유연하지 않습니다.",
    "SelectD": "프로비저닝된 용량 모드로 DynamoDB 테이블을 생성하고, 이를 글로벌 테이블(global table)로 구성하십시오.",
    "SelectD_Commentary": "글로벌 테이블을 통한 다중 리전 동기화는 가용성과 지연 시간 개선에 도움을 주지만, 비용 최적화와预测 불가 트래픽에 대한 신속 대응 면에서는 on-demand 모드가 더 적합합니다."
  },
  {
    "Question_Number": "Q80",
    "Question_Description": "한 회사는 최근 애플리케이션 마이그레이션 이니셔티브를 지원하기 위해 AWS Managed Service Provider(MSP) Partner와 계약을 체결했습니다. 한 Solutions Architect는 기존 AWS 계정의 Amazon Machine Image(AMI)를 MSP Partner의 AWS 계정과 공유해야 합니다. 해당 AMI는 Amazon Elastic Block Store(Amazon EBS) 기반이며, EBS 볼륨 스냅샷 암호화를 위해 AWS Key Management Service(AWS KMS)의 customer managed key를 사용합니다. 가장 보안성이 높은 방식으로 AMI를 MSP Partner의 AWS 계정과 공유하려면 어떻게 해야 합니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85606-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 KMS customer managed key로 암호화된 Amazon EBS 기반 AMI를 외부 AWS 계정(MSP Partner)과 공유할 때의 보안적인 방법을 묻습니다. 공유 대상 계정이 스냅샷을 복호화하고 AMI를 정상적으로 사용할 수 있도록, launchPermission과 KMS 키 정책을 적절히 설정해야 합니다. 퍼블릭 공유나 Export 절차 등은 불필요하거나 보안 위험, 운영 복잡성을 유발하므로 피해야 합니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1",
      "1.3"
    ],
    "Keywords": [
      "AWS Managed Service Provider(MSP) Partner",
      "Amazon Machine Image(AMI)",
      "암호화된 AMI 공유",
      "AWS KMS customer managed key",
      "EBS 볼륨 스냅샷"
    ],
    "Terms": [
      "Amazon Machine Image (AMI)",
      "AWS Managed Service Provider (MSP)",
      "Amazon EBS",
      "AWS KMS",
      "customer managed key",
      "EBS volume snapshots",
      "Key Policy",
      "launchPermission"
    ],
    "SelectA": "암호화된 AMI와 스냅샷을 퍼블릭하게 만듭니다. 키 정책을 수정하여 MSP Partner의 AWS 계정이 해당 키를 사용할 수 있도록 허용합니다.",
    "SelectA_Commentary": "AMI와 스냅샷을 퍼블릭하게 하면 보안을 크게 약화시키므로, 필요 계정 이외에는 공개하지 않는 것이 안전합니다.",
    "SelectB": "AMI의 launchPermission 속성을 수정합니다. AMI를 MSP Partner의 AWS 계정과만 공유합니다. 키 정책을 수정하여 MSP Partner의 AWS 계정이 키를 사용할 수 있도록 허용합니다.",
    "SelectB_Commentary": "AMI를 특정 계정만 접근 가능하도록 설정하고, KMS 키 정책을 통해 해당 계정이 암호화된 스냅샷을 복호화할 수 있게 해주는 가장 보안적이고 간결한 접근입니다.",
    "SelectC": "AMI의 launchPermission 속성을 수정합니다. AMI를 MSP Partner의 AWS 계정과만 공유합니다. 새로 MSP Partner가 소유한 KMS 키가 암호화에 신뢰되도록 키 정책을 수정합니다.",
    "SelectC_Commentary": "기존 KMS 키가 아닌 다른 키를 쓰려면 재암호화가 필요할 수 있어 운영 복잡성이 상당히 높아집니다.",
    "SelectD": "소스 계정에서 AMI를 MSP Partner의 AWS 계정 내 Amazon S3 버킷으로 Export합니다. 해당 S3 버킷을 MSP Partner가 소유한 새로운 KMS 키로 암호화합니다. 그런 다음 AMI를 MSP Partner의 AWS 계정에서 복사 및 실행합니다.",
    "SelectD_Commentary": "Export 후 다시 Import하는 과정이 늘어나고 재암호화도 필수여서 절차가 복잡해집니다. 기존 KMS 키를 활용해 직접 공유하는 것보다 효율성이 떨어집니다."
  },
  {
    "Question_Number": "Q81",
    "Question_Description": "한 솔루션스 아키텍트가 새로운 애플리케이션을 AWS에 배포하기 위해 클라우드 아키텍처를 설계하고 있습니다. 이 프로세스는 병렬로 실행되며, 처리해야 할 작업(job)의 수에 따라 애플리케이션 노드를 추가하거나 제거할 수 있어야 합니다. 프로세서 애플리케이션(processor application)은 무상태(stateless)입니다. 솔루션스 아키텍트는 애플리케이션이 느슨하게 결합되어 있고, 작업 항목(job items)이 영구적으로(durably) 저장되도록 해야 합니다. 이러한 요구사항을 충족하는 설계는 무엇입니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86621-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 대규모 병렬 처리가 필요한 무상태 애플리케이션을 효율적으로 확장하고 작업을 안정적으로 저장하는 방법을 묻습니다. 느슨한 결합을 위해 Amazon SQS 같은 Queue 서비스 활용이 핵심이며, 처리량에 따라 Auto Scaling group을 동적으로 조정해야 합니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1"
    ],
    "Keywords": [
      "병렬 처리",
      "무상태 애플리케이션",
      "느슨한 결합",
      "durably stored",
      "Amazon SQS",
      "Auto Scaling"
    ],
    "Terms": [
      "Amazon SNS",
      "Amazon SQS",
      "Amazon Machine Image (AMI)",
      "Launch Configuration",
      "Launch Template",
      "Auto Scaling group",
      "Scaling policy",
      "Stateless"
    ],
    "SelectA": "처리할 작업을 Amazon SNS 토픽을 통해 전송합니다. 프로세서 애플리케이션이 포함된 Amazon Machine Image(AMI)를 생성합니다. 해당 AMI를 사용하는 Launch Configuration을 만들고, Auto Scaling group을 생성합니다. Auto Scaling group은 CPU 사용률에 따라 노드를 조정합니다.",
    "SelectA_Commentary": "SNS는 주로 알림·푸시 기반 서비스로 내재적 메시지 보관 기능이 제한적입니다. 상태 저장과 내구성(durability)을 보장하기 어렵고 CPU 사용량만으로 처리가 필요한 작업 수를 제대로 반영하기 어렵습니다.",
    "SelectB": "처리할 작업을 저장할 Amazon SQS 큐를 만듭니다. 프로세서 애플리케이션이 포함된 AMI를 생성합니다. 해당 AMI를 사용하는 Launch Configuration을 구성하고, Auto Scaling group을 만듭니다. 네트워크 사용량을 기준으로 노드를 조정하도록 설정합니다.",
    "SelectB_Commentary": "SQS를 이용해 내구성 있는 큐를 사용하지만, 네트워크 사용량을 기준으로 확장하면 작업 수와 직접 연관되지 않아 과소 혹은 과다 확장이 일어날 수 있습니다.",
    "SelectC": "처리할 작업을 저장할 Amazon SQS 큐를 만듭니다. 프로세서 애플리케이션이 포함된 AMI를 생성합니다. 해당 AMI를 사용하는 Launch Template을 생성하고, Auto Scaling group을 구성합니다. SQS 큐의 항목 수에 따라 노드를 추가하거나 제거하도록 설정합니다.",
    "SelectC_Commentary": "SQS 큐는 메시지를 내구성 있게 저장해 느슨한 결합을 실현하며, 큐 항목 수에 따라 수평 확장을 자동으로 조정할 수 있어 요구사항을 완벽히 충족합니다.",
    "SelectD": "처리할 작업을 Amazon SNS 토픽에 전송합니다. 프로세서 애플리케이션이 포함된 AMI를 생성합니다. 해당 AMI를 사용하는 Launch Template을 생성하고, Auto Scaling group을 만듭니다. SNS 토픽에 게시되는 메시지 수를 기준으로 노드를 조정합니다.",
    "SelectD_Commentary": "SNS 토픽은 알림 기반이며 큐 자체가 없으므로 작업의 내구성 보장이 어렵습니다. 게시 메시지 수만으로는 실제 큐잉 기반 처리와 달리 안정적인 확장을 보장하기 어렵습니다."
  },
  {
    "Question_Number": "Q82",
    "Question_Description": "한 회사가 AWS Cloud에서 웹 애플리케이션을 호스팅하고 있습니다. 이 회사는 Elastic Load Balancer가 AWS Certificate Manager (ACM)에 가져온 인증서를 사용하도록 구성했습니다. 회사의 보안 팀은 각 인증서가 만료되기 30일 전에 반드시 알림을 받아야 합니다. 이 요구 사항을 충족하기 위해 Solutions Architect는 무엇을 권장해야 합니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85615-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 ACM에 등록된 인증서가 만료되기 전에 어떻게 미리 알림을 보낼 수 있는가에 대한 보안 운용 시나리오입니다. AWS Certificate Manager 자체에는 만료 알림을 직접 Scheduler 형태로 구성하는 기능이 없으므로, 만료 30일 전에 자동으로 리소스를 검사하고 알림을 보낼 수 있는 기능이 필요합니다. AWS Config는 acm-certificate-expiration-check라는 관리형 규칙을 제공하며, 이 규칙을 기반으로 인증서 만료 시점을 모니터링할 수 있습니다. EventBridge 규칙과 Amazon SNS 알림을 연계하여 보안 팀에 자동으로 알림을 전송하도록 설정할 수 있으므로 운영 복잡성 없이 요구사항을 충족합니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.2"
    ],
    "Keywords": [
      "AWS Cloud",
      "Elastic Load Balancer",
      "AWS Certificate Manager (ACM)",
      "인증서 만료",
      "30일 전 알림"
    ],
    "Terms": [
      "AWS Certificate Manager (ACM)",
      "Elastic Load Balancer",
      "AWS Config",
      "acm-certificate-expiration-check",
      "Amazon EventBridge (Amazon CloudWatch Events)",
      "Amazon Simple Notification Service (Amazon SNS)",
      "AWS Lambda",
      "AWS Trusted Advisor"
    ],
    "SelectA": "ACM에서 30일 전부터 매일 Amazon Simple Notification Service (Amazon SNS) 토픽으로 커스텀 메시지를 게시하는 규칙을 추가합니다.",
    "SelectA_Commentary": "ACM 자체에 이러한 규칙을 직접 추가할 수 없기에 부적절합니다.",
    "SelectB": "AWS Config에서 만료 30일 이내의 인증서를 검사하는 규칙을 생성하고, Amazon EventBridge (Amazon CloudWatch Events)와 SNS를 연동하여 비준수 리소스가 보고되면 경고를 발송합니다.",
    "SelectB_Commentary": "AWS Config의 acm-certificate-expiration-check 관리형 규칙을 사용하여 간단히 해결 가능합니다.",
    "SelectC": "AWS Trusted Advisor로 30일 이내에 만료될 인증서를 확인하고, Trusted Advisor의 상태 변경 지표를 기반으로 Amazon CloudWatch 알람을 생성해 SNS로 커스텀 알림을 전송합니다.",
    "SelectC_Commentary": "Trusted Advisor는 일부 인증서 점검을 제공하지만 즉시적이고 세밀한 만료 알림에는 AWS Config가 더 적합합니다.",
    "SelectD": "Amazon EventBridge (Amazon CloudWatch Events) 규칙을 생성해 30일 이내에 만료될 인증서를 감지하고, AWS Lambda 함수를 호출하여 SNS를 통해 커스텀 알림을 전송합니다.",
    "SelectD_Commentary": "직접 증분 로직을 구현해야 하므로, 이미 관리형 규칙이 있는 AWS Config 방식보다 복잡합니다."
  },
  {
    "Question_Number": "Q83",
    "Question_Description": "한 회사의 동적 웹사이트가 미국에 있는 on-premises 서버를 사용해 호스팅되고 있습니다. 이 회사는 유럽에서 제품 출시를 앞두고, 새롭게 유럽 지역 사용자들의 사이트 로딩 시간을 최적화하고자 합니다. 하지만 웹사이트 백엔드는 계속 미국에 유지되어야 하며, 제품은 며칠 뒤 출시 예정이므로 즉시 구현 가능한 솔루션이 필요합니다. 솔루션스 아키텍트는 어떤 해결책을 제안해야 합니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85902-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 유럽 사용자들의 웹사이트 로딩 시간을 개선하면서 백엔드는 미국에 그대로 두고, 빠르게 구현 가능한 방안을 찾는 것이 핵심입니다. CloudFront를 사용해 on-premises 서버를 custom origin으로 설정하면, 전 세계 Edge Location에서 콘텐츠를 캐싱해 유럽 사용자에게 신속하게 전달할 수 있습니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.4"
    ],
    "Keywords": [
      "동적 웹사이트",
      "on-premises 서버",
      "유럽 사용자",
      "사이트 로딩 시간 최적화",
      "즉시 솔루션",
      "Amazon CloudFront",
      "custom origin"
    ],
    "Terms": [
      "Amazon CloudFront",
      "on-premises",
      "Amazon EC2",
      "Amazon S3",
      "Cross-Region Replication",
      "Amazon Route 53 geoproximity routing"
    ],
    "SelectA": "us-east-1에 Amazon EC2 인스턴스를 실행하고 사이트를 해당 인스턴스로 마이그레이션합니다.",
    "SelectA_Commentary": "사이트를 EC2로 이전해도 물리적 거리가 멀어 유럽 사용자의 지연 시간이 크게 개선되지 않고, 즉시 사용하기에도 시간이 더 소요됩니다.",
    "SelectB": "웹사이트를 Amazon S3로 이전합니다. 리전 간 Cross-Region Replication을 사용합니다.",
    "SelectB_Commentary": "정적 콘텐츠에는 적절하지만, 동적 웹사이트와 미국에 있는 on-premises 백엔드를 그대로 활용하는 요구사항에는 부적합합니다.",
    "SelectC": "Amazon CloudFront를 사용하고, on-premises 서버를 custom origin으로 지정합니다.",
    "SelectC_Commentary": "CloudFront의 CDN 기능으로 전 세계 Edge Location에서 콘텐츠를 제공해 유럽 사용자에게 빠른 응답 속도를 보장하므로 즉각적 해결책입니다.",
    "SelectD": "Amazon Route 53 geoproximity 라우팅 정책을 사용하여 on-premises 서버로 트래픽을 라우팅합니다.",
    "SelectD_Commentary": "지리적 위치에 따라 트래픽을 분산하지만, 미국에 있는 백엔드 자체를 그대로 두면서 유럽에서의 지연을 낮추기 위해서는 CloudFront와 같은 CDN 방식이 더 적합합니다."
  },
  {
    "Question_Number": "Q84",
    "Question_Description": "한 회사가 기존의 3티어 웹 아키텍처 비용을 절감하려고 합니다. 웹, 애플리케이션, 데이터베이스 서버는 개발, 테스트, 프로덕션 환경에서 Amazon EC2 인스턴스로 실행되고 있습니다. EC2 인스턴스는 피크 시간대에 평균 30% CPU 활용률, 비피크 시간대에 평균 10% CPU 활용률을 보입니다. 프로덕션 EC2 인스턴스는 하루 24시간 실행되며, 개발 및 테스트 EC2 인스턴스는 매일 최소 8시간씩 실행됩니다. 회사는 사용 중이 아닐 때 개발 및 테스트 EC2 인스턴스를 중지하도록 자동화를 구현하려고 합니다. 회사 요구사항을 가장 비용 효율적으로 충족하는 EC2 인스턴스 구매 솔루션은 무엇입니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85665-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 프로덕션에서는 24시간 상시 실행이 필요하므로 미리 예약해 두어 할인 혜택이 적용되는 Reserved Instances가 적합합니다. 개발과 테스트는 사용하지 않을 때 자동으로 중지할 수 있어 가변적인 시간만큼 비용을 지불하는 On-Demand Instances가 합리적입니다. Spot blocks는 현재 제공되지 않고, 상시 가용성이 필요한 프로덕션에 Spot Instances를 사용하는 것은 적합하지 않습니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.2"
    ],
    "Keywords": [
      "비용 절감",
      "Amazon EC2",
      "Reserved Instances",
      "On-Demand Instances",
      "Spot Instances",
      "Spot blocks",
      "개발 환경",
      "테스트 환경",
      "프로덕션 환경"
    ],
    "Terms": [
      "CPU Utilization",
      "Three-tier web architecture",
      "Reserved Instances",
      "On-Demand Instances",
      "Spot Instances",
      "Spot blocks"
    ],
    "SelectA": "Spot Instances를 프로덕션 EC2 인스턴스에 사용하고, Reserved Instances를 개발 및 테스트 인스턴스에 사용합니다.",
    "SelectA_Commentary": "프로덕션은 24시간 안정적으로 실행되어야 하므로 중단될 수 있는 Spot Instances는 적절하지 않습니다.",
    "SelectB": "Reserved Instances를 프로덕션 EC2 인스턴스에 사용하고, On-Demand Instances를 개발 및 테스트 인스턴스에 사용합니다.",
    "SelectB_Commentary": "프로덕션은 상시 실행이 필요하므로 Reserved Instances로 장기 비용 절감이 가능하며, 개발 및 테스트는 On-Demand Instances로 필요 시간만큼만 비용을 지불할 수 있어 가장 효율적입니다.",
    "SelectC": "Spot blocks를 프로덕션 EC2 인스턴스에 사용하고, Reserved Instances를 개발 및 테스트 인스턴스에 사용합니다.",
    "SelectC_Commentary": "Spot blocks는 현재 이용할 수 없으며, 프로덕션의 안정적인 가동에도 적합하지 않습니다.",
    "SelectD": "On-Demand Instances를 프로덕션 EC2 인스턴스에 사용하고, Spot blocks를 개발 및 테스트 인스턴스에 사용합니다.",
    "SelectD_Commentary": "Spot blocks는 더 이상 제공되지 않으며, 자동 중지와 재시작이 가능한 On-Demand가 아닌 다른 방식으로는 개발/테스트의 탄력성을 제대로 살리기 어렵습니다."
  },
  {
    "Question_Number": "Q85",
    "Question_Description": "한 회사는 프로덕션 웹 애플리케이션을 운영 중이며, 웹 인터페이스나 모바일 앱을 통해 사용자가 문서를 업로드합니다. 새로운 규제 요구사항에 따르면, 업로드된 문서는 저장 후 수정하거나 삭제할 수 없습니다. 이 요구사항을 충족하기 위해 Solutions Architect는 무엇을 해야 합니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85751-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 규제 요구사항을 만족하기 위해 문서를 저장한 후 변경 또는 삭제가 불가능하도록 해야 하는 시나리오입니다. 정답은 Amazon S3에서 S3 Object Lock과 S3 Versioning을 함께 활성화하여 WORM(Write-Once-Read-Many) 보관 모델을 구현하는 것입니다. 이를 통해 업로드된 문서를 일정 기간 또는 무기한 동안 삭제나 덮어쓰기를 방지하고, 규제 요구사항을 충족할 수 있습니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.3"
    ],
    "Keywords": [
      "프로덕션 웹 애플리케이션",
      "문서 업로드",
      "규제 요구사항",
      "변경 불가능",
      "Amazon S3",
      "S3 Object Lock",
      "S3 Versioning"
    ],
    "Terms": [
      "S3 Object Lock",
      "WORM",
      "S3 Versioning",
      "S3 Lifecycle Policy",
      "ACL",
      "Amazon EFS"
    ],
    "SelectA": "업로드된 문서를 S3 Versioning과 S3 Object Lock이 활성화된 Amazon S3 버킷에 저장합니다.",
    "SelectA_Commentary": "S3 Object Lock을 통해 저장된 객체를 변경 또는 삭제할 수 없는 WORM 모드를 구현할 수 있으므로 규제 요구사항을 충족합니다.",
    "SelectB": "업로드된 문서를 Amazon S3 버킷에 저장하고, 주기적으로 S3 Lifecycle 정책을 구성해 아카이빙합니다.",
    "SelectB_Commentary": "Lifecycle 정책은 객체를 다른 스토리지 클래스로 이동 또는 만료시키는 기능으로, 수정·삭제 방지를 보장하지 못합니다.",
    "SelectC": "S3 Versioning이 활성화된 Amazon S3 버킷에 업로드하고, ACL를 통해 읽기 전용 권한으로 제한합니다.",
    "SelectC_Commentary": "ACL만으로는 사용자가 객체를 수정하거나 삭제하지 못하게 완전히 막을 수 없으며, 규제 수준의 변경 불가 상태를 구현하기 어렵습니다.",
    "SelectD": "Amazon EFS 볼륨에 문서를 저장하고, read-only 모드로 볼륨을 마운트하여 데이터를 액세스합니다.",
    "SelectD_Commentary": "read-only 모드만으로는 EFS 데이터 변경을 영구적으로 금지하는 보장을 하지 못하고, WORM 요구사항을 충족하지 못합니다."
  },
  {
    "Question_Number": "Q86",
    "Question_Description": "한 회사는 여러 웹 서버에서 공용 Amazon RDS MySQL Multi-AZ DB 인스턴스에 자주 액세스해야 합니다. 회사는 안전한 방법으로 웹 서버가 데이터베이스에 연결할 수 있기를 원하며, 사용자 자격 증명을 자주 교체(회전)해야 한다는 보안 요구사항을 충족해야 합니다. 이러한 요구사항을 만족하는 솔루션은 무엇입니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85753-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 웹 서버가 Amazon RDS MySQL Multi-AZ DB 인스턴스에 안전하게 연결하면서 자격 증명을 자주 교체해야 하는 상황을 다룹니다. AWS Secrets Manager를 사용하면 자격 증명을 안전하게 저장하고, 자동으로 회전시킬 수 있어 보안과 운영 편의성을 모두 충족합니다. 이를 통해 하드코딩된 비밀번호 위험을 최소화하고, 규칙적인 자격 증명 교체가 가능합니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.3"
    ],
    "Keywords": [
      "Amazon RDS MySQL Multi-AZ",
      "사용자 자격 증명 교체",
      "AWS Secrets Manager",
      "IAM 권한",
      "웹 서버 안전 연결"
    ],
    "Terms": [
      "Amazon RDS MySQL Multi-AZ",
      "AWS Secrets Manager",
      "IAM",
      "AWS Systems Manager OpsCenter",
      "Amazon S3",
      "AWS Key Management Service (AWS KMS)"
    ],
    "SelectA": "데이터베이스 사용자 자격 증명을 AWS Secrets Manager에 저장합니다. 웹 서버가 AWS Secrets Manager에 액세스할 수 있도록 필요한 IAM 권한을 부여합니다.",
    "SelectA_Commentary": "AWS Secrets Manager는 자격 증명 저장 및 자동 회전을 제공하여 보안을 크게 강화합니다. 웹 서버에서는 IAM 권한을 통해 동적으로 자격 증명을 받아 안전하게 DB에 연결할 수 있습니다.",
    "SelectB": "데이터베이스 사용자 자격 증명을 AWS Systems Manager OpsCenter에 저장합니다. 웹 서버가 OpsCenter에 액세스할 수 있도록 필요한 IAM 권한을 부여합니다.",
    "SelectB_Commentary": "OpsCenter는 운영 이벤트 관리를 주 목적으로 하며, Secrets Manager처럼 전문적으로 자격 증명을 자동 회전해 주는 기능이 별도로 제공되지 않습니다.",
    "SelectC": "데이터베이스 사용자 자격 증명을 보안 설정된 Amazon S3 버킷에 저장합니다. 필요한 IAM 권한을 부여하여 웹 서버가 자격 증명을 조회하고 DB에 액세스하도록 합니다.",
    "SelectC_Commentary": "S3 버킷을 사용할 경우 자격 증명 자동 회전을 지원하지 않아 수동 관리가 필요하며, 보안 및 운영 측면에서 추가 부담이 생깁니다.",
    "SelectD": "데이터베이스 사용자 자격 증명을 AWS KMS로 암호화된 파일 형태로 웹 서버 파일 시스템에 저장합니다. 웹 서버가 이 파일을 복호화하여 DB에 액세스하도록 합니다.",
    "SelectD_Commentary": "웹 서버 내에 직접 자격 증명을 저장하면 보안 리스크가 높고, 자격 증명 교체 시 운영 절차가 복잡해집니다. 자동 교체도 지원되지 않습니다."
  },
  {
    "Question_Number": "Q87",
    "Question_Description": "한 회사는 Amazon API Gateway API에 의해 호출되는 AWS Lambda 함수를 사용하여 애플리케이션을 호스팅하고 있습니다. Lambda 함수는 고객 데이터를 Amazon Aurora MySQL 데이터베이스에 저장합니다. 회사가 데이터베이스를 업그레이드할 때마다 업그레이드가 완료될 때까지 Lambda 함수가 데이터베이스 연결을 수립하지 못하게 되어, 이벤트 중 일부 고객 데이터가 기록되지 않는 문제가 발생합니다. 솔루션 아키텍트는 데이터베이스가 업그레이드되는 동안 생성되는 고객 데이터를 저장할 방안을 설계해야 합니다. 어떤 솔루션이 이 요구사항을 충족합니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85319-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 데이터베이스 업그레이드 시점에 데이터베이스 연결이 일시적으로 끊겨 고객 데이터가 손실되는 상황을 방지하고자 하는 요구사항을 다룹니다. 업그레이드 중에도 데이터를 안전하게 저장한 뒤, 데이터베이스가 정상화되면 다시 처리할 수 있는 구조가 핵심입니다. 정답은 Amazon SQS FIFO 큐를 통해 데이터가 유실되지 않고 큐에 보관되도록 하여, 이후 새로운 Lambda 함수가 해당 데이터를 데이터베이스에 정상 반영하도록 하는 방식입니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.2"
    ],
    "Keywords": [
      "AWS Lambda",
      "Amazon API Gateway",
      "Amazon Aurora MySQL",
      "Database Upgrade",
      "SQS FIFO Queue",
      "고객 데이터 보존"
    ],
    "Terms": [
      "Amazon RDS Proxy",
      "Lambda local storage",
      "Amazon SQS FIFO",
      "Retry mechanism",
      "Aurora MySQL 업그레이드",
      "Amazon RDS",
      "Lambda 함수"
    ],
    "SelectA": "Amazon RDS Proxy를 설정하여 Lambda 함수와 데이터베이스 사이에 두고, Lambda 함수가 RDS Proxy로 연결하도록 구성합니다.",
    "SelectA_Commentary": "RDS Proxy는 데이터베이스 장애나 재시작 시 연결 풀을 관리해 failover 시간을 단축하지만, 업그레이드 중 데이터베이스가 완전히 오프라인이면 여전히 요청이 실패할 수 있어 업그레이드 동안 생성되는 데이터를 보관하는 데에는 한계가 있습니다.",
    "SelectB": "Lambda 함수 실행 시간을 최대로 늘리고, 고객 데이터를 데이터베이스에 저장하는 코드에 재시도 메커니즘을 구현합니다.",
    "SelectB_Commentary": "재시도 로직만으로는 데이터베이스가 업그레이드로 인해 길게 다운되는 상황에서 요청이 계속 실패할 가능성이 높아, 데이터 손실을 완전히 방지하기 어렵습니다.",
    "SelectC": "Lambda local storage에 고객 데이터를 저장합니다. 그리고 새 Lambda 함수를 구성하여 local storage에 저장된 데이터를 데이터베이스에 저장하도록 합니다.",
    "SelectC_Commentary": "Lambda 함수의 local storage는 영구적이지 않고 함수별 격리 영역이 달라서 업그레이드 기간 동안의 데이터를 안정적으로 보관하기 적합하지 않습니다.",
    "SelectD": "Amazon Simple Queue Service(Amazon SQS) FIFO 큐에 고객 데이터를 저장합니다. 새로운 Lambda 함수를 생성하여 큐에서 메시지를 폴링하여 데이터베이스에 저장하도록 합니다.",
    "SelectD_Commentary": "데이터베이스가 업그레이드 중이더라도 SQS가 데이터를 안전하게 보관하고, 데이터베이스가 정상화되면 큐에서 메시지를 읽어 처리할 수 있어 데이터 손실을 방지하고 서비스의 복원력을 높입니다."
  },
  {
    "Question_Number": "Q88",
    "Question_Description": "한 설문 회사가 미국 지역에서 여러 해 동안 수집한 데이터를 Amazon S3 버킷에 저장하고 있으며, 현재 크기는 3TB이고 계속 증가하고 있습니다. 이 회사는 유럽에 있는 마케팅 회사와 데이터를 공유하기 시작했으며, 마케팅 회사 또한 S3 버킷을 가지고 있습니다. 회사는 자신의 데이터 전송 비용을 최대한 낮게 유지하기를 원합니다. 이러한 요구사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85738-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 기업이 보유한 S3 데이터를 해외 파트너에게 공유할 때 발생하는 데이터 전송 비용을 최소화하기 위한 방법을 묻습니다. Requester Pays를 사용하면 다운로드 비용을 요청 측(마케팅 회사)으로 전가하여, 데이터 제공 회사(미국 회사) 측의 트래픽 비용을 최소화할 수 있습니다. 다른 옵션들은 회사 측에 각종 전송 및 복제 비용이 발생하거나 단순 스토리지 비용 절감에 머물기 때문에, 문제의 요구사항인 '자사의 전송 비용 최소화' 핵심에 부합하지 않습니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.4"
    ],
    "Keywords": [
      "데이터 전송 비용",
      "Amazon S3",
      "Requester Pays",
      "Cross-Region Replication",
      "S3 Intelligent-Tiering"
    ],
    "Terms": [
      "Requester Pays",
      "S3 Cross-Region Replication",
      "Cross-account access",
      "S3 Intelligent-Tiering",
      "Amazon S3 버킷"
    ],
    "SelectA": "회사의 S3 버킷에서 Requester Pays 기능을 구성합니다.",
    "SelectA_Commentary": "Requester Pays 사용 시, 데이터 요청자(마케팅 회사)가 전송 요금을 부담하여 회사의 전송 비용을 최소화할 수 있습니다.",
    "SelectB": "회사의 S3 버킷에서 마케팅 회사의 S3 버킷으로 S3 Cross-Region Replication을 구성합니다.",
    "SelectB_Commentary": "회사 측이 복제 비용과 전송 비용을 부담하므로, 전송 비용 절감 목표에 부합하지 않습니다.",
    "SelectC": "Cross-account access를 구성하여 마케팅 회사가 회사의 S3 버킷에 접근하도록 설정합니다.",
    "SelectC_Commentary": "접근 권한만 부여할 뿐 전송 비용은 여전히 회사가 부담할 가능성이 크므로 요구사항에 맞지 않습니다.",
    "SelectD": "회사의 S3 버킷을 S3 Intelligent-Tiering으로 설정하고, 해당 버킷을 마케팅 회사의 S3 버킷과 동기화합니다.",
    "SelectD_Commentary": "S3 Intelligent-Tiering은 스토리지 비용을 자동 관리하지만, 전송 비용 절감에는 직접적인 도움이 되지 않습니다."
  },
  {
    "Question_Number": "Q89",
    "Question_Description": "한 회사가 Amazon S3를 사용하여 기밀 감사 문서를 저장하고 있습니다. 해당 S3 버킷은 감사 팀 IAM 사용자 자격 증명에 대해 최소 권한 원칙에 따라 접근을 제한하기 위해 버킷 정책을 사용합니다. 회사의 관리자들은 S3 버킷에 있는 문서가 실수로 삭제되는 상황을 우려하며, 더 안전한 솔루션을 원하고 있습니다. 문서를 안전하게 보호하기 위해 Solutions Architect가 해야 할 작업은 무엇입니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85808-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 S3에 저장된 감사 문서를 실수로 삭제하지 않도록 안전하게 보호하는 방법을 묻습니다. 단순히 IAM 사용자에게 MFA를 적용하는 것만으로는 실수로 인한 삭제를 막을 수 없습니다. 대신 Versioning을 활성화하고 MFA Delete를 적용하면 버킷에서 객체를 완전히 삭제하기 전 추가적인 MFA 검증이 필요해, 실수 혹은 권한 남용으로 인한 영구 삭제를 예방할 수 있습니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1",
      "1.3"
    ],
    "Keywords": [
      "기밀 감사 문서",
      "실수로 삭제",
      "S3 버킷 보안",
      "Versioning",
      "MFA Delete"
    ],
    "Terms": [
      "Amazon S3",
      "IAM",
      "Versioning",
      "MFA Delete",
      "Multi-factor authentication(MFA)",
      "S3 Lifecycle policy",
      "s3:DeleteObject",
      "AWS Key Management Service(AWS KMS)"
    ],
    "SelectA": "S3 버킷에서 Versioning과 MFA Delete 기능을 활성화합니다.",
    "SelectA_Commentary": "Versioning과 MFA Delete를 함께 사용하면 객체가 잘못 삭제되더라도 복원이 가능하며, 영구 삭제 전에 추가 MFA 확인이 필요해 보안을 크게 강화합니다.",
    "SelectB": "각 감사 팀 IAM 사용자 계정의 IAM 사용자 자격 증명에 대해 Multi-factor authentication(MFA)을 활성화합니다.",
    "SelectB_Commentary": "사용자 인증 자체에 MFA를 적용하는 것은 유용하나, 객체 삭제시 별도의 MFA 확인을 거치지 않으므로 실수로 인한 영구 삭제를 근본적으로 방지하지 못합니다.",
    "SelectC": "감사 팀 IAM 사용자 계정에 대해 S3 Lifecycle 정책을 추가하여 감사 기간 동안 s3:DeleteObject 작업을 거부하도록 설정합니다.",
    "SelectC_Commentary": "감사 기간에만 삭제를 막는 정책으로, 기간 외에는 실수 삭제가 여전히 가능하고 다른 운영 상 제약이 발생하므로 최적의 솔루션이 아닙니다.",
    "SelectD": "AWS Key Management Service(AWS KMS)를 사용해 S3 버킷을 암호화하고 감사 팀 IAM 사용자 계정이 KMS 키에 접근하지 못하도록 제한합니다.",
    "SelectD_Commentary": "데이터 암호화는 기밀성을 높일 수 있으나, 실수로 삭제되는 자체 문제를 해결하지 못하므로 요구사항에 부합하지 않습니다."
  },
  {
    "Question_Number": "Q90",
    "Question_Description": "한 회사는 공개적으로 접근 가능한 영화 데이터를 저장하기 위해 SQL database를 사용하고 있습니다. 이 database는 Amazon RDS Single-AZ DB instance에서 실행 중입니다. 매일 불규칙한 간격으로 스크립트가 실행되어 database에 새로 추가된 영화의 개수를 기록합니다. 이 스크립트는 업무 시간 동안 최종 집계 결과를 보고해야 합니다. 회사의 개발팀은 스크립트가 실행되고 있을 때 database 성능이 개발 업무에 충분하지 않다고 판단했습니다. 솔루션스 아키텍트는 이 문제를 해결하는 솔루션을 권장해야 합니다. 가장 낮은 운영 오버헤드로 이 요구사항을 만족하는 솔루션은 무엇입니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85339-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 주기적으로 실행되는 스크립트가 읽기 부하를 유발해 개발팀 업무에 영향을 주는 상황입니다. read replica를 사용하면 최소한의 추가 설정으로 성능 저하 문제를 효과적으로 해결할 수 있습니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.3"
    ],
    "Keywords": [
      "SQL database",
      "Amazon RDS Single-AZ DB instance",
      "새로 추가된 영화",
      "개발팀 성능 문제",
      "read replica"
    ],
    "Terms": [
      "Amazon RDS Single-AZ",
      "Multi-AZ deployment",
      "Read Replica",
      "Amazon ElastiCache"
    ],
    "SelectA": "DB instance를 Multi-AZ로 변경합니다.",
    "SelectA_Commentary": "Multi-AZ는 장애 복구나 고가용성에는 좋지만 읽기 부담을 분산시키지 못해 스크립트로 인한 성능 문제는 크게 해소되지 않습니다.",
    "SelectB": "데이터베이스의 read replica를 생성하고 스크립트가 오직 해당 read replica만 조회하도록 구성합니다.",
    "SelectB_Commentary": "스크립트로 인한 읽기 부하를 본 DB에서 분리해 성능 문제를 해결할 수 있으며, 추가 작업이 적어 운영 오버헤드가 낮은 최적의 방법입니다.",
    "SelectC": "개발팀에 매일 말에 데이터베이스 항목들을 수동으로 export하도록 지시합니다.",
    "SelectC_Commentary": "사람이 직접 수행해야 하므로 운영 오버헤드가 높고, 실수 가능성도 있어서 자동화되지 않은 비효율적인 방법입니다.",
    "SelectD": "Amazon ElastiCache를 사용해 스크립트가 자주 실행하는 쿼리 결과를 캐싱합니다.",
    "SelectD_Commentary": "ElastiCache는 반복 조회에 유용하지만, 새로 추가된 영화 수를 매번 조회하는 경우 캐시 이점이 적어 근본적인 해결책이 되기 어렵습니다."
  },
  {
    "Question_Number": "Q91",
    "Question_Description": "한 회사에서 Amazon EC2 인스턴스가 있는 VPC에서 애플리케이션을 운영하고 있습니다. 그중 하나의 애플리케이션에서는 Amazon S3 API를 호출해 객체를 저장하고 읽어야 합니다. 회사의 보안 규정에 따르면 애플리케이션에서 발생하는 어떠한 트래픽도 인터넷을 통과해서는 안 됩니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85667-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 VPC 내의 EC2 인스턴스에서 S3로의 통신이 인터넷을 거치지 않도록 하는 보안 액세스 설계가 핵심입니다. NAT Gateway나 Internet Gateway 등 외부 연결을 사용하지 않으려면 S3 Gateway Endpoint를 설정하여 프라이빗 네트워크 환경에서 S3에 직접 연결하도록 구성하는 것이 최선의 방법입니다. 이를 통해 EC2 인스턴스 트래픽이 인터넷으로 노출되지 않고 S3 접근이 가능해집니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1"
    ],
    "Keywords": [
      "VPC",
      "Amazon EC2",
      "Amazon S3",
      "보안 규정",
      "인터넷 통신 제한",
      "S3 Gateway Endpoint"
    ],
    "Terms": [
      "S3 Gateway Endpoint",
      "NAT Gateway",
      "VPC",
      "Amazon EC2",
      "Amazon S3",
      "Private Subnet"
    ],
    "SelectA": "S3 Gateway Endpoint를 구성합니다.",
    "SelectA_Commentary": "S3 Gateway Endpoint를 사용하면 VPC 내에서 인터넷을 통하지 않고 Amazon S3에 안전하게 연결할 수 있습니다. 회사의 보안 규정에도 부합합니다.",
    "SelectB": "Private Subnet에 S3 버킷을 생성합니다.",
    "SelectB_Commentary": "S3는 기본적으로 리전 기반의 글로벌 서비스라 Private Subnet에 직접 버킷을 생성하는 개념이 없습니다. 보안 요구사항도 충족할 수 없습니다.",
    "SelectC": "EC2 인스턴스와 동일한 AWS Region에 S3 버킷을 생성합니다.",
    "SelectC_Commentary": "동일 리전에 버킷을 생성해도 기본적으로 인터넷 경로를 통한 통신이 발생할 수 있어 보안 규정을 만족하지 못합니다.",
    "SelectD": "EC2 인스턴스와 동일 서브넷에 NAT Gateway를 구성합니다.",
    "SelectD_Commentary": "NAT Gateway를 구성하면 인터넷을 통해서 S3와 통신하므로 회사의 ‘인터넷을 거치면 안 된다’는 규정에 어긋납니다."
  },
  {
    "Question_Number": "Q92",
    "Question_Description": "한 회사가 민감한 사용자 정보를 Amazon S3 버킷에 저장하고 있습니다. 회사는 VPC 내부에서 동작하는 Amazon EC2 인스턴스 애플리케이션 계층에서 이 버킷에 안전하게 액세스하기를 원합니다. 이를 달성하기 위해 솔루션스 아키텍트가 취해야 하는 단계 조합은 어떤 것인가요? (2개를 고르십시오.)",
    "Answer": "A,C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85903-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 VPC 내부 EC2 인스턴스가 Amazon S3 버킷에 직접적이고 안전하게 접근하도록 설계하는 방법입니다. VPC Gateway Endpoint를 사용하면 내부 망을 통해서만 S3와 통신할 수 있어 외부 노출이 줄어듭니다. 또한 Bucket Policy로 특정 VPC 리소스만 접근 가능하도록 제한해 보안을 강화합니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1",
      "1.3"
    ],
    "Keywords": [
      "민감한 사용자 정보",
      "Amazon S3 버킷",
      "VPC 내부 EC2",
      "안전한 액세스",
      "VPC Gateway Endpoint",
      "Bucket Policy"
    ],
    "Terms": [
      "Amazon S3",
      "Amazon EC2",
      "VPC",
      "VPC Gateway Endpoint",
      "Bucket Policy",
      "NAT instance",
      "IAM user",
      "S3 Access Policy"
    ],
    "SelectA": "VPC 내부에 Amazon S3용 VPC Gateway Endpoint를 구성합니다.",
    "SelectA_Commentary": "VPC Gateway Endpoint를 사용하면 S3 트래픽이 인터넷으로 나가지 않고 내부 경로로만 전송되어 안전성과 성능이 향상됩니다.",
    "SelectB": "S3 버킷의 객체를 공개로 설정하는 Bucket Policy를 생성합니다.",
    "SelectB_Commentary": "버킷을 ‘public’으로 설정하면 민감 데이터가 노출될 위험이 커지므로 안전한 구성에 부적합합니다.",
    "SelectC": "VPC에서 동작하는 애플리케이션 계층에만 액세스를 제한하는 Bucket Policy를 생성합니다.",
    "SelectC_Commentary": "Bucket Policy를 통해 특정 VPC 리소스에서만 접근이 가능하도록 설정하면 민감한 데이터 보호에 효과적입니다.",
    "SelectD": "IAM 사용자와 S3 액세스 정책을 만들고 이 자격 증명을 EC2 인스턴스에 복사합니다.",
    "SelectD_Commentary": "IAM 사용자 자격 증명을 직접 인스턴스에 복사하는 방식은 관리가 어렵고 보안에 취약하므로 모범 사례가 아닙니다.",
    "SelectE": "NAT 인스턴스를 생성하고 EC2 인스턴스가 NAT 인스턴스를 통해 S3 버킷에 액세스하도록 합니다.",
    "SelectE_Commentary": "NAT 인스턴스 사용은 인터넷 게이트웨이를 통한 통신을 전제로 하므로 민감 데이터에 대한 내부 전송 요구사항에 적합하지 않습니다."
  },
  {
    "Question_Number": "Q93",
    "Question_Description": "회사는 MySQL 데이터베이스로 구동되는 온프레미스 애플리케이션을 운영하고 있습니다. 애플리케이션의 탄력성(elasticity)과 가용성을 높이기 위해 AWS로 마이그레이션하려고 합니다. 현재 아키텍처는 정상 운영 시 데이터베이스에 대한 무거운 읽기 부하가 있습니다. 또한 4시간마다 운영 환경 데이터베이스를 전체 export하여 스테이징 환경에 구성하고 있는데, 이 과정에서 사용자들은 애플리케이션 지연을 겪으며, 개발팀도 export가 끝날 때까지 스테이징 환경을 사용할 수 없습니다. 솔루션스 아키텍트는 이 애플리케이션 지연 문제를 완화하고, 개발팀이 지연 없이 스테이징 환경을 꾸준히 사용할 수 있도록 하는 대체 아키텍처를 제안해야 합니다. 이 요구사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85729-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 MySQL 기반 온프레미스 환경에서 AWS로 이전하여, 데이터베이스 부하와 스테이징 환경 동시 사용 이슈를 해결하려는 상황입니다. Amazon Aurora MySQL의 database cloning을 활용해 큰 부하 없이 스테이징 환경을 구성하고, Multi-AZ Aurora Replicas로 가용성과 성능을 확보할 수 있습니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.3"
    ],
    "Keywords": [
      "온프레미스 MySQL",
      "높은 읽기 부하",
      "4시간마다 전체 export",
      "스테이징 환경",
      "애플리케이션 지연",
      "Amazon Aurora MySQL",
      "Multi-AZ Aurora Replicas",
      "database cloning"
    ],
    "Terms": [
      "Amazon Aurora MySQL",
      "Multi-AZ Aurora Replicas",
      "Amazon RDS for MySQL",
      "read replicas",
      "mysqldump",
      "database cloning",
      "standby instance"
    ],
    "SelectA": "Amazon Aurora MySQL with Multi-AZ Aurora Replicas를 운영 환경으로 사용합니다. mysqldump 유틸리티를 사용하는 백업/복원 프로세스로 스테이징 데이터베이스를 채웁니다.",
    "SelectA_Commentary": "mysqldump 활용 시 운영 DB에 큰 부하가 발생하고, 스테이징 환경 준비에도 시간이 오래 걸려 기존 문제를 해결하기에 충분치 않습니다.",
    "SelectB": "Amazon Aurora MySQL with Multi-AZ Aurora Replicas를 운영 환경으로 사용합니다. database cloning을 적용하여 필요 시 스테이징 데이터베이스를 온디맨드로 생성합니다.",
    "SelectB_Commentary": "Aurora database cloning은 운영 DB를 빠르고 효율적으로 복제해 초기화 시간을 단축하고 부하를 최소화하므로 요구사항을 충족하는 최적의 솔루션입니다.",
    "SelectC": "Amazon RDS for MySQL Multi-AZ 배포와 read replicas를 운영 환경으로 사용합니다. 스탠바이 인스턴스를 스테이징 데이터베이스로 활용합니다.",
    "SelectC_Commentary": "Multi-AZ 스탠바이는 장애 복구용으로 동기화되어 있어 자유로운 스테이징 환경 구성에 제약이 크고, read replica만으로는 완전한 스테이징 구축이 어렵습니다.",
    "SelectD": "Amazon RDS for MySQL Multi-AZ 배포와 read replicas를 운영 환경으로 사용합니다. mysqldump 유틸리티를 사용하는 백업/복원 프로세스로 스테이징 데이터베이스를 구성합니다.",
    "SelectD_Commentary": "mysqldump로 생성·복원 시 운영에 부하가 커 문제 해결에 적합하지 않으며, 스테이징 환경 준비 과정이 지연되므로 요구사항을 만족시키기 어렵습니다."
  },
  {
    "Question_Number": "Q94",
    "Question_Description": "한 회사에서 사용자가 작은 파일을 Amazon S3에 업로드하는 애플리케이션을 설계하고 있습니다. 사용자가 파일을 업로드하면, 그 파일은 한 번의 간단한 처리를 통해 데이터를 변환하고 이후 분석을 위해 JSON 형식으로 저장되어야 합니다. 파일은 업로드되자마자 가능한 한 빨리 처리되어야 하며, 수요는 날짜에 따라 매우 많이 업로드될 수도 있고 거의 업로드되지 않을 수도 있습니다. 이때 운영 오버헤드를 최소화하면서 요구 사항을 만족하는 솔루션은 무엇입니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86676-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 사용자의 가변적인 업로드 빈도에 실시간으로 대응하면서 파일을 신속하게 처리하고, 운영상의 복잡함을 최소화하는 아키텍처를 구현하는 방법을 묻습니다. 서버나 클러스터 관리가 필요 없는 서버리스 기반 이벤트 구독 방식(Amazon S3 이벤트 → Amazon SQS → AWS Lambda)이 운영 오버헤드를 크게 줄이고 자동 확장에 유리합니다. 또한 결과를 NoSQL 기반인 Amazon DynamoDB에 JSON 형식으로 저장하면 구조화 과정이 간편하고 확장성도 뛰어납니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1"
    ],
    "Keywords": [
      "S3 업로드",
      "JSON 변환",
      "단일 처리",
      "가변적인 수요",
      "최소 운영 오버헤드"
    ],
    "Terms": [
      "Amazon S3",
      "Amazon EMR",
      "Amazon Aurora",
      "Amazon Simple Queue Service (Amazon SQS)",
      "AWS Lambda",
      "Amazon EC2",
      "Amazon EventBridge (Amazon CloudWatch Events)",
      "Amazon Kinesis Data Streams",
      "Amazon DynamoDB"
    ],
    "SelectA": "Amazon EMR을 구성하여 Amazon S3에서 텍스트 파일을 읽고, 스크립트를 실행해 데이터를 변환한 뒤 Amazon Aurora DB cluster에 JSON 파일을 저장합니다.",
    "SelectA_Commentary": "EMR 클러스터 관리와 오케스트레이션이 필요해 운영 오버헤드가 높으며, 간단한 처리를 위해 과도한 솔루션입니다.",
    "SelectB": "Amazon S3 이벤트 알림을 Amazon SQS 큐로 전송합니다. Amazon EC2 인스턴스가 큐를 읽고 데이터를 처리하여 JSON 파일을 Amazon DynamoDB에 저장합니다.",
    "SelectB_Commentary": "EC2 인스턴스 운영 및 스케일링 관리가 필요해 서버리스보다 오버헤드가 큽니다.",
    "SelectC": "Amazon S3 이벤트 알림을 Amazon SQS 큐로 전송합니다. AWS Lambda 함수가 큐에서 메시지를 읽어 데이터를 처리하고, JSON 파일을 Amazon DynamoDB에 저장합니다.",
    "SelectC_Commentary": "서버리스 아키텍처로 자동 확장과 이벤트 기반 처리가 가능하여 운영 오버헤드가 가장 적은 최적 솔루션입니다.",
    "SelectD": "Amazon EventBridge(Amazon CloudWatch Events)를 구성하여 새 파일이 업로드될 때 Amazon Kinesis Data Streams로 이벤트를 전송합니다. AWS Lambda 함수가 스트림 이벤트를 처리하고, Amazon Aurora DB cluster에 JSON 파일을 저장합니다.",
    "SelectD_Commentary": "Kinesis 스트림 설정과 Aurora 관리가 필요해 처리 흐름이 복잡하며, 단순 처리 요구 사항에 비해 과도한 구성입니다."
  },
  {
    "Question_Number": "Q95",
    "Question_Description": "한 회사의 본사 사용자들이 제품 데이터에 액세스할 수 있는 애플리케이션이 있습니다. 이 제품 데이터는 Amazon RDS MySQL DB instance에 저장되어 있습니다. 운영 팀은 애플리케이션 성능 저하 문제를 찾아내었고, 읽기 트래픽과 쓰기 트래픽을 분리하고자 합니다. 솔루션스 아키텍트는 애플리케이션 성능을 빠르게 최적화해야 합니다. 어떤 조치를 권장해야 합니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85906-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 Amazon RDS 환경에서읽기 트래픽을 효율적으로 분산해 애플리케이션 성능을 빠르게 높이는 방법을 묻습니다. Multi-AZ는 고가용성 목적이므로, Read Replica를 동일 리소스로 구성해 읽기 요청을 분산해야 합니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.3"
    ],
    "Keywords": [
      "Amazon RDS MySQL",
      "애플리케이션 성능",
      "읽기 트래픽",
      "쓰기 트래픽",
      "Read Replica",
      "Multi-AZ"
    ],
    "Terms": [
      "Amazon RDS MySQL",
      "DB instance",
      "Multi-AZ",
      "Read Replica",
      "Primary Availability Zone",
      "Secondary Availability Zone"
    ],
    "SelectA": "기존 데이터베이스를 Multi-AZ 배포로 변경합니다. 읽기 요청은 기본 가용 영역에서 처리합니다.",
    "SelectA_Commentary": "Multi-AZ는 주로 장애 대비용이며, 기본 가용 영역에서 읽기를 처리하면 여전히 트래픽 부담이 높아집니다.",
    "SelectB": "기존 데이터베이스를 Multi-AZ 배포로 변경합니다. 읽기 요청은 보조 가용 영역에서 처리합니다.",
    "SelectB_Commentary": "Multi-AZ 보조 노드는 장애 발생 시에만 활성화되므로, 일반 상황에서 읽기 요청 처리에 적합하지 않습니다.",
    "SelectC": "데이터베이스에 대해 Read Replica를 생성합니다. Read Replica에는 원본 DB의 절반 크기의 컴퓨트와 스토리지 리소스를 구성합니다.",
    "SelectC_Commentary": "읽기 부하 분산은 가능하지만, 리소스가 부족하면 대규모 동시 읽기 요청 처리에 한계가 있습니다.",
    "SelectD": "데이터베이스에 대해 Read Replica를 생성합니다. Read Replica에 원본 DB와 동일한 컴퓨트와 스토리지 리소스를 구성합니다.",
    "SelectD_Commentary": "Read Replica를 충분한 리소스로 구성해 읽기 트래픽을 안정적으로 분산함으로써, 빠르게 성능을 개선할 수 있습니다."
  },
  {
    "Question_Number": "Q96",
    "Question_Description": "한 Amazon EC2 관리자가 여러 사용자가 포함된 IAM 그룹에 다음 정책을 연결했습니다. 이 정책에는 특정 리전과 소스 IP 조건이 설정되어 있습니다. 이 정책의 효과는 무엇입니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86460-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 IAM Policy의 조건(Context Keys) 설정을 통해 EC2 인스턴스 종료 권한을 제한하는 방법을 묻습니다. 정책에서는 us-east-1 리전 내에서, 소스 IP가 10.100.100.254인 경우에만 EC2 인스턴스 종료를 허용하고 다른 리전에서는 종료를 허용하지 않습니다. 따라서 이 정책의 실제 효과를 바르게 파악해야 합니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1"
    ],
    "Keywords": [
      "us-east-1 리전",
      "EC2 인스턴스 종료",
      "소스 IP 주소",
      "IAM 정책"
    ],
    "Terms": [
      "Amazon EC2",
      "IAM 그룹",
      "Policy",
      "소스 IP",
      "us-east-1 Region",
      "TerminateInstance"
    ],
    "SelectA": "사용자는 us-east-1을 제외한 모든 AWS 리전에서 EC2 인스턴스를 종료할 수 있습니다.",
    "SelectA_Commentary": "정책 해석과 반대되는 설명입니다. 정책은 오직 us-east-1 리전에서만 종료를 허용합니다.",
    "SelectB": "사용자는 us-east-1 리전에서 IP 주소가 10.100.100.1인 EC2 인스턴스를 종료할 수 있습니다.",
    "SelectB_Commentary": "정책에서 요구하는 소스 IP는 10.100.100.254이며, 10.100.100.1은 조건에 부합하지 않습니다.",
    "SelectC": "사용자의 소스 IP가 10.100.100.254일 때, us-east-1 리전에서 EC2 인스턴스를 종료할 수 있습니다.",
    "SelectC_Commentary": "정책에서 us-east-1 리전과 소스 IP가 10.100.100.254인 경우에만 종료를 허용하므로, 이 설명이 정확한 정책의 효과입니다.",
    "SelectD": "사용자의 소스 IP가 10.100.100.254일 때, us-east-1 리전에서 EC2 인스턴스를 종료할 수 없습니다.",
    "SelectD_Commentary": "정책은 소스 IP가 10.100.100.254이고 리전이 us-east-1일 경우에 종료를 허용하므로, 이 설명은 틀립니다."
  },
  {
    "Question_Number": "Q97",
    "Question_Description": "한 회사는 온프레미스에서 대규모 Microsoft SharePoint를 운영 중이며, Microsoft Windows 공유 파일 스토리지가 필요합니다. 이 워크로드를 AWS Cloud로 마이그레이션하려고 하며, 여러 스토리지 옵션을 검토 중입니다. 스토리지 솔루션은 고가용성을 갖추고, 액세스 제어를 위해 Active Directory와 통합되어야 합니다. 이러한 요구 사항을 만족하는 솔루션은 무엇입니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86626-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 Windows 기반 워크로드(SharePoint)를 AWS로 마이그레이션하면서, 고가용성이 보장되고 Active Directory를 통한 인증이 필요한 공유 파일 스토리지를 설계하는 방법을 묻습니다. 네이티브 Windows 파일 시스템 통합과 SMB 프로토콜, AD 연동이 필수적이므로, Amazon FSx for Windows File Server가 가장 적합합니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.2"
    ],
    "Keywords": [
      "Microsoft SharePoint",
      "Microsoft Windows 공유 파일 스토리지",
      "Active Directory",
      "고가용성",
      "Amazon FSx for Windows File Server"
    ],
    "Terms": [
      "Amazon EFS",
      "AWS Storage Gateway",
      "SMB",
      "Amazon S3",
      "Amazon FSx for Windows File Server",
      "Active Directory",
      "고가용성"
    ],
    "SelectA": "Amazon EFS 스토리지를 구성하고 Active Directory 도메인을 인증용으로 설정합니다.",
    "SelectA_Commentary": "Amazon EFS는 Linux 기반 NFS 프로토콜을 주로 사용하므로 Windows 환경 및 SMB 기반 공유에는 최적화되어 있지 않습니다.",
    "SelectB": "두 개의 가용 영역(AZ)에 AWS Storage Gateway 파일 게이트웨이로 SMB 파일 공유를 생성합니다.",
    "SelectB_Commentary": "파일 게이트웨이는 온프레미스와의 하이브리드 환경에 유용하지만, 완전한 AD 통합 및 고가용성 측면에서 FSx보다 부족합니다.",
    "SelectC": "Amazon S3 버킷을 생성하고 Microsoft Windows Server에서 이를 볼륨으로 마운트하도록 구성합니다.",
    "SelectC_Commentary": "Windows 서버에서 S3 버킷을 직접 파일 공유로 쓰기는 제한적이며, 대비해 SMB 및 AD 통합 지원이 부족합니다.",
    "SelectD": "AWS에서 Amazon FSx for Windows File Server 파일 시스템을 생성하고, 인증용으로 Active Directory 도메인을 설정합니다.",
    "SelectD_Commentary": "SMB 프로토콜과 AD 통합을 기본 제공하며, 고가용성을 확보하기 위해 여러 AZ에 데이터를 복제하므로 요구사항을 모두 충족합니다."
  },
  {
    "Question_Number": "Q98",
    "Question_Description": "한 이미지 처리 회사에서는 사용자가 이미지를 업로드할 수 있는 웹 애플리케이션을 운영하고 있습니다. 이 애플리케이션은 Amazon S3 버킷에 이미지를 업로드합니다. 회사는 Amazon Simple Queue Service(Amazon SQS) 스탠다드 큐로 객체 생성 이벤트를 게시하기 위해 S3 event notifications을 설정했습니다. 이 SQS 큐는 이미지를 처리하고 처리 결과를 이메일로 사용자에게 전송하는 AWS Lambda 함수의 이벤트 소스로 동작합니다.\n\n사용자들은 업로드된 각 이미지마다 여러 개의 이메일이 도착하고 있다고 보고했습니다. 솔루션스 아키텍트는 SQS 메시지가 Lambda 함수를 여러 번 호출하여 중복 이메일이 발생한다는 사실을 확인했습니다.\n\n가장 적은 운영 오버헤드로 이 문제를 해결하기 위해 솔루션스 아키텍트는 무엇을 해야 합니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85185-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 SQS 메시지가 Lambda 함수의 처리 시간보다 일찍 다시 큐로 돌아와 중복 처리가 발생하는 상황을 해결하는 방법을 묻습니다. Lambda가 메시지를 정상적으로 처리할 수 있도록 visibility timeout을 충분히 늘리면, 메시지가 재전송되지 않아 중복 메일이 방지됩니다. 이는 운영적인 부담이 적고 가장 직접적인 해결책입니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1"
    ],
    "Keywords": [
      "중복 메시지",
      "Lambda 함수",
      "visibility timeout",
      "이메일 중복",
      "운영 오버헤드 최소화"
    ],
    "Terms": [
      "Amazon S3",
      "S3 event notifications",
      "Amazon Simple Queue Service (Amazon SQS)",
      "SQS standard queue",
      "AWS Lambda",
      "visibility timeout",
      "SQS FIFO queue",
      "long polling",
      "function timeout",
      "batch window timeout",
      "message deduplication ID"
    ],
    "SelectA": "SQS 큐에 long polling을 설정하기 위해 ReceiveMessage 대기 시간을 30초로 늘립니다.",
    "SelectA_Commentary": "long polling은 메시지 수신 빈도를 줄일 뿐, Lambda 처리 시간 초과로 인한 중복 메시지 문제는 근본적으로 해결하지 못합니다.",
    "SelectB": "SQS 스탠다드 큐를 SQS FIFO 큐로 변경합니다. 메시지 deduplication ID를 사용해 중복 메시지를 제거합니다.",
    "SelectB_Commentary": "FIFO 큐로 바꾸면 메시지 순서 및 Deduplication 기능이 있지만, 애플리케이션 구조를 변경해야 하고 스루풋 제한도 있어 운영 오버헤드가 증가할 수 있습니다.",
    "SelectC": "Lambda 함수 타임아웃 및 배치 윈도우 타임아웃의 합보다 큰 값을 SQS 큐의 visibility timeout으로 설정합니다.",
    "SelectC_Commentary": "Lambda가 메시지를 완전히 처리하기 전에 메시지가 다시 큐로 돌아오지 않도록 visibility timeout을 충분히 늘려 한 번만 처리되게 하는 최적의 솔루션입니다.",
    "SelectD": "Lambda 함수가 메시지를 읽자마자 메시지를 처리하기 전에 SQS 큐에서 메시지를 삭제하도록 수정합니다.",
    "SelectD_Commentary": "처리 전에 메시지를 삭제하면 Lambda 함수 오류 발생 시 메시지를 잃을 위험이 큽니다. 신뢰성을 저해하는 좋지 않은 방식입니다."
  },
  {
    "Question_Number": "Q99",
    "Question_Description": "한 회사가 온프레미스 데이터 센터에서 호스팅되는 게임 애플리케이션을 위한 공유 스토리지 솔루션을 구현하려고 합니다. 이 회사는 Lustre 클라이언트를 사용하여 데이터를 액세스할 수 있는 기능이 필요하며, 이 솔루션은 완전관리형(Fully Managed)이어야 합니다. 이러한 요구사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85811-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 Lustre 클라이언트를 활용해야 하며 완전관리형 서비스가 필수적입니다. FSx for Lustre는 HPC 환경에서 자주 사용되는 Lustre 파일 시스템을 완전관리형으로 제공하므로 정답입니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.1"
    ],
    "Keywords": [
      "공유 스토리지",
      "완전관리형",
      "Lustre 클라이언트",
      "온프레미스 데이터 센터",
      "게임 애플리케이션"
    ],
    "Terms": [
      "Amazon FSx for Lustre",
      "Amazon EFS",
      "AWS Storage Gateway",
      "Windows file share role",
      "Lustre",
      "Shared Storage"
    ],
    "SelectA": "AWS Storage Gateway file gateway를 생성하고 필요한 클라이언트 프로토콜을 사용하는 파일 공유를 생성한 후, 해당 애플리케이션 서버를 연결합니다.",
    "SelectA_Commentary": "AWS Storage Gateway는 Lustre 클라이언트를 직접 지원하지 않으므로 요구사항을 충족하지 못합니다.",
    "SelectB": "Amazon EC2 Windows 인스턴스를 생성하고 Windows file share role을 설치·구성합니다. 애플리케이션 서버를 해당 파일 공유에 연결합니다.",
    "SelectB_Commentary": "Windows 기반 파일 공유는 Lustre 프로토콜을 지원하지 않아 요구사항에 부합하지 않습니다.",
    "SelectC": "Amazon Elastic File System(Amazon EFS) 파일 시스템을 생성하여 Lustre 지원으로 구성한 뒤 원본 서버와 연결하고, 애플리케이션 서버를 파일 시스템에 연결합니다.",
    "SelectC_Commentary": "Amazon EFS는 Lustre 프로토콜을 기본적으로 지원하지 않아 요구사항을 충족하기 어렵습니다.",
    "SelectD": "Amazon FSx for Lustre 파일 시스템을 생성하고 이를 원본 서버에 연결합니다. 애플리케이션 서버를 해당 파일 시스템에 연결합니다.",
    "SelectD_Commentary": "Amazon FSx for Lustre는 Lustre를 완전관리형으로 제공하며 고성능 공유 스토리지를 구현하기에 적합한 정답입니다."
  },
  {
    "Question_Number": "Q100",
    "Question_Description": "한 회사의 컨테이너화된 애플리케이션이 Amazon EC2 인스턴스에서 실행됩니다. 이 애플리케이션은 다른 비즈니스 애플리케이션과 통신하기 전에 보안 인증서를 다운로드해야 합니다. 회사는 보안 인증서를 실시간에 가깝게 암호화 및 복호화할 수 있는 고도로 안전한 솔루션을 원합니다. 또한 암호화된 후의 데이터를 고가용성 스토리지에 저장해야 하며, 운용상 오버헤드를 최소화해야 합니다. 이러한 요구 사항을 가장 적은 운영 복잡도로 충족할 솔루션은 무엇입니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85186-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 EC2 상에서 실행되는 컨테이너화된 애플리케이션이 보안 인증서를 안전하고 빠르게 다루는 방법을 묻습니다. 실시간에 가까운 암호화·복호화가 필요하며, 암호화된 데이터를 고가용성 스토리지에 저장해야 합니다. 가장 효율적이고 운영 부담이 적은 방식은 AWS KMS를 사용하여 데이터를 암호화하고, Amazon S3와 결합해 보관하는 것입니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.3"
    ],
    "Keywords": [
      "컨테이너화된 애플리케이션",
      "보안 인증서",
      "암호화",
      "복호화",
      "고가용성 스토리지",
      "운영 오버헤드"
    ],
    "Terms": [
      "AWS Secrets Manager",
      "IAM",
      "AWS Lambda",
      "AWS Key Management Service (AWS KMS)",
      "Customer Managed Key",
      "Amazon EC2",
      "Amazon S3",
      "Amazon EBS"
    ],
    "SelectA": "AWS Secrets Manager에 보안 인증서를 암호화된 형태로 저장하고, 필요할 때 수동으로 인증서를 업데이트합니다. 세분화된 IAM 액세스를 사용하여 데이터에 대한 접근을 제어합니다.",
    "SelectA_Commentary": "Secrets Manager를 통해 인증서 관리를 할 수 있으나, 실시간에 가까운 암호화·복호화가 큰 규모로 요구될 때는 수동 업데이트가 오버헤드를 증가시키고 실시간 처리에 제약이 있을 수 있습니다.",
    "SelectB": "Python cryptography 라이브러리를 사용하는 AWS Lambda 함수를 생성하여 암호화 작업을 수행합니다. 해당 함수를 Amazon S3 버킷에 저장합니다.",
    "SelectB_Commentary": "직접 암호화 라이브러리를 관리하고 Lambda의 배포·버전을 관리해야 하므로 운영 복잡도가 상대적으로 높아집니다. 완전관리형 KMS보다 실시간 처리나 보안 관리 면에서 비효율적일 수 있습니다.",
    "SelectC": "AWS KMS Customer Managed Key를 생성하고, EC2 역할이 이 KMS 키를 암호화 작업에 사용할 수 있도록 허용합니다. 암호화된 데이터를 Amazon S3에 저장합니다.",
    "SelectC_Commentary": "KMS를 통해 안전하고 실시간에 가까운 암호화·복호화를 수행하고, Amazon S3에 저장함으로써 고가용성과 낮은 운영 부담을 동시에 달성할 수 있으므로 가장 적합한 솔루션입니다.",
    "SelectD": "AWS KMS Customer Managed Key를 생성하고, EC2 역할이 이 KMS 키를 암호화 작업에 사용할 수 있도록 허용합니다. 암호화된 데이터를 Amazon EBS 볼륨에 저장합니다.",
    "SelectD_Commentary": "Amazon EBS는 인스턴스 기반 스토리지로, Amazon S3 대비 고가용성이 낮으며 추가적인 백업 및 확장 구성이 필요해 운영 오버헤드가 더 큽니다."
  },
  {
    "Question_Number": "Q101",
    "Question_Description": "한 솔루션스 아키텍트가 IPv4 CIDR 블록을 사용하는 VPC를 설계하고 있으며, 공용 서브넷과 사설 서브넷을 구성하려고 합니다. 고가용성을 위해 3개의 AZ 각각에 공용 서브넷 1개, 사설 서브넷 1개씩 배치합니다. 공용 서브넷에는 인터넷 게이트웨이를 사용하여 인터넷 접속을 제공합니다. 사설 서브넷은 Amazon EC2 인스턴스에서 소프트웨어 업데이트를 다운로드하기 위해 인터넷에 접근해야 합니다. 사설 서브넷에서 인터넷에 접속하기 위해서는 어떻게 해야 합니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86019-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 사설 서브넷에서 패치나 업데이트를 위해 외부 인터넷에 접근해야 하는 구조를 어떻게 고가용성으로 구성할지 묻습니다. 가장 모범 사례는 각 AZ의 공용 서브넷에 NAT Gateway를 생성하고, 각 사설 서브넷의 라우팅을 해당 NAT Gateway로 보내는 것입니다. NAT Instance 대신 NAT Gateway를 권장하며, NAT Gateway는 반드시 공용 서브넷에 생성해야 합니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.2"
    ],
    "Keywords": [
      "사설 서브넷 인터넷 액세스",
      "공용 서브넷",
      "NAT Gateway",
      "고가용성"
    ],
    "Terms": [
      "NAT Gateway",
      "NAT Instance",
      "Egress-Only Internet Gateway",
      "인터넷 게이트웨이(IGW)",
      "공용 서브넷",
      "사설 서브넷",
      "VPC",
      "Route Table"
    ],
    "SelectA": "각 AZ 내 공용 서브넷에 NAT Gateway를 하나씩 생성하고, 각 AZ에 대한 사설 라우트 테이블을 생성하여 VPC 외부 트래픽을 해당 AZ의 NAT Gateway로 포워딩합니다.",
    "SelectA_Commentary": "NAT Gateway는 공용 서브넷에 위치해야 하며, 사설 라우트 테이블에서 0.0.0.0/0을 NAT Gateway로 라우팅함으로써 사설 서브넷 인스턴스가 인터넷 통신을 안전하고 간단하게 수행할 수 있습니다.",
    "SelectB": "각 AZ 내 사설 서브넷에 NAT Instance를 하나씩 생성하고, 각 AZ에 대한 사설 라우트 테이블을 생성하여 VPC 외부 트래픽을 해당 AZ의 NAT Instance로 포워딩합니다.",
    "SelectB_Commentary": "NAT Instance는 공용 서브넷에 있어야 인터넷 게이트웨이를 통해 외부와 통신할 수 있습니다. 사설 서브넷에 직접 배치하면 인터넷 접근이 불가능하므로 올바르지 않습니다.",
    "SelectC": "사설 서브넷 중 하나에 두 번째 인터넷 게이트웨이를 생성하고, 사설 서브넷의 라우트 테이블을 변경하여 VPC 외부 트래픽을 새 인터넷 게이트웨이로 포워딩합니다.",
    "SelectC_Commentary": "인터넷 게이트웨이는 VPC 단위로만 연결되며 서브넷마다 별도로 설치할 수 없습니다. 게다가 한 VPC에는 한 개의 인터넷 게이트웨이만 연결 가능합니다.",
    "SelectD": "공용 서브넷 중 하나에 Egress-Only Internet Gateway를 생성하고, 사설 서브넷의 라우트 테이블을 변경하여 VPC 외부 트래픽을 Egress-Only Internet Gateway로 포워딩합니다.",
    "SelectD_Commentary": "Egress-Only Internet Gateway는 IPv6 전용 트래픽에 사용되므로, IPv4 기반 사설 서브넷의 인터넷 다운로드 요구사항을 충족하지 못합니다."
  },
  {
    "Question_Number": "Q102",
    "Question_Description": "한 회사가 온프레미스 데이터 센터를 AWS로 마이그레이션하려고 합니다. 해당 데이터 센터에는 SFTP 서버가 있으며, 이 서버는 NFS 기반 파일 시스템에 데이터를 저장하고 있습니다. 현재 서버에는 전송해야 할 데이터가 200GB 있습니다. 이 서버는 Amazon EFS 파일 시스템을 사용하는 Amazon EC2 인스턴스에서 호스팅되어야 합니다. 이 작업을 자동화하기 위해 솔루션스 아키텍트는 어떤 단계 조합을 수행해야 합니까? (2개를 선택하십시오.)",
    "Answer": "B,E",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85814-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 온프레미스 SFTP 서버의 데이터를 Amazon EFS로 자동 전송하는 방안을 묻습니다. AWS DataSync agent를 온프레미스에 설치한 뒤, SFTP 서버 위치를 DataSync로 설정해 자동화 전송을 구현하는 것이 핵심입니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1"
    ],
    "Keywords": [
      "온프레미스 데이터 센터 마이그레이션",
      "SFTP 서버",
      "NFS 기반 파일 시스템",
      "Amazon EC2 인스턴스",
      "Amazon EFS",
      "AWS DataSync"
    ],
    "Terms": [
      "SFTP server",
      "NFS-based file system",
      "Amazon EC2",
      "Amazon EFS",
      "AWS DataSync",
      "DataSync agent",
      "Amazon EBS"
    ],
    "SelectA": "EC2 인스턴스를 EFS 파일 시스템과 동일한 Availability Zone에 Launch합니다.",
    "SelectA_Commentary": "EFS는 여러 AZ에 걸쳐 사용할 수 있어 같은 AZ에 EC2를 배치할 필요가 없으며, 자동화 전송과 직접적인 연관성이 적습니다.",
    "SelectB": "온프레미스 데이터 센터에 AWS DataSync agent를 설치합니다.",
    "SelectB_Commentary": "자동화된 데이터 전송을 위해서는 온프레미스측에 DataSync agent가 필요하며, 이를 통해 대량 데이터를 안전하게 AWS로 이동할 수 있습니다.",
    "SelectC": "EC2 인스턴스에 데이터를 저장할 보조 Amazon EBS 볼륨을 생성합니다.",
    "SelectC_Commentary": "Amazon EFS를 이미 사용할 예정이므로, 추가적인 EBS 볼륨은 필요 없으며 자동화 전송과도 직접 연관이 없습니다.",
    "SelectD": "수동으로 운영체제 copy 명령어를 사용해 데이터를 EC2 인스턴스로 전송합니다.",
    "SelectD_Commentary": "수동 복사는 자동화되지 않고, 대규모 데이터 이전에 비효율적이므로 문제 요구사항에 부합하지 않습니다.",
    "SelectE": "AWS DataSync를 사용해 온프레미스 SFTP 서버에 대한 적절한 location 구성을 생성합니다.",
    "SelectE_Commentary": "SFTP 서버에서 EFS로 자동으로 데이터를 이동하려면 DataSync location 구성이 필수이므로, 이는 자동화 전송을 위한 핵심 단계입니다."
  },
  {
    "Question_Number": "Q103",
    "Question_Description": "한 회사가 매일 같은 시간에 실행되는 AWS Glue ETL 작업을 보유하고 있습니다. 이 작업은 Amazon S3 버킷에 저장된 XML 데이터를 처리합니다. 매일 새 데이터가 S3 버킷에 추가되지만, 솔루션스 아키텍트는 AWS Glue가 매번 모든 데이터를 다시 처리하고 있음을 발견했습니다. 이전에 처리된 데이터를 재처리하지 않도록 하려면 어떻게 해야 합니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85781-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "AWS Glue Job Bookmark 기능을 사용하면 이전 실행에서 처리한 데이터를 기록해 반복 처리를 방지할 수 있습니다. 이를 적용하면 매일 추가되는 새로운 데이터만 효율적으로 처리할 수 있어 불필요한 리소스 사용과 시간을 줄일 수 있습니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.5"
    ],
    "Keywords": [
      "AWS Glue",
      "ETL 작업",
      "XML 데이터",
      "S3 버킷",
      "재처리 방지",
      "job bookmark"
    ],
    "Terms": [
      "AWS Glue",
      "Amazon S3",
      "ETL",
      "Job Bookmark",
      "FindMatches",
      "NumberOfWorkers",
      "XML Data"
    ],
    "SelectA": "Edit the job to use job bookmarks.",
    "SelectA_Commentary": "Job Bookmark을 사용하면 이전에 처리된 데이터를 건너뛰고 새로 추가된 데이터만 처리할 수 있어 효율적입니다.",
    "SelectB": "Edit the job to delete data after the data is processed.",
    "SelectB_Commentary": "데이터 삭제는 재처리를 방지할 수 있지만, 보관 및 분석 목적으로 기존 데이터가 필요할 수 있어 권장되지 않습니다.",
    "SelectC": "Edit the job by setting the NumberOfWorkers field to 1.",
    "SelectC_Commentary": "작업자 수를 변경해도 이전 데이터 재처리를 막지는 못합니다. 성능 조정과 관련됩니다.",
    "SelectD": "Use a FindMatches machine learning (ML) transform.",
    "SelectD_Commentary": "FindMatches는 유사 레코드를 식별하는 용도로, 이전 데이터 재처리 방지와는 직접적으로 관련이 없습니다."
  },
  {
    "Question_Number": "Q104",
    "Question_Description": "한 솔루션스 아키텍트가 웹사이트의 고가용성 인프라를 설계해야 합니다. 이 웹사이트는 Amazon EC2 인스턴스에서 동작하는 Windows 웹 서버에 의해 제공됩니다. 솔루션스 아키텍트는 수천 개의 IP 주소에서 발생하는 대규모 DDoS 공격을 완화할 수 있는 솔루션을 구현해야 합니다. 웹사이트에 다운타임이 발생하면 안 됩니다. 이러한 공격으로부터 웹사이트를 보호하기 위해 어떤 조치를 취해야 합니까? (2개를 선택하십시오.)",
    "Answer": "A,C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85342-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 대규모 DDoS 공격으로부터 웹사이트를 보호해야 하며, 다운타임이 허용되지 않는 상황을 다룹니다. AWS Shield Advanced는 L3/L4를 포함한 다양한 계층에서의 보호를 제공하며, Amazon CloudFront는 트래픽을 분산시켜 공격 규모를 줄입니다. 보안 관점에서 효율적인 방어와 동시에 고가용성을 유지하려면 이 두 가지를 함께 활용하는 전략이 최적입니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.2"
    ],
    "Keywords": [
      "고가용성 웹사이트",
      "대규모 DDoS 공격",
      "AWS Shield Advanced",
      "Amazon CloudFront",
      "Downtime 방지"
    ],
    "Terms": [
      "AWS Shield Advanced",
      "Amazon GuardDuty",
      "Amazon CloudFront",
      "AWS Lambda",
      "VPC Network ACL",
      "EC2 Spot Instances",
      "Auto Scaling",
      "Target Tracking Scaling Policy"
    ],
    "SelectA": "AWS Shield Advanced를 사용하여 DDoS 공격을 중단합니다.",
    "SelectA_Commentary": "AWS Shield Advanced는 대규모 DDoS 공격 방어에 최적화된 서비스이며, 고급 모니터링과 자동 완화 기능을 제공합니다. 다운타임 방지에 매우 효과적입니다.",
    "SelectB": "Amazon GuardDuty를 구성하여 공격자를 자동으로 차단합니다.",
    "SelectB_Commentary": "Amazon GuardDuty는 위협 탐지 서비스로, 공격 발생 사실을 알려주지만 직접적으로 DDoS 트래픽을 차단하지는 않습니다. 완전한 방어책에 적합하지 않습니다.",
    "SelectC": "웹사이트를 정적 및 동적 콘텐츠 모두 Amazon CloudFront를 통해 제공하도록 구성합니다.",
    "SelectC_Commentary": "Amazon CloudFront는 엣지 로케이션을 통해 전 세계 트래픽을 분산시켜 공격을 완화하고, 지연 시간을 줄여 고가용성을 유지하는 데 도움이 됩니다.",
    "SelectD": "AWS Lambda 함수를 사용하여 공격자 IP 주소를 자동으로 VPC Network ACL에 추가합니다.",
    "SelectD_Commentary": "일시적으로는 도움이 되지만 공격 IP가 수천 개에 달할 수 있어 관리 복잡성이 증가합니다. 대규모 공격에 즉각적으로 대응하기엔 한계가 있습니다.",
    "SelectE": "EC2 Spot Instances를 Auto Scaling group에서 CPU 사용률 80%로 목표 추적 확장 정책과 함께 사용합니다.",
    "SelectE_Commentary": "확장 정책은 트래픽 급증 시 임시적으로 대응할 수는 있지만, 근본적인 DDoS 방어책이 아니며 Spot 특성상 인스턴스가 회수될 위험도 있습니다."
  },
  {
    "Question_Number": "Q105",
    "Question_Description": "한 회사가 새로운 서버리스 워크로드를 배포할 준비를 하고 있습니다. Solutions Architect는 AWS Lambda function을 실행할 최소 권한 원칙(Principle of Least Privilege)을 준수하도록 권한을 설정해야 합니다. Amazon EventBridge(Amazon CloudWatch Events) 규칙이 이 함수를 호출합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85816-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 EventBridge가 AWS Lambda 함수를 호출할 때 필요한 권한 설정을 최소 권한 원칙에 맞춰 구성하는 방법을 묻습니다. EventBridge가 함수를 직접 호출하려면 함수에 Resource-based policy를 부여하여 events.amazonaws.com에 lambda:InvokeFunction 액션만 허용하면 됩니다. 이는 Role보다 직접적인 방식으로 외부 서비스가 함수를 호출할 수 있도록 허용하며, 필요 이상으로 광범위한 권한을 부여하지 않습니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1"
    ],
    "Keywords": [
      "AWS Lambda",
      "EventBridge",
      "최소 권한 원칙",
      "Resource-based policy"
    ],
    "Terms": [
      "AWS Lambda",
      "Amazon EventBridge (Amazon CloudWatch Events)",
      "lambda:InvokeFunction",
      "Service: events.amazonaws.com",
      "Resource-based policy",
      "Execution role",
      "Least Privilege"
    ],
    "SelectA": "함수에 실행 역할을 추가하고, 작업을 lambda:InvokeFunction, Principal을 * 로 설정합니다.",
    "SelectA_Commentary": "Principal 값이 * 로 설정되면 누구나 함수 호출이 가능해져 최소 권한 원칙을 위배하므로 적절하지 않습니다.",
    "SelectB": "함수에 실행 역할을 추가하고, 작업을 lambda:InvokeFunction, Principal을 Service: lambda.amazonaws.com 로 설정합니다.",
    "SelectB_Commentary": "lambda.amazonaws.com은 Lambda 자체 서비스에 대한 Principal입니다. EventBridge가 함수를 호출하려면 events.amazonaws.com에 대한 권한이 필요합니다.",
    "SelectC": "함수에 Resource-based policy를 추가하고, 작업을 lambda:* 로, Principal을 Service: events.amazonaws.com 로 설정합니다.",
    "SelectC_Commentary": "lambda:* 는 InvokeFunction 이상의 권한을 포함할 수 있어 최소 권한 원칙에 맞지 않습니다.",
    "SelectD": "함수에 Resource-based policy를 추가하고, 작업을 lambda:InvokeFunction, Principal을 Service: events.amazonaws.com 로 설정합니다.",
    "SelectD_Commentary": "EventBridge 이벤트가 함수를 호출하기 위해 필요한 정확한 액션(lambda:InvokeFunction)과 Principal(events.amazonaws.com)만 허용하므로 최소 권한 원칙을 준수합니다."
  },
  {
    "Question_Number": "Q106",
    "Question_Description": "한 회사가 기밀 데이터를 Amazon S3에 저장하려고 합니다. 컴플라이언스 요건상 데이터는 저장 시점에서 암호화되어야 하며, 암호화 키 사용 내역이 감사 목적으로 로깅되어야 합니다. 또한 키는 매년 교체(회전)되어야 합니다. 이 요구 사항을 충족하며 운영 효율성이 가장 높은 솔루션은 무엇입니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85817-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 S3에 저장되는 기밀 정보를 암호화하고, 키 사용 이력을 로깅하며, 매년 키를 교체해야 하는 요구 사항을 만족해야 합니다. SSE-KMS 자동 키 회전은 로깅과 연간 회전을 자동화해 운영 부담을 최소화합니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.3"
    ],
    "Keywords": [
      "Amazon S3",
      "기밀 데이터",
      "암호화",
      "키 사용 로깅",
      "연간 키 회전",
      "SSE-KMS",
      "자동 키 교체"
    ],
    "Terms": [
      "Server-side encryption",
      "SSE-C",
      "SSE-S3",
      "SSE-KMS",
      "AWS KMS",
      "Key Rotation",
      "자동 키 회전",
      "Manual Rotation"
    ],
    "SelectA": "Server-side encryption with customer-provided keys (SSE-C)",
    "SelectA_Commentary": "고객이 직접 키를 관리해야 하며, 연간 회전과 로깅도 직접 처리해야 합니다. 운영 효율성이 떨어집니다.",
    "SelectB": "Server-side encryption with Amazon S3 managed keys (SSE-S3)",
    "SelectB_Commentary": "S3가 자체 관리하는 키지만, 키 사용 로깅과 연간 키 교체 제어가 제한적이라 감사 요건 충족이 어렵습니다.",
    "SelectC": "Server-side encryption with AWS KMS keys (SSE-KMS) with manual rotation",
    "SelectC_Commentary": "AWS KMS로 키 사용을 로깅할 수 있지만, 키 회전을 사람이 직접 수행해야 하므로 운영 비용이 증가합니다.",
    "SelectD": "Server-side encryption with AWS KMS keys (SSE-KMS) with automatic rotation",
    "SelectD_Commentary": "키 사용 로깅이 자동으로 제공되고, 연간 키 회전도 자동화되어 운영 복잡도를 최소화하는 최적의 솔루션입니다."
  },
  {
    "Question_Number": "Q107",
    "Question_Description": "한 자전거 공유 회사가 피크 시간대에 자전거 위치를 추적하기 위한 멀티 계층 아키텍처를 개발하고 있습니다. 회사는 이 위치 데이터 포인트를 기존 애널리틱스 플랫폼에서 활용하려고 합니다. 솔루션스 아키텍트는 이 아키텍처를 지원하기 위한 가장 적합한 멀티 계층 방안을 결정해야 합니다. 위치 데이터 포인트는 REST API를 통해 액세스 가능해야 합니다. 이러한 요구사항을 충족하기 위해 위치 데이터를 저장하고 조회하기 위한 방법은 무엇입니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85212-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 REST API를 통해 빠르게 위치 데이터를 수집하고, 기존 분석 플랫폼에서 활용하기 위한 멀티 계층 아키텍처를 구성하는 방법을 묻습니다. 빠른 호출과 간단한 연동이 가능한 Amazon API Gateway와 AWS Lambda를 사용하는 것이 핵심 포인트입니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.1",
      "3.5"
    ],
    "Keywords": [
      "자전거 위치 추적",
      "멀티 계층 아키텍처",
      "피크 시간대",
      "기존 애널리틱스 플랫폼",
      "REST API",
      "위치 데이터"
    ],
    "Terms": [
      "Amazon Athena",
      "Amazon S3",
      "Amazon API Gateway",
      "AWS Lambda",
      "Amazon QuickSight",
      "Amazon Redshift",
      "Amazon Kinesis Data Analytics",
      "REST API"
    ],
    "SelectA": "Amazon Athena와 Amazon S3를 사용합니다.",
    "SelectA_Commentary": "단순 스토리지와 쿼리 서비스로 REST API 접근을 제공하기 어렵습니다. 저장은 가능하나 실시간 처리나 API 호출과의 연계가 부족합니다.",
    "SelectB": "Amazon API Gateway와 AWS Lambda를 사용합니다.",
    "SelectB_Commentary": "REST API 엔드포인트와 Lambda 함수를 통해 실시간으로 위치 데이터를 수집하고 빠르게 분석 플랫폼으로 전달할 수 있는 가장 적합한 솔루션입니다.",
    "SelectC": "Amazon QuickSight와 Amazon Redshift를 사용합니다.",
    "SelectC_Commentary": "분석 및 시각화 용도로 적합하지만, 실시간 REST API 접근성을 위한 멀티 계층 아키텍처로 사용하기에는 제한적입니다.",
    "SelectD": "Amazon API Gateway와 Amazon Kinesis Data Analytics를 사용합니다.",
    "SelectD_Commentary": "Kinesis Data Analytics는 스트리밍 데이터 분석을 위한 서비스이지만, 기존 플랫폼에 데이터를 즉시 연결하는 데는 과도하며 필요 이상으로 복잡합니다."
  },
  {
    "Question_Number": "Q108",
    "Question_Description": "한 회사가 자동차 판매 웹사이트를 운영하며, Amazon RDS에 데이터베이스를 두고 자동차 매물 정보를 저장하고 있습니다. 어떤 자동차가 판매되면 이 매물은 웹사이트에서 제거되어야 하며, 해당 데이터는 여러 대상 시스템으로 전송되어야 합니다. 이러한 요구사항을 만족하기 위해 솔루션스 아키텍트는 어떤 설계를 추천해야 합니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85427-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 RDS 데이터를 다른 여러 시스템으로 동시에 전달해야 하며, RDS 자체 이벤트로는 데이터 업데이트를 감지할 수 없다는 점이 핵심입니다. 따라서 Lambda 함수를 통해 DB 변경 시점을 애플리케이션 계층에서 감지하고 SQS를 이용해 비동기적으로 여러 목표 시스템에 데이터를 전송하는 방식이 적절합니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1"
    ],
    "Keywords": [
      "자동차 판매 웹사이트",
      "매물 삭제",
      "다중 대상 시스템 전송",
      "데이터베이스 업데이트",
      "Amazon RDS"
    ],
    "Terms": [
      "Amazon RDS",
      "AWS Lambda",
      "Amazon Simple Queue Service (Amazon SQS)",
      "Amazon Simple Notification Service (Amazon SNS)",
      "RDS event notification",
      "FIFO queue",
      "fanout"
    ],
    "SelectA": "데이터베이스(Amazon RDS)가 업데이트될 때 트리거되는 AWS Lambda 함수를 만들어, 해당 정보를 Amazon Simple Queue Service(Amazon SQS) 큐에 전송하여 대상들이 소비하도록 구성합니다.",
    "SelectA_Commentary": "DB 업데이트 시 직접 Lambda를 호출하여 SQS에 메시지를 전달하면, 여러 시스템에 비동기적으로 분산할 수 있어 확장성과 작업 분리를 모두 달성할 수 있으므로 올바른 솔루션입니다.",
    "SelectB": "데이터베이스(Amazon RDS)가 업데이트될 때 트리거되는 AWS Lambda 함수를 만들어, Amazon Simple Queue Service(Amazon SQS) FIFO 큐에 정보를 전송하여 대상들이 소비하도록 구성합니다.",
    "SelectB_Commentary": "FIFO 큐가 필요한 순서 보장이 요구되지 않으며, 단순한 이벤트 전파에 불필요한 과도한 기능으로 오히려 처리량과 확장성에 제약이 생길 수 있으므로 적합하지 않습니다.",
    "SelectC": "RDS 이벤트 알림에 가입하고 Amazon Simple Queue Service(Amazon SQS) 큐를 여러 Amazon Simple Notification Service(Amazon SNS) 토픽으로 퍼나르게 구성합니다. 그 후 AWS Lambda 함수를 사용하여 대상들을 업데이트합니다.",
    "SelectC_Commentary": "RDS 이벤트 알림은 데이터 자체 변경(INSERT, UPDATE, DELETE)을 지원하지 않아 이를 활용한 팬아웃 구조는 의미가 없어 요구사항을 충족하지 못합니다.",
    "SelectD": "RDS 이벤트 알림에 가입하고 Amazon Simple Notification Service(Amazon SNS) 토픽을 여러 Amazon Simple Queue Service(Amazon SQS) 큐로 퍼나르게 구성합니다. 그 후 AWS Lambda 함수를 사용하여 대상들을 업데이트합니다.",
    "SelectD_Commentary": "마찬가지로 RDS 이벤트 알림에서 DB 내부 데이터 변경 이벤트를 제공하지 않으므로, SNS와 SQS를 통한 팬아웃이 구현되지 않아 목적을 달성할 수 없습니다."
  },
  {
    "Question_Number": "Q109",
    "Question_Description": "한 회사가 Amazon S3에 데이터를 저장해야 하며, 데이터가 변경되지 않도록 해야 합니다. 회사는 Amazon S3에 업로드되는 새 객체가 회사가 객체를 수정하기로 결정할 때까지 불특정 기간 변경 불가능하게 유지되길 원합니다. 또한 회사의 AWS 계정에서 오직 특정 사용자만 해당 객체를 삭제할 수 있도록 해야 합니다. 이러한 요구 사항을 충족하기 위해 솔루션스 아키텍트는 어떻게 해야 합니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85634-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 Amazon S3에 업로드된 객체가 회사가 의도하기 전까지 수정·삭제되지 않도록 설정하는 방법을 묻습니다. S3 Object Lock의 Legal hold는 만료 기간 없이 객체를 잠그고, 특정 권한을 가진 사용자만 해제할 수 있어 요구 사항에 부합합니다. 버저닝과 함께 사용하면 기존 객체가 변경되더라도 이전 버전을 보존할 수 있습니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.3"
    ],
    "Keywords": [
      "Amazon S3",
      "S3 Object Lock",
      "불특정 기간 변경 불가",
      "Legal hold",
      "특정 사용자 삭제 권한",
      "Versioning"
    ],
    "Terms": [
      "S3 Glacier vault",
      "WORM (Write-Once, Read-Many) vault lock policy",
      "S3 Object Lock",
      "Versioning",
      "Retention period",
      "Governance mode",
      "Legal hold",
      "AWS CloudTrail",
      "s3:PutObjectLegalHold",
      "IAM policy"
    ],
    "SelectA": "S3 Glacier vault를 생성하고 WORM vault lock policy를 적용합니다.",
    "SelectA_Commentary": "Vault lock 정책은 확대 적용되며, 특정 사용자만 삭제 권한을 갖도록 세분화하기가 용이하지 않습니다.",
    "SelectB": "S3 Object Lock을 활성화한 S3 버킷과 100년 보존기간을 설정하고 governance mode로 기본 모드를 설정합니다.",
    "SelectB_Commentary": "명확히 정해진 보존 기간(100년)이 필요하므로, 불특정 기간 보존 요구사항에는 맞지 않습니다.",
    "SelectC": "S3 버킷을 생성하고, AWS CloudTrail로 모든 S3 API 이벤트를 추적한 뒤 변경 사실이 있으면 백업에서 복원합니다.",
    "SelectC_Commentary": "변경 발생 후 복원 방식이므로 객체가 아예 변경되지 않도록 막는 요구사항과 맞지 않습니다.",
    "SelectD": "S3 Object Lock을 활성화한 S3 버킷을 생성하고, 버저닝을 활성화한 뒤 객체에 legal hold를 추가합니다. s3:PutObjectLegalHold IAM 권한을 부여받은 사용자만 해당 객체를 삭제할 수 있게 합니다.",
    "SelectD_Commentary": "Legal hold는 별도 기간 없이 객체를 잠글 수 있으며 특정 IAM 권한을 통해서만 해제가 가능하므로 요구사항을 충족하는 가장 적합한 옵션입니다."
  },
  {
    "Question_Number": "Q110",
    "Question_Description": "한 소셜 미디어 회사는 사용자가 웹사이트에 이미지를 업로드하도록 허용합니다. 웹사이트는 Amazon EC2 인스턴스에서 동작합니다. 업로드 요청 중 웹사이트는 이미지를 표준 크기로 리사이즈한 뒤 Amazon S3에 저장합니다. 현재 사용자들은 업로드 요청이 느리다고 호소하고 있습니다. 회사는 애플리케이션 내 결합도를 낮추고 웹사이트 성능을 향상해야 합니다. 솔루션스 아키텍트는 이미지 업로드 과정을 가장 운영 효율적으로 설계해야 합니다. 이 요구사항을 충족하기 위해 필요한 조치의 조합은 무엇입니까? (두 개를 선택하세요.)",
    "Answer": "C,D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86471-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 이미지 업로드 시 동기적 리사이즈로 인해 웹 서버에 부하가 집중되고, 이 과정이 애플리케이션과 강하게 결합되어 있어 성능이 저하되는 상황입니다. Pre-Signed URL을 통해 브라우저가 직접 Amazon S3로 이미지를 업로드하도록 분산하면 웹 서버의 부담이 크게 감소합니다. 이후 S3 Event Notifications를 사용해 AWS Lambda 함수로 비동기 이미지 리사이즈를 수행하면 애플리케이션의 결합도가 낮아지고, 업로드 성능과 확장성을 효과적으로 높일 수 있습니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1"
    ],
    "Keywords": [
      "이미지 업로드",
      "웹사이트 성능",
      "느슨한 결합",
      "Amazon S3",
      "AWS Lambda"
    ],
    "Terms": [
      "Amazon EC2",
      "Amazon S3",
      "S3 Glacier",
      "AWS Lambda",
      "S3 Event Notifications",
      "Amazon EventBridge (Amazon CloudWatch Events)",
      "Pre-Signed URL"
    ],
    "SelectA": "애플리케이션이 이미지를 S3 Glacier로 업로드하도록 설정합니다.",
    "SelectA_Commentary": "S3 Glacier는 장기 보관용 스토리지 클래스로, 업로드 시점에 쓰기에는 부적합하며 실시간 접근성과 성능 개선에도 도움이 되지 않습니다.",
    "SelectB": "웹 서버가 원본 이미지를 Amazon S3로 업로드하도록 설정합니다.",
    "SelectB_Commentary": "웹 서버에서 직접 업로드하면 일부 개선은 가능하지만, 여전히 웹 서버가 관여하여 결합을 완전히 해소하지 못하고 부하가 남아있습니다.",
    "SelectC": "Pre-Signed URL을 사용하여 각 사용자의 브라우저에서 Amazon S3로 직접 이미지를 업로드하도록 애플리케이션을 설정합니다.",
    "SelectC_Commentary": "정답. 이렇게 하면 웹 서버의 중간 처리 없이 브라우저가 직접 업로드하여 결합을 줄이고 웹 서버 부하를 크게 낮출 수 있습니다.",
    "SelectD": "이미지가 업로드될 때 AWS Lambda 함수를 호출하도록 S3 Event Notifications를 구성하고, 해당 함수에서 이미지를 리사이즈합니다.",
    "SelectD_Commentary": "정답. 업로드 후 비동기적으로 이미지를 리사이즈하므로, 웹 서버 성능을 개선하고 애플리케이션 결합도를 줄이는 효과가 있습니다.",
    "SelectE": "Amazon EventBridge (Amazon CloudWatch Events) 룰을 만들어 일정에 따라 AWS Lambda 함수를 호출하여 업로드된 이미지를 리사이즈합니다.",
    "SelectE_Commentary": "스케줄 기반 처리는 실시간으로 이미지를 리사이즈하기 어려워 사용자의 즉각적 경험을 만족시키기 어렵고, 진정한 애플리케이션 결합 해소에도 한계가 있습니다."
  },
  {
    "Question_Number": "Q111",
    "Question_Description": "한 회사가 최근 메시지 처리 시스템을 AWS로 마이그레이션했습니다. 이 시스템은 Amazon EC2에서 실행되는 ActiveMQ 큐를 통해 메시지를 수신하고, Amazon EC2에서 실행되는 consumer 애플리케이션이 메시지를 처리하여 Amazon EC2에서 구동되는 MySQL 데이터베이스에 결과를 기록합니다. 회사는 낮은 운영 복잡도로 높은 가용성을 달성하기를 원합니다. 다음 중 가장 높은 가용성을 제공하는 아키텍처는 무엇입니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85910-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 메시지 큐, 애플리케이션, 데이터베이스를 모두 이중화하거나 관리형 서비스를 사용하여 운영 복잡도를 줄이면서 고가용성을 달성하는 방법을 묻습니다. 관리형 Amazon MQ, Auto Scaling group, 그리고 Amazon RDS for MySQL Multi-AZ로 구성된 D안이 가장 높은 가용성을 보장합니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1",
      "2.2"
    ],
    "Keywords": [
      "메시지 처리",
      "가용성",
      "운영 복잡도",
      "Multi-AZ",
      "Auto Scaling group"
    ],
    "Terms": [
      "Amazon EC2",
      "ActiveMQ",
      "consumer 애플리케이션",
      "Amazon MQ",
      "active/standby brokers",
      "MySQL",
      "Amazon RDS for MySQL",
      "Multi-AZ",
      "Auto Scaling group"
    ],
    "SelectA": "ActiveMQ 서버를 추가로 다른 Availability Zone에 배포하고, 추가 consumer EC2 인스턴스를 다른 Availability Zone에 두며, MySQL 데이터베이스를 다른 Availability Zone으로 복제합니다.",
    "SelectA_Commentary": "직접 ActiveMQ와 MySQL 복제를 구성해야 하므로 관리 오버헤드가 크고, RDS Multi-AZ 활용이 없어 고가용성 수준이 떨어집니다.",
    "SelectB": "두 개의 Availability Zone에 걸쳐 active/standby brokers를 구성한 Amazon MQ를 사용하고, 추가 consumer EC2 인스턴스를 다른 Availability Zone에 배포합니다. MySQL 데이터베이스는 다른 Availability Zone으로만 복제합니다.",
    "SelectB_Commentary": "Amazon MQ는 관리형이지만 MySQL은 RDS Multi-AZ가 아니므로, 수동 복제가 필요해 운영 복잡성이 있고 완전한 고가용성을 보장하기 어렵습니다.",
    "SelectC": "두 개의 Availability Zone에 걸쳐 active/standby brokers를 구성한 Amazon MQ를 사용하고, 추가 consumer EC2 인스턴스를 다른 Availability Zone에 배포합니다. MySQL 데이터베이스는 Multi-AZ를 활성화한 Amazon RDS for MySQL을 사용합니다.",
    "SelectC_Commentary": "DB와 MQ는 고가용성 옵션이지만 consumer EC2 인스턴스가 단순 이중화만 되어 Auto Scaling group이 없어 장애 시 자동 복구가 제한적입니다.",
    "SelectD": "두 개의 Availability Zone에 걸쳐 active/standby brokers를 구성한 Amazon MQ를 사용하고, Auto Scaling group을 통해 두 개의 Availability Zone 전반에 consumer EC2 인스턴스를 배포합니다. MySQL 데이터베이스는 Multi-AZ를 활성화한 Amazon RDS for MySQL을 사용합니다.",
    "SelectD_Commentary": "MQ, DB, 그리고 consumer 인스턴스 모두 관리형 고가용성 구성을 적용해, 장애에 즉시 대응하면서 운영 복잡도를 최소화하는 가장 완벽한 솔루션입니다."
  },
  {
    "Question_Number": "Q112",
    "Question_Description": "한 회사가 내부 데이터 센터의 서버 여러 대에서 컨테이너화된 웹 애플리케이션을 운영하여 들어오는 요청을 처리하고 있습니다. 요청 수가 빠르게 증가하여 내부 서버들이 더 이상 늘어난 요청을 감당하지 못하고 있습니다. 회사는 애플리케이션 코드를 최소한으로 수정하고 개발 노력을 최소화하면서 AWS로 이전하고자 합니다. 이러한 요구사항을 충족하며 오퍼레이셔널 오버헤드를 가장 적게 소모하는 솔루션은 무엇입니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85913-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 내부 서버에서 운영 중인 컨테이너화된 웹 애플리케이션을 최소한의 코드 변경으로 AWS로 이전하는 상황입니다. AWS Fargate를 사용하면 EC2 인스턴스 관리를 생략해 오퍼레이셔널 오버헤드가 줄어들고, Service Auto Scaling과 ALB로 높은 확장성을 확보할 수 있습니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1"
    ],
    "Keywords": [
      "컨테이너화된 웹 애플리케이션",
      "오퍼레이셔널 오버헤드",
      "AWS로 이전",
      "AWS Fargate",
      "Service Auto Scaling",
      "Application Load Balancer"
    ],
    "Terms": [
      "AWS Fargate",
      "Amazon ECS",
      "Service Auto Scaling",
      "Application Load Balancer",
      "Amazon EC2",
      "AWS Lambda",
      "Amazon API Gateway",
      "AWS ParallelCluster"
    ],
    "SelectA": "AWS Fargate를 사용하는 Amazon ECS에서 컨테이너화된 웹 애플리케이션을 실행하고 Service Auto Scaling을 설정합니다. Application Load Balancer를 통해 요청을 분산합니다.",
    "SelectA_Commentary": "AWS Fargate는 서버 관리를 대신 처리하므로 운영 부담이 낮고, Service Auto Scaling과 ALB를 통해 손쉽게 확장성을 확보할 수 있어 요구사항에 가장 적합합니다.",
    "SelectB": "Amazon EC2 인스턴스 두 대를 사용해 컨테이너화된 웹 애플리케이션을 배포하고 Application Load Balancer로 트래픽을 분산합니다.",
    "SelectB_Commentary": "EC2 인스턴스 관리가 필요하여 운영 부담이 늘어나고, 트래픽 증가 시 인스턴스 증설 등의 추가 작업이 필요해 오퍼레이셔널 오버헤드가 커집니다.",
    "SelectC": "AWS Lambda에서 지원되는 언어 중 하나로 새 코드를 작성하고 여러 Lambda function을 생성하여 부하를 처리합니다. Amazon API Gateway를 엔드포인트로 사용합니다.",
    "SelectC_Commentary": "기존 컨테이너 애플리케이션을 완전히 재개발해야 하므로 개발 노력이 상당하며, 최소한의 코드 변경이라는 목표에 맞지 않습니다.",
    "SelectD": "AWS ParallelCluster를 사용해 HPC 클러스터를 구성하고 증가하는 요청을 처리할 수 있도록 고성능 환경을 설정합니다.",
    "SelectD_Commentary": "HPC 클러스터는 대규모 과학 계산 등에 적합하며, 웹 트래픽 처리에는 과도한 설정과 복잡도가 추가되어 오퍼레이셔널 오버헤드가 커집니다."
  },
  {
    "Question_Number": "Q113",
    "Question_Description": "한 회사는 보고 용도로 50TB의 데이터를 사용하고 있습니다. 이 회사는 온프레미스에서 이 데이터를 AWS로 이전하려고 합니다. 회사 데이터 센터에는 매주 데이터 변환 작업을 수행하는 맞춤형 애플리케이션이 있으며, 회사는 데이터 전송 완료 후 최대한 빠르게 이 전송 작업을 시작하고자 합니다. 하지만 데이터 센터에는 추가 작업 부하를 처리할 만한 네트워크 대역폭이 없습니다. 솔루션스 아키텍트는 데이터를 전송하고 이 변환 작업을 AWS Cloud에서 계속 실행할 수 있도록 구성해야 합니다. 최소한의 운영 오버헤드로 이러한 요구사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85912-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 온프레미스에서 50TB 대규모 데이터를 빠른 시일 내에 전송해야 하며, 추가 네트워크 사용이 어려운 상황입니다. Snowball Edge Storage Optimized 디바이스를 이용해 물리적으로 데이터를 전송함과 동시에 AWS Glue로 자동화된 변환 작업을 구성하면, 운영 부담을 최소화하고 빠르게 문제를 해결할 수 있습니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.1",
      "3.5"
    ],
    "Keywords": [
      "50TB",
      "데이터 전송",
      "온프레미스",
      "Snowball Edge",
      "AWS Glue"
    ],
    "Terms": [
      "AWS DataSync",
      "AWS Glue",
      "AWS Snowcone",
      "Amazon EC2",
      "Snowball Edge",
      "Storage Optimized",
      "ETL"
    ],
    "SelectA": "AWS DataSync를 사용하여 데이터를 전송하고 AWS Glue를 사용하여 맞춤형 변환 작업을 생성합니다.",
    "SelectA_Commentary": "DataSync는 추가 네트워크 대역폭이 필요한데 데이터 센터에 여유가 없으므로 적절하지 않습니다.",
    "SelectB": "AWS Snowcone 디바이스를 주문하여 데이터를 전송하고, 변환 애플리케이션을 해당 디바이스에 배포합니다.",
    "SelectB_Commentary": "Snowcone은 최대 약 14TB 용량으로 50TB 데이터를 모두 담기에 부족합니다.",
    "SelectC": "AWS Snowball Edge Storage Optimized 디바이스를 주문하고 데이터를 디바이스에 복사합니다. AWS Glue를 사용해 맞춤형 변환 작업을 생성합니다.",
    "SelectC_Commentary": "대용량 Snowball Edge 디바이스를 통해 빠른 물리 전송이 가능하며, 서버리스 AWS Glue로 변환 작업을 수행하므로 운영 오버헤드가 가장 적은 해결책입니다.",
    "SelectD": "Amazon EC2 컴퓨팅이 포함된 AWS Snowball Edge Storage Optimized 디바이스를 주문하고 데이터를 디바이스에 복사합니다. AWS에서 새 EC2 인스턴스를 생성해 변환 애플리케이션을 실행합니다.",
    "SelectD_Commentary": "EC2 인스턴스 배포와 유지 관리가 필요해 운영 부담이 증가하며, C 옵션보다 오버헤드가 큽니다."
  },
  {
    "Question_Number": "Q114",
    "Question_Description": "한 회사가 사용자들이 사진을 업로드하고 자신의 이미지에 사진 테두리를 추가할 수 있는 이미지 분석 애플리케이션을 만들었습니다. 사용자는 이미지를 업로드하고, 어떤 사진 테두리를 추가할지 표시하기 위한 메타데이터도 함께 업로드합니다. 현재 애플리케이션은 단일 Amazon EC2 인스턴스와 Amazon DynamoDB를 사용하여 메타데이터를 저장하고 있습니다. 그러나 애플리케이션의 인기가 높아지면서 사용자 수가 증가하고, 하루 중 특정 시간대나 요일에 따라 동시 사용자 수가 크게 달라질 수 있습니다. 이 회사는 증가하는 사용자 수요를 충족하기 위해 애플리케이션이 확장 가능해야 함을 요구합니다. 이러한 요구사항을 만족하는 솔루션은 무엇입니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85189-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 애플리케이션이 동시 사용자 수 증가에 맞춰 유연하고 자동으로 확장할 수 있는 구조를 구성해야 하는 상황을 다룹니다. 이미지를 데이터베이스에 직접 저장하는 것은 비효율적이며, Lambda 등 서버리스 기반의 확장성과 저비용 스토리지인 Amazon S3를 조합해 아키텍처를 구성하는 것이 일반적인 모범 사례입니다. 따라서 이미지 처리를 Lambda로 수행하고 사진은 S3에 두며 메타데이터만 DynamoDB에 저장하면 쉽고 효과적으로 확장할 수 있습니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1"
    ],
    "Keywords": [
      "이미지 분석 애플리케이션",
      "사진 업로드",
      "메타데이터",
      "동시 사용자 확장",
      "AWS Lambda",
      "Amazon S3",
      "Amazon DynamoDB"
    ],
    "Terms": [
      "Amazon EC2",
      "Amazon DynamoDB",
      "AWS Lambda",
      "Amazon S3",
      "Amazon Kinesis Data Firehose",
      "Amazon EBS (io2)"
    ],
    "SelectA": "Use AWS Lambda to process the photos. Store the photos and metadata in DynamoDB.",
    "SelectA_Commentary": "이미지를 직접 DynamoDB에 저장하면 비용이 많이 들고 확장성 측면에서 비효율적이므로 적절하지 않습니다.",
    "SelectB": "Use Amazon Kinesis Data Firehose to process the photos and to store the photos and metadata.",
    "SelectB_Commentary": "Kinesis Data Firehose는 스트리밍 데이터 수집에 적합하지만, 이미지 파일 자체를 저장하고 처리하기엔 구조가 복잡하고 적절치 않습니다.",
    "SelectC": "Use AWS Lambda to process the photos. Store the photos in Amazon S3. Retain DynamoDB to store the metadata.",
    "SelectC_Commentary": "Lambda가 자동으로 확장되며, S3에 이미지를 저장해 비용과 확장성 문제를 해결하고, DynamoDB에는 메타데이터만 저장하여 가볍고 탄력적인 아키텍처를 구성하므로 정답입니다.",
    "SelectD": "Increase the number of EC2 instances to three. Use Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volumes to store the photos and metadata.",
    "SelectD_Commentary": "EC2 인스턴스 수를 늘리고 EBS를 사용하는 방식은 서버 관리를 직접 해야 하고, 빠르게 달라지는 수요에 따라 자동 확장이 제한적이므로 적절하지 않습니다."
  },
  {
    "Question_Number": "Q115",
    "Question_Description": "한 의료 기록 회사가 Amazon EC2 인스턴스에서 애플리케이션을 호스팅하고 있습니다. 이 애플리케이션은 Amazon S3에 저장된 고객 데이터 파일을 처리합니다. 현재 EC2 인스턴스는 public subnets에 위치해 있으며, 인터넷을 통해 Amazon S3에 액세스하고 있습니다. 그러나 EC2 인스턴스에는 그 외 다른 네트워크 액세스가 필요하지는 않습니다. 새로운 요구사항으로 인해 파일 전송에 대한 네트워크 트래픽은 인터넷이 아닌 사설 경로를 통해서만 전송되어야 합니다. 이 요구사항을 충족하기 위해 솔루션스 아키텍트는 네트워크 아키텍처에 어떤 변경 사항을 권장해야 합니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86031-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 EC2 인스턴스에서 Amazon S3로의 파일 전송을 퍼블릭 인터넷을 거치지 않고 사설 경로로 전송하게끔 네트워크를 재설계하는 방법을 묻습니다. 정답은 private subnets로 옮긴 뒤 VPC endpoint를 사용하여 트래픽이 인터넷에 노출되지 않는 경로로 전송하는 것입니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1",
      "1.3"
    ],
    "Keywords": [
      "파일 전송",
      "사설 경로",
      "public subnets",
      "private subnets",
      "Amazon EC2",
      "Amazon S3",
      "VPC endpoint",
      "internet gateway"
    ],
    "Terms": [
      "Amazon EC2",
      "Amazon S3",
      "public subnets",
      "private subnets",
      "VPC endpoint",
      "NAT gateway",
      "internet gateway",
      "security group",
      "S3 prefix list",
      "AWS Direct Connect",
      "route table"
    ],
    "SelectA": "NAT gateway를 생성합니다. public subnets의 route table을 구성하여 Amazon S3로의 트래픽을 NAT gateway를 통해 전송하도록 설정합니다.",
    "SelectA_Commentary": "NAT gateway는 사설 서브넷에서 아웃바운드 인터넷 접속을 제공할 때 사용되며, 여전히 인터넷을 통한 연결이 필요하므로 사설 경로 요구사항을 충족하지 못합니다.",
    "SelectB": "EC2 인스턴스에 대한 security group을 설정하여 아웃바운드 트래픽을 S3 prefix list로의 트래픽만 허용하도록 제한합니다.",
    "SelectB_Commentary": "Security group 규칙만으로는 인터넷을 우회하는 사설 경로를 확보할 수 없습니다. 트래픽 경로 자체가 여전히 인터넷을 포함하게 됩니다.",
    "SelectC": "EC2 인스턴스를 private subnets로 이동합니다. Amazon S3에 대한 VPC endpoint를 생성하고, 이 endpoint를 private subnets의 route table에 연결합니다.",
    "SelectC_Commentary": "EC2 인스턴스를 사설 서브넷에 배치하고 S3 VPC endpoint를 사용하면, 인터넷을 거치지 않고도 내부 경로를 통해 안전하게 S3에 접근할 수 있으므로 요구사항을 모두 만족시킵니다.",
    "SelectD": "VPC에서 internet gateway를 제거합니다. AWS Direct Connect 연결을 설정하고, Amazon S3로 가는 트래픽을 Direct Connect 연결을 통해 전송합니다.",
    "SelectD_Commentary": "Direct Connect는 온프레미스 환경과 AWS 간 사설 연결에 주로 사용되며, 이 문제에서는 사설 경로 확보에 과도한 솔루션으로 운영 복잡도와 비용이 증가합니다."
  },
  {
    "Question_Number": "Q116",
    "Question_Description": "한 회사가 인기 있는 CMS(콘텐츠 관리 시스템)를 사용해 기업 웹사이트를 운영하고 있었으나, 패치 및 유지보수에 대한 부담이 큽니다. 회사는 웹사이트를 새로 설계하려고 하며, 연 4회 업데이트만 필요하고 동적으로 생성되는 콘텐츠도 필요 없습니다. 이 솔루션은 높은 확장성과 보안을 제공해야 하며, 운영 오버헤드를 최소화해야 합니다. 다음 중 이러한 요구사항을 가장 적은 운영 오버헤드로 충족하는 솔루션 조합은 무엇입니까? (2개를 선택하세요.)",
    "Answer": "A,D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85996-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 정적 콘텐츠를 활용해 운영 부담을 줄이고, 동시에 높은 확장성과 보안을 구현하는 것이 핵심입니다. CloudFront와 S3 정적 웹 호스팅을 결합하면 손쉽게 HTTPS 접근과 전 세계적 콘텐츠 캐싱이 가능하여, 관리해야 할 인프라가 줄어들고 패치 부담도 없어집니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.1",
      "3.4"
    ],
    "Keywords": [
      "기업 웹사이트",
      "CMS 부담",
      "정적 사이트",
      "높은 확장성",
      "보안 강화",
      "운영 오버헤드 최소화"
    ],
    "Terms": [
      "Amazon CloudFront",
      "AWS WAF web ACL",
      "AWS Lambda",
      "Amazon S3",
      "Auto Scaling group",
      "Amazon EC2",
      "Application Load Balancer",
      "HTTPS"
    ],
    "SelectA": "Amazon CloudFront를 웹사이트 앞단에 구성하여 HTTPS 기능을 사용하도록 합니다.",
    "SelectA_Commentary": "CloudFront를 통해 전 세계 엣지 로케이션에서 캐싱하며 HTTPS 설정도 간단해, 보안과 확장성 모두를 향상시키는 효과적인 방식입니다.",
    "SelectB": "AWS WAF web ACL을 웹사이트 앞단에 배포해 HTTPS 기능을 제공합니다.",
    "SelectB_Commentary": "AWS WAF는 HTTPS를 직접 제공하지 않고, 주로 웹 공격 방어를 위한 보안 계층이므로 HTTPS 자체 제공 목적에는 적합하지 않습니다.",
    "SelectC": "웹사이트 콘텐츠를 관리하고 제공하기 위해 AWS Lambda 함수를 생성 후 배포합니다.",
    "SelectC_Commentary": "Lambda를 사용해 정적 콘텐츠를 서빙하려면 별도의 설정과 코드가 필요해 운영 복잡도가 오히려 증가합니다.",
    "SelectD": "새로운 웹사이트를 Amazon S3 버킷에 생성하고, 정적 웹사이트 호스팅을 활성화하여 웹사이트를 배포합니다.",
    "SelectD_Commentary": "정적 웹사이트 호스팅으로 S3를 이용하면 관리 서버 없이 확장성과 저비용을 달성할 수 있어 운영 부담이 크게 줄어듭니다.",
    "SelectE": "새로운 웹사이트를 생성 후, Application Load Balancer 뒤의 Amazon EC2 Auto Scaling 그룹을 통해 웹사이트를 배포합니다.",
    "SelectE_Commentary": "EC2 인스턴스와 Auto Scaling, 로드 밸런서 등을 설정해야 하므로 관리가 복잡해지고 오버헤드가 크게 증가합니다."
  },
  {
    "Question_Number": "Q117",
    "Question_Description": "한 회사가 Amazon CloudWatch Logs log group에 애플리케이션 로그를 저장하고 있습니다. 새로운 정책에 따라, 회사는 모든 애플리케이션 로그를 거의 실시간으로 Amazon OpenSearch Service(Amazon Elasticsearch Service)에 저장해야 합니다. 가장 적은 운영 오버헤드로 이 요구사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85802-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 CloudWatch Logs log group에 저장된 애플리케이션 로그를 Amazon OpenSearch Service(Amazon Elasticsearch Service)에 실시간에 가깝게 전달하는 방안을 묻습니다. 가장 운영 오버헤드가 적은 방법은 CloudWatch Logs 자체 기능을 사용하여 로그를 직접 스트리밍하는 것입니다. 별도의 추가 구성(예: Kinesis Data Firehose나 Lambda 함수, Kinesis Agent 설치 등)을 최소화함으로써 운영 비용과 복잡도를 줄일 수 있습니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.5"
    ],
    "Keywords": [
      "애플리케이션 로그",
      "CloudWatch Logs",
      "Amazon OpenSearch Service(Amazon Elasticsearch Service)",
      "운영 오버헤드",
      "거의 실시간"
    ],
    "Terms": [
      "CloudWatch Logs subscription",
      "Amazon OpenSearch Service(Amazon Elasticsearch Service)",
      "Kinesis Data Firehose",
      "Amazon Kinesis Data Streams",
      "AWS Lambda",
      "Amazon Kinesis Agent"
    ],
    "SelectA": "CloudWatch Logs 구독(subscription)을 구성하여 로그를 Amazon OpenSearch Service(Amazon Elasticsearch Service)로 스트리밍합니다.",
    "SelectA_Commentary": "CloudWatch Logs에서 제공하는 기본 구독 기능을 통해 추가 인프라 없이 거의 실시간으로 로그를 전송할 수 있어 가장 적은 운영 오버헤드를 제공합니다.",
    "SelectB": "AWS Lambda 함수를 생성합니다. log group으로 함수를 호출하여 로그를 Amazon OpenSearch Service(Amazon Elasticsearch Service)에 기록합니다.",
    "SelectB_Commentary": "Lambda 함수를 사용하면 실시간 처리가 가능하지만, 함수 코드 유지보수와 트리거 구성 등 추가적 운영 부담이 생깁니다.",
    "SelectC": "Amazon Kinesis Data Firehose delivery stream을 생성합니다. log group을 소스로 구성하고, Amazon OpenSearch Service(Amazon Elasticsearch Service)를 대상로 구성합니다.",
    "SelectC_Commentary": "Kinesis Data Firehose는 실시간에 가까운 데이터 전송이 가능하지만, 별도 스트리밍 리소스 설정과 관리가 필요해 오버헤드가 더 높습니다.",
    "SelectD": "각 애플리케이션 서버에 Amazon Kinesis Agent를 설치하고 Kinesis Data Streams로 로그를 전송합니다. 그리고 Kinesis Data Streams를 통해 로그를 Amazon OpenSearch Service(Amazon Elasticsearch Service)에 전송합니다.",
    "SelectD_Commentary": "Kinesis Agent 설치와 Data Streams 관리가 모두 필요하므로 구성과 운영이 복잡하며, 실시간에 가깝게 전송 가능하지만 운영 오버헤드가 매우 커집니다."
  },
  {
    "Question_Number": "Q118",
    "Question_Description": "한 회사에서 여러 Availability Zone에 분산된 Amazon EC2 인스턴스 위에서 실행되는 웹 기반 애플리케이션을 구축하고 있습니다. 이 웹 애플리케이션은 총 900TB에 달하는 텍스트 문서 저장소에 대한 접근 기능을 제공합니다. 회사는 웹 애플리케이션이 높은 트래픽을 경험할 시기가 있을 것으로 예상하며, 솔루션스 아키텍트는 텍스트 문서를 저장하는 스토리지가 언제나 애플리케이션의 수요를 충족할 수 있도록 확장 가능해야 한다고 요구합니다. 또한 회사는 전체 솔루션에 대한 비용 문제를 우려하고 있습니다. 이 요구사항을 가장 비용 효율적으로 충족하는 스토리지 솔루션은 무엇입니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86512-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 대용량(900TB) 텍스트 문서를 저장하면서, 높은 트래픽 부담에도 확장 가능하고 비용을 최소화해야 하는 시나리오입니다. Amazon S3는 저렴한 가격 구조와 탄력적 확장이 가능해 요구사항을 모두 충족합니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.1",
      "2.1"
    ],
    "Keywords": [
      "웹 기반 애플리케이션",
      "텍스트 문서",
      "900TB",
      "확장 가능",
      "비용 효율"
    ],
    "Terms": [
      "Amazon EC2",
      "Availability Zone",
      "Amazon Elastic Block Store (Amazon EBS)",
      "Amazon Elastic File System (Amazon EFS)",
      "Amazon OpenSearch Service (Amazon Elasticsearch Service)",
      "Amazon S3"
    ],
    "SelectA": "Amazon EBS",
    "SelectA_Commentary": "블록 스토리지로서 고성능이지만 단일 AZ 기반이고 비용이 높아 대규모 데이터 저장에는 비효율적입니다.",
    "SelectB": "Amazon EFS",
    "SelectB_Commentary": "파일 스토리지 서비스로 확장성은 좋지만, S3에 비해 GB당 비용이 높아 900TB 규모의 데이터에는 부담이 큽니다.",
    "SelectC": "Amazon OpenSearch Service (Amazon Elasticsearch Service)",
    "SelectC_Commentary": "실시간 검색 및 분석용 서비스로, 대규모 단순 데이터 저장에는 부적합하며 비용 또한 높습니다.",
    "SelectD": "Amazon S3",
    "SelectD_Commentary": "비용이 저렴하고 무제한에 가까운 확장성을 제공하므로, 대규모 텍스트 문서 저장 요구사항에 가장 적합합니다."
  },
  {
    "Question_Number": "Q119",
    "Question_Description": "글로벌 회사가 us-east-1 리전과 ap-southeast-2 리전에서 운영되는 Amazon API Gateway로 REST API를 설계하고 있습니다. 여러 AWS 계정에서 이 API를 사용하고 있으며, SQL injection과 cross-site scripting 공격으로부터 안전하게 보호해야 합니다. Solutions Architect는 관리 오버헤드를 최소화하면서 API Gateway에서 이러한 공격을 방어할 수 있는 방법을 설계해야 합니다. 다음 중 어떤 솔루션이 최소한의 관리 노력으로 요구 사항을 충족합니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86450-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 여러 리전과 계정에 걸쳐 운영되는 API를 공격으로부터 보호하면서, 관리 노력도 최소화해야 합니다. AWS Firewall Manager를 사용하면 여러 계정과 리전에서 중앙 집중식으로 AWS WAF 구성을 관리할 수 있어 SQL injection, XSS 같은 공격을 효율적으로 방어하고 자동화된 보호까지 가능하도록 해줍니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.2"
    ],
    "Keywords": [
      "API Gateway",
      "SQL injection",
      "cross-site scripting",
      "관리 오버헤드 최소화"
    ],
    "Terms": [
      "Amazon API Gateway",
      "AWS WAF",
      "AWS Firewall Manager",
      "AWS Shield",
      "Regional web ACL",
      "API stage",
      "REST API"
    ],
    "SelectA": "두 리전에서 AWS WAF를 설정하고, Regional web ACL을 API 스테이지에 연결합니다.",
    "SelectA_Commentary": "모든 리전에 개별적으로 WAF를 설정해야 하므로 계정 단위의 관리가 여전히 복잡합니다.",
    "SelectB": "두 리전에서 AWS Firewall Manager를 설정하고, 중앙에서 AWS WAF 규칙을 구성합니다.",
    "SelectB_Commentary": "AWS Firewall Manager로 여러 계정과 리전을 일괄 관리할 수 있어 노력과 복잡도가 크게 줄어듭니다.",
    "SelectC": "두 리전에서 AWS Shield를 설정하고, Regional web ACL을 API 스테이지에 연결합니다.",
    "SelectC_Commentary": "AWS Shield는 주로 DDoS 방어에 특화되어 있고, SQL injection 및 XSS 방어를 위해선 WAF 구성이 별도로 필요합니다.",
    "SelectD": "한 리전에서만 AWS Shield를 설정하고, Regional web ACL을 API 스테이지에 연결합니다.",
    "SelectD_Commentary": "DDoS 방어 위주 솔루션이며 전 세계적으로 SQL injection과 XSS를 다루기엔 효과적이지 못합니다."
  },
  {
    "Question_Number": "Q120",
    "Question_Description": "한 회사가 us-west-2 리전에 위치한 Network Load Balancer(NLB) 뒤의 Amazon EC2 인스턴스 3대를 활용하여 자체 관리 DNS 솔루션을 구성했습니다. 이 회사의 대부분 사용자들은 미국과 유럽에 분포해 있으며, 성능과 가용성을 더욱 개선하고자 합니다. 회사는 eu-west-1 리전에도 EC2 인스턴스 3대를 추가로 구축하고 새로운 NLB의 대상으로 등록했습니다. 모든 EC2 인스턴스로 트래픽을 라우팅하기 위해 어떤 솔루션을 사용할 수 있습니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85807-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 여러 리전에 분산된 EC2 인스턴스를 사용해 자체 관리 DNS를 제공할 때, 성능과 가용성을 높이는 방안을 묻습니다. 답은 AWS Global Accelerator를 사용해 us-west-2와 eu-west-1 두 리전에 걸쳐 엔드포인트 그룹을 생성한 뒤, 각각의 NLB를 엔드포인트로 추가하는 방법입니다. 이렇게 하면 글로벌 사용자가 물리적 위치와 관계없이 가장 빠른 네트워크 경로를 통해 DNS 서비스에 접근할 수 있어 성능과 가용성이 모두 개선됩니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.2"
    ],
    "Keywords": [
      "성능",
      "가용성",
      "Network Load Balancer",
      "AWS Global Accelerator",
      "라우팅"
    ],
    "Terms": [
      "Amazon EC2",
      "Network Load Balancer(NLB)",
      "AWS Global Accelerator",
      "Endpoint groups",
      "Amazon Route 53",
      "Geolocation routing policy",
      "Latency routing policy",
      "Application Load Balancer(ALB)",
      "Amazon CloudFront",
      "Elastic IP addresses"
    ],
    "SelectA": "Amazon Route 53 지리 위치(geolocation) 라우팅 정책을 만들어 두 NLB 중 하나로 요청을 전달합니다. 그런 다음 Amazon CloudFront 배포를 생성하고, Route 53 레코드를 배포의 오리진으로 사용합니다.",
    "SelectA_Commentary": "지리 위치 라우팅은 지역을 기준으로 트래픽을 나누지만, 글로벌 사용자의 가장 빠른 경로 제공을 보장하지 못합니다. CloudFront 배포를 추가하더라도 NLB 선택에 있어 지역 구분만 사용하므로 성능 향상이 제한적입니다.",
    "SelectB": "AWS Global Accelerator 표준 가속기를 생성합니다. us-west-2와 eu-west-1 리전에 엔드포인트 그룹을 만들고, 두 NLB를 엔드포인트로 추가합니다.",
    "SelectB_Commentary": "AWS Global Accelerator를 사용하면 전 세계 사용자에게 단일 고정 진입점을 제공하고, 글로벌 AWS 네트워크를 통해 가장 빠른 경로로 트래픽을 라우팅하므로 성능과 가용성이 크게 향상됩니다. 정답입니다.",
    "SelectC": "6대의 EC2 인스턴스 각각에 Elastic IP 주소를 할당합니다. Amazon Route 53 지리 위치 라우팅 정책으로 6대 중 하나로 트래픽을 전달하게 구성합니다. 그리고 Amazon CloudFront 배포를 생성하고, 해당 Route 53 레코드를 CloudFront의 오리진으로 사용합니다.",
    "SelectC_Commentary": "개별 EC2 인스턴스에 직접 트래픽을 전달하면 로드 밸런서를 통한 확장성, 고가용성 이점을 활용하기 어렵고, 관리 복잡성도 높아집니다.",
    "SelectD": "두 NLB를 두 Application Load Balancer(ALB)로 교체합니다. Amazon Route 53 지연 시간(latency) 라우팅 정책을 생성하여 두 ALB 중 하나로 요청을 전송합니다. 그런 다음 Amazon CloudFront 배포를 만들고, Route 53 레코드를 배포의 오리진으로 사용합니다.",
    "SelectD_Commentary": "지연 시간 라우팅을 사용해도, AWS Global Accelerator가 제공하는 전용 글로벌 백본을 통한 성능 이점을 활용하기는 어렵습니다. ALB 교체 및 CloudFront 연동 또한 필요한 단계를 늘려 운영 부담이 커집니다."
  },
  {
    "Question_Number": "Q121",
    "Question_Description": "한 회사가 온라인 트랜잭션 처리(OLTP) 워크로드를 AWS에서 운영하고 있습니다. 이 워크로드는 Multi-AZ 배포 구성의 암호화되지 않은 Amazon RDS DB 인스턴스를 사용하며, 매일 이 인스턴스에서 데이터베이스 스냅샷을 생성하고 있습니다. 앞으로 데이터베이스와 스냅샷을 항상 암호화하기 위해 솔루션스 아키텍트는 어떤 조치를 취해야 합니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85941-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "기존에 암호화되지 않은 DB 인스턴스는 직접 암호화를 활성화할 수 없습니다. 대신 먼저 최신 DB 스냅샷의 암호화 복사본을 만들고, 해당 스냅샷을 복원하여 암호화된 새 DB 인스턴스를 생성하면 이후 모든 스냅샷도 자동으로 암호화됩니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.3"
    ],
    "Keywords": [
      "Amazon RDS",
      "Multi-AZ",
      "암호화",
      "DB 스냅샷",
      "AWS KMS",
      "OLTP"
    ],
    "Terms": [
      "Amazon RDS DB instance",
      "Multi-AZ deployment",
      "Daily database snapshots",
      "AWS Key Management Service (AWS KMS)",
      "Unencrypted DB instance",
      "SSE-KMS",
      "Amazon EBS volume",
      "Restore"
    ],
    "SelectA": "최신 DB 스냅샷의 암호화 복사본을 생성한 뒤, 해당 암호화된 스냅샷을 복원하여 기존 DB 인스턴스를 교체합니다.",
    "SelectA_Commentary": "암호화된 DB 스냅샷을 생성 후 이를 복원해 새 인스턴스를 만들면 암호화가 적용된 인스턴스를 확보해 이후 스냅샷도 모두 암호화됩니다. 이 접근이 올바른 해결책입니다.",
    "SelectB": "새로운 암호화된 Amazon EBS 볼륨을 생성하고, 기존 스냅샷을 그 볼륨에 복사합니다. 이후 DB 인스턴스에 암호화를 활성화합니다.",
    "SelectB_Commentary": "RDS 인스턴스는 생성 시점에만 암호화 설정이 가능하므로, 단순히 EBS 볼륨을 암호화해도 현재 DB 인스턴스를 바로 암호화할 수 없어서 적합하지 않습니다.",
    "SelectC": "스냅샷을 복사하면서 AWS KMS를 사용해 암호화를 활성화합니다. 암호화된 스냅샷을 기존 DB 인스턴스에 복원합니다.",
    "SelectC_Commentary": "암호화된 스냅샷은 기존 DB 인스턴스에 직접 복원될 수 없고, 새 인스턴스로 복원해야 실제로 암호화 인스턴스를 얻을 수 있으므로 이 방식은 부적절합니다.",
    "SelectD": "스냅샷을 AWS KMS 관리 암호화(SSE-KMS)가 적용된 Amazon S3 버킷으로 복사합니다.",
    "SelectD_Commentary": "스냅샷을 S3로 암호화해 보관해도 원래의 RDS 인스턴스를 암호화 상태로 전환하지 못하므로, 이후 DB 스냅샷 자동 암호화에도 영향을 주지 못합니다."
  },
  {
    "Question_Number": "Q122",
    "Question_Description": "회사는 애플리케이션에서 데이터를 암호화해야 하는 개발자들을 지원하기 위해 스케일러블한 키 관리 인프라스트럭처를 구축하려고 합니다. 솔루션스 아키텍트의 운영 부담을 줄이기 위한 최적의 방안은 무엇입니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85942-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 암호화 키를 안전하고 확장 가능하게 관리해야 하는 상황입니다. AWS KMS는 중앙 집중식 키 생성, 로테이션, 접근 제어 등을 자동화하여 운영 부담을 크게 줄여 줍니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.3"
    ],
    "Keywords": [
      "스케일러블 키 관리",
      "암호화",
      "애플리케이션",
      "운영 부담 최소화",
      "AWS KMS"
    ],
    "Terms": [
      "AWS Key Management Service (AWS KMS)",
      "IAM policy",
      "AWS Certificate Manager (ACM)",
      "Multi-factor authentication (MFA)",
      "Encryption keys"
    ],
    "SelectA": "Multi-factor authentication (MFA)를 사용하여 Encryption keys를 보호합니다.",
    "SelectA_Commentary": "MFA는 계정 보호에는 유용하나, 자체적으로 스케일러블한 키 관리 인프라스트럭처를 제공하지 못합니다.",
    "SelectB": "AWS Key Management Service (AWS KMS)를 사용하여 Encryption keys를 보호합니다.",
    "SelectB_Commentary": "AWS KMS는 암호화 키를 안전하게 저장, 관리, 자동 로테이션할 수 있어 운영 부담과 라이선스 비용을 모두 줄일 수 있는 최적의 솔루션입니다.",
    "SelectC": "AWS Certificate Manager (ACM)을 사용하여 Encryption keys를 생성, 저장 및 할당합니다.",
    "SelectC_Commentary": "ACM은 주로 SSL/TLS 인증서 관리에 특화되어 있으며, 일반적인 데이터 암호화를 위한 키 관리에는 적합하지 않습니다.",
    "SelectD": "IAM policy로 Encryption keys에 대한 액세스 권한 범위를 제한합니다.",
    "SelectD_Commentary": "IAM policy로 접근을 제어하는 것은 중요하지만, 키를 직접 관리하고 자동화할 수 있는 기능은 제공하지 못해 운영 부담을 충분히 줄이지 못합니다."
  },
  {
    "Question_Number": "Q123",
    "Question_Description": "한 회사가 두 대의 Amazon EC2 인스턴스에 동적 웹 애플리케이션을 호스팅하고 있습니다. 회사는 자체 SSL certificate를 보유하고 있으며, 각 인스턴스에서 SSL termination을 수행하고 있습니다. 최근 트래픽이 증가하여 운영팀은 SSL 암호화·복호화 작업이 웹 서버의 컴퓨팅 용량을 최대치로 사용한다고 판단했습니다. 애플리케이션 성능을 높이기 위해 Solutions Architect는 무엇을 해야 합니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85943-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제의 핵심은 SSL 암복호화로 인한 서버 부하를 줄여 애플리케이션 성능을 높이는 것입니다. 인스턴스에서 SSL termination을 수행하면 CPU 사용량이 크게 증가하므로, AWS Certificate Manager에 SSL certificate를 등록하고 ALB(HTTPS listener)에서 암복호화를 처리하도록 오프로딩하는 방식이 최적의 해결책입니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.2"
    ],
    "Keywords": [
      "SSL certificate",
      "SSL termination",
      "AWS Certificate Manager",
      "Application Load Balancer",
      "EC2",
      "HTTPS",
      "웹 서버 성능"
    ],
    "Terms": [
      "SSL certificate",
      "SSL termination",
      "AWS Certificate Manager (ACM)",
      "Application Load Balancer",
      "Amazon EC2",
      "Amazon S3",
      "Proxy server",
      "HTTPS listener"
    ],
    "SelectA": "AWS Certificate Manager (ACM)에서 새로운 SSL certificate를 생성하고, 각 인스턴스에 설치합니다.",
    "SelectA_Commentary": "각 EC2 인스턴스에서 SSL termination을 계속 수행하므로 암복호화 부담이 여전히 남아 있습니다.",
    "SelectB": "Amazon S3 버킷을 생성하고 SSL certificate를 마이그레이션한 후, EC2 인스턴스가 해당 버킷을 참조하도록 설정합니다.",
    "SelectB_Commentary": "S3 버킷은 SSL termination 기능을 제공하지 않으므로 서버 부하 문제를 해결할 수 없습니다.",
    "SelectC": "프록시 서버용 새 EC2 인스턴스를 생성하고, SSL certificate를 마이그레이션하여 기존 EC2 인스턴스에 연결되도록 구성합니다.",
    "SelectC_Commentary": "프록시 서버에서 SSL termination을 수행하지만, EC2 인스턴스를 하나 더 두어 직접 관리해야 하므로 운영 복잡도가 증가합니다.",
    "SelectD": "SSL certificate를 AWS Certificate Manager (ACM)에 가져옵니다. ACM의 SSL certificate를 사용하는 HTTPS listener가 있는 Application Load Balancer를 생성합니다.",
    "SelectD_Commentary": "ALB에서 SSL 암복호화를 담당하게 하여 웹 서버의 부하를 줄이고, 관리 편의성과 보안성을 모두 높이는 최적의 방법입니다."
  },
  {
    "Question_Number": "Q124",
    "Question_Description": "한 회사는 매우 동적인 배치 처리 작업을 수행하기 위해 여러 Amazon EC2 인스턴스를 사용하고 있습니다. 이 작업은 상태 정보가 없으므로 언제든지 중지 및 재시작해도 문제가 없으며, 보통 전체 수행 시간은 60분 이상 걸립니다. 회사는 이 작업의 요구 사항을 충족하면서 확장 가능하고 비용 효율적인 솔루션을 설계해 달라고 Solutions Architect에게 요청했습니다. 어떤 솔루션을 권장해야 할까요?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86038-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 배치 작업을 간헐적으로 중단·재시작해도 무방한 특성을 활용해, 중단 허용 워크로드에 최적화된 저비용 실행 방안을 찾는 것입니다. EC2 Spot Instances는 일반 온디맨드 대비 매우 저렴하며, 작업이 중단되어도 문제가 없는 배치 처리를 수행하기에 안성맞춤입니다. On-Demand Instances는 유연성이 있으나 비용이 더 높고, Reserved Instances는 장기 약정이 필요해 유연성이 떨어집니다. AWS Lambda는 실행 시간 제한(최대 15분)이 있어 60분 이상의 작업에는 적합하지 않습니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.2"
    ],
    "Keywords": [
      "동적인 배치 처리",
      "stateless",
      "확장 가능",
      "비용 효율적인 솔루션",
      "60분 이상",
      "EC2 Spot Instances"
    ],
    "Terms": [
      "Amazon EC2",
      "EC2 Spot Instances",
      "EC2 Reserved Instances",
      "EC2 On-Demand Instances",
      "AWS Lambda"
    ],
    "SelectA": "EC2 Spot Instances를 구현합니다.",
    "SelectA_Commentary": "Spot은 중단될 가능성이 있지만, 유연한 배치 작업에는 저비용으로 적합한 선택입니다.",
    "SelectB": "EC2 Reserved Instances를 구매합니다.",
    "SelectB_Commentary": "장기 약정으로 비용은 줄일 수 있으나, 동적 배치 작업을 수시로 중지/재시작하기에는 적합하지 않습니다.",
    "SelectC": "EC2 On-Demand Instances를 구현합니다.",
    "SelectC_Commentary": "온디맨드는 유연하지만 Spot 대비 비용이 높아 대규모 배치 작업의 비용 절감에는 한계가 있습니다.",
    "SelectD": "AWS Lambda에서 작업을 처리합니다.",
    "SelectD_Commentary": "Lambda의 최대 실행 시간이 15분이라 60분 이상 소요되는 배치 작업에는 맞지 않습니다."
  },
  {
    "Question_Number": "Q125",
    "Question_Description": "한 회사가 AWS에서 2티어 전자상거래 웹사이트를 운영하고 있습니다. 웹 티어는 로드 밸런서를 통해 Amazon EC2 인스턴스로 트래픽을 전송합니다. 데이터베이스 티어는 Amazon RDS DB 인스턴스를 사용합니다. EC2 인스턴스와 RDS DB 인스턴스는 퍼블릭 인터넷에 노출되어서는 안 됩니다. 하지만 EC2 인스턴스는 서드파티 웹 서비스를 통한 결제 처리를 위해 인터넷 액세스가 필요합니다. 또한 애플리케이션은 고가용성을 유지해야 합니다. 이러한 요구사항을 충족하는 구성 옵션의 조합은 무엇입니까? (정답으로 2개를 고르세요.)",
    "Answer": "A,D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85221-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 인터넷에 직접 노출되지 않는 EC2 인스턴스와 RDS DB 인스턴스를 구성하면서도, EC2 인스턴스가 외부 결제 서비스를 위해 아웃바운드 인터넷 액세스를 갖도록 해야 하며, 동시에 고가용성을 보장해야 합니다. EC2와 RDS를 모두 Private Subnet에 배치하고, Public Subnet에는 NAT Gateway와 Application Load Balancer를 다중 AZ로 구성함으로써 이러한 요구사항을 충족할 수 있습니다. 따라서 Auto Scaling group으로 Private Subnet에 EC2를 생성하고 Multi-AZ로 RDS를 구성하며, Public Subnet에 배포된 NAT Gateway와 ALB를 통해 인터넷 액세스와 외부 트래픽 수용을 동시에 만족시킬 수 있는 (A)와 (E)가 정답입니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "1.1",
      "2.2"
    ],
    "Keywords": [
      "2티어 전자상거래",
      "EC2 인스턴스",
      "RDS DB 인스턴스",
      "NAT Gateway",
      "퍼블릭 인터넷 노출 방지",
      "고가용성"
    ],
    "Terms": [
      "Amazon EC2",
      "Amazon RDS",
      "Multi-AZ",
      "Auto Scaling group",
      "VPC",
      "Public Subnet",
      "Private Subnet",
      "NAT Gateway",
      "Application Load Balancer"
    ],
    "SelectA": "Auto Scaling group을 사용하여 EC2 인스턴스를 Private Subnet에 생성합니다. RDS Multi-AZ DB 인스턴스를 Private Subnet에 배포합니다.",
    "SelectA_Commentary": "EC2와 RDS 모두 Private Subnet에서 운영하며, Multi-AZ 구성으로 고가용성을 지원합니다. EC2는 NAT Gateway를 통해 인터넷에 접근할 수 있게 됩니다.",
    "SelectB": "두 개의 Private Subnet과 두 NAT Gateway를 두 개의 Availability Zone에 구성합니다. Application Load Balancer를 Private Subnet에 배포합니다.",
    "SelectB_Commentary": "ALB가 Private Subnet에 위치하면 외부에서 직접 접근이 불가하여 웹 트래픽 수용이 불가능하므로 요구사항을 충족하지 못합니다.",
    "SelectC": "Auto Scaling group을 사용하여 EC2 인스턴스를 두 개의 Availability Zone에 걸쳐 Public Subnet에 생성합니다. RDS Multi-AZ DB 인스턴스를 Private Subnet에 배포합니다.",
    "SelectC_Commentary": "EC2 인스턴스가 Public Subnet에 위치하여 퍼블릭 인터넷에 노출되므로, 보안 요구사항에 어긋납니다.",
    "SelectD": "하나의 Public Subnet과 하나의 Private Subnet, 그리고 두 NAT Gateway를 두 개의 Availability Zone에 구성합니다. Application Load Balancer를 Public Subnet에 배포합니다.",
    "SelectD_Commentary": "Public Subnet이 단 하나뿐이라 가용 영역을 둘 이상 활용하기 어렵고, 고가용성 구성에 부합하지 않습니다.",
    "SelectE": "두 개의 Public Subnet, 두 개의 Private Subnet, 그리고 두 NAT Gateway를 두 개의 Availability Zone에 구성합니다. Application Load Balancer를 Public Subnet들에 배포합니다.",
    "SelectE_Commentary": "각 AZ에 Public Subnet과 Private Subnet을 모두 두고, ALB를 Public Subnet에 배치해 외부 트래픽을 처리합니다. EC2와 RDS는 Private Subnet에 배치해 인터넷 노출을 방지하면서 NAT Gateway를 통해 필요한 아웃바운드 액세스를 보장합니다. 다중 AZ 구성을 통해 높은 가용성을 달성할 수 있습니다."
  },
  {
    "Question_Number": "Q126",
    "Question_Description": "한 솔루션즈 아키텍트가 회사의 스토리지 비용 절감을 위해 솔루션을 구현해야 합니다. 회사의 모든 데이터는 Amazon S3 Standard 스토리지 클래스로 저장되어 있습니다. 회사는 최소 25년 동안 모든 데이터를 보관해야 합니다. 가장 최근 2년간의 데이터는 높은 가용성과 즉시 검색이 가능해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86731-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "데이터를 2년간은 Amazon S3 Standard로 유지해 즉시 액세스를 보장하고, 이후에는 S3 Glacier Deep Archive로 전환하여 비용을 크게 절감할 수 있습니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.1"
    ],
    "Keywords": [
      "25년 보관",
      "2년 즉시 검색",
      "Amazon S3 Standard",
      "S3 Glacier Deep Archive",
      "스토리지 비용 절감"
    ],
    "Terms": [
      "Amazon S3 Standard",
      "S3 Lifecycle policy",
      "S3 Glacier Deep Archive",
      "S3 Intelligent-Tiering",
      "S3 One Zone-Infrequent Access (S3 One Zone-IA)"
    ],
    "SelectA": "객체를 S3 Glacier Deep Archive로 즉시 전환하도록 S3 Lifecycle policy를 설정합니다.",
    "SelectA_Commentary": "즉시 전환 시 2년 동안 필요한 즉시 검색 요구를 충족할 수 없어 부적합합니다.",
    "SelectB": "객체를 2년 후에 S3 Glacier Deep Archive로 전환하도록 S3 Lifecycle policy를 설정합니다.",
    "SelectB_Commentary": "정답. 먼저 2년간 Amazon S3 Standard로 즉시 접근성을 보장하고 이후 장기 보관을 위해 S3 Glacier Deep Archive로 전환해 비용을 절감합니다.",
    "SelectC": "S3 Intelligent-Tiering을 사용합니다. archiving option을 활성화하여 S3 Glacier Deep Archive에 데이터가 보관되도록 합니다.",
    "SelectC_Commentary": "archiving option 활성 시 2년 이내 데이터도 아카이빙될 수 있으므로, 필요한 즉시 검색 요구사항을 충족하기 어렵습니다.",
    "SelectD": "객체를 즉시 S3 One Zone-Infrequent Access (S3 One Zone-IA)로 전환하고, 2년 후에는 S3 Glacier Deep Archive로 전환하도록 S3 Lifecycle policy를 설정합니다.",
    "SelectD_Commentary": "One Zone-IA는 가용성이 낮고, 2년간의 높은 가용성 및 즉시 검색 요구에도 부합하지 않습니다."
  },
  {
    "Question_Number": "Q127",
    "Question_Description": "한 미디어 회사가 시스템을 AWS 클라우드로 이전하는 방안을 검토하고 있습니다. 회사는 비디오 처리용으로 최대한의 I/O 성능을 제공해야 하는 최소 10TB의 스토리지, 미디어 콘텐츠를 저장하기 위한 300TB의 매우 내구성 있는 스토리지, 그리고 이미 사용되지 않는 아카이브 미디어 보관을 위한 900TB의 스토리지가 필요합니다. 이 요구 사항을 충족하기 위해 솔루션스 아키텍트가 권장해야 할 서비스 조합은 무엇입니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85432-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 각각의 스토리지 요구 사항에 맞춰 가장 적합한 AWS 스토리지 서비스를 선택하는 상황입니다. 높은 I/O 성능을 위해 Instance Store를 활용하고, 내구성이 필요한 300TB는 Amazon S3에 저장하며, 사용하지 않는 900TB 데이터는 S3 Glacier로 아카이빙하는 구성이 최적 해법입니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.1"
    ],
    "Keywords": [
      "비디오 처리",
      "최대 I/O 성능",
      "내구성 높은 스토리지",
      "아카이브 스토리지"
    ],
    "Terms": [
      "Amazon EC2 Instance Store",
      "Amazon S3",
      "Amazon S3 Glacier",
      "Amazon EBS",
      "Amazon EFS"
    ],
    "SelectA": "Amazon EBS로 최대 성능을 확보하고, Amazon S3로 내구성 있는 스토리지를 제공하며, Amazon S3 Glacier로 아카이브 스토리지 구현",
    "SelectA_Commentary": "EBS는 상당히 높은 성능을 제공하지만 Instance Store에 비해 최대 I/O 성능이 제한적이라 요구 사항을 최적화하기에 부족합니다.",
    "SelectB": "Amazon EBS로 최대 성능을 확보하고, Amazon EFS로 내구성 있는 스토리지를 제공하며, Amazon S3 Glacier로 아카이브 스토리지 구현",
    "SelectB_Commentary": "EFS는 파일 스토리지로 유연하나 대규모 300TB를 비용 효율적으로 저장하기에는 S3만큼 적합하지 않습니다.",
    "SelectC": "Amazon EC2 Instance Store로 최대 성능을 확보하고, Amazon EFS로 내구성 있는 스토리지를 제공하며, Amazon S3로 아카이브 스토리지 구현",
    "SelectC_Commentary": "Instance Store로 I/O 성능은 충족하나, 900TB의 아카이브 데이터를 S3에만 보관하면 장기 아카이빙 용도에 적합한 Glacier를 활용하지 못합니다.",
    "SelectD": "Amazon EC2 Instance Store로 최대 성능을 확보하고, Amazon S3로 내구성 있는 스토리지를 제공하며, Amazon S3 Glacier로 아카이브 스토리지 구현",
    "SelectD_Commentary": "Instance Store가 EBS 대비 더 높은 최대 I/O 성능을 제공하고, 300TB의 내구성 높은 스토리지로 Amazon S3를 활용하며, 900TB의 장기 보관 데이터는 비용 효율적인 S3 Glacier로 아카이빙하는 가장 적절한 방안입니다."
  },
  {
    "Question_Number": "Q128",
    "Question_Description": "한 회사가 AWS Cloud에서 컨테이너로 애플리케이션을 실행하려고 합니다. 이 애플리케이션들은 stateless하며 기본 인프라의 중단을 견딜 수 있습니다. 회사는 비용과 운영 오버헤드를 최소화하는 솔루션이 필요합니다. 어떤 방법이 이 요구사항을 충족할까요?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85404-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 컨테이너 기반의 애플리케이션을 중단에 견딜 수 있을 만큼 유연하게 설계하고, 동시에 비용과 운영 부담을 줄이는 방법을 묻습니다. Stateless 특성으로 인해 Spot Instances의 중단 위험을 감내할 수 있어 온디맨드 대비 매우 저렴한 비용으로 운영 가능합니다. 따라서 Spot Instances를 사용하는 것이 최적의 선택입니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.2"
    ],
    "Keywords": [
      "컨테이너",
      "stateless",
      "중단 견딤",
      "비용 최적화",
      "운영 오버헤드",
      "Spot Instances",
      "On-Demand Instances"
    ],
    "Terms": [
      "Amazon EC2 Auto Scaling group",
      "Amazon EKS",
      "Spot Instances",
      "On-Demand Instances",
      "Managed node group"
    ],
    "SelectA": "Amazon EC2 Auto Scaling group에서 Spot Instances를 사용하여 애플리케이션 컨테이너를 실행합니다.",
    "SelectA_Commentary": "Stateless 애플리케이션 환경에서는 Spot 중단도 쉽게 복구할 수 있어 비용을 크게 줄이고 운영 오버헤드도 최소화할 수 있는 최적의 방법입니다.",
    "SelectB": "Spot Instances를 Amazon Elastic Kubernetes Service(Amazon EKS) managed node group에서 사용합니다.",
    "SelectB_Commentary": "EKS를 사용하면 컨트롤 플레인 및 관리 비용이 추가되어 운영 오버헤드가 증가하므로 요구사항에 비해 비효율적입니다.",
    "SelectC": "Amazon EC2 Auto Scaling group에서 On-Demand Instances를 사용하여 애플리케이션 컨테이너를 실행합니다.",
    "SelectC_Commentary": "On-Demand Instances는 유연하나, Spot 대비 비용이 더 높기 때문에 비용 최소화 목표에 부합하지 않습니다.",
    "SelectD": "On-Demand Instances를 Amazon Elastic Kubernetes Service(Amazon EKS) managed node group에서 사용합니다.",
    "SelectD_Commentary": "On-Demand와 EKS 조합은 비용 절감 효과가 적고, EKS 운영 오버헤드도 추가되어 요구사항을 만족하기 어렵습니다."
  },
  {
    "Question_Number": "Q129",
    "Question_Description": "한 회사가 온프레미스 환경에서 멀티 티어 웹 애플리케이션을 운영하고 있습니다. 이 애플리케이션은 컨테이너화되어 있으며, 사용자 레코드를 포함하는 PostgreSQL 데이터베이스와 연결된 여러 Linux 호스트에서 동작합니다. 인프라 유지관리와 용량 계획에 대한 운영 오버헤드가 회사의 성장을 제한하고 있어, 솔루션스 아키텍트는 애플리케이션 인프라를 개선해야 합니다. 이러한 요구사항을 달성하기 위해 솔루션스 아키텍트가 취해야 할 조합은 무엇입니까? (2개를 고르세요.)",
    "Answer": "A,E",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86658-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "운영 오버헤드와 용량 계획 문제를 해소하려면 완전관리형 서비스로 마이그레이션하는 것이 효과적입니다. Amazon Aurora로 데이터베이스를 이전하면 고가용성 및 확장성을 쉽게 확보할 수 있고, AWS Fargate를 통해 컨테이너 오케스트레이션 환경을 자동으로 관리할 수 있어 인프라 관리 부담이 크게 줄어듭니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1",
      "2.2"
    ],
    "Keywords": [
      "온프레미스",
      "멀티 티어 웹 애플리케이션",
      "컨테이너화",
      "PostgreSQL",
      "사용자 레코드",
      "운영 오버헤드",
      "용량 계획",
      "인프라 개선"
    ],
    "Terms": [
      "Amazon Aurora",
      "AWS Fargate",
      "Amazon Elastic Container Service (Amazon ECS)",
      "Amazon CloudFront",
      "Amazon EC2",
      "Amazon ElastiCache",
      "PostgreSQL"
    ],
    "SelectA": "PostgreSQL 데이터베이스를 Amazon Aurora로 마이그레이션합니다.",
    "SelectA_Commentary": "Amazon Aurora는 높은 확장성, 관리 편의성을 제공하므로 운영 오버헤드를 줄이고 고가용성을 보장하는 데 적합합니다.",
    "SelectB": "웹 애플리케이션을 Amazon EC2 인스턴스에서 호스팅하도록 마이그레이션합니다.",
    "SelectB_Commentary": "Amazon EC2는 직접 인스턴스를 관리해야 하므로, 인프라 관리 부담이 여전히 남아 문제 개선 효과가 제한적입니다.",
    "SelectC": "웹 애플리케이션 콘텐츠를 위해 Amazon CloudFront 배포를 설정합니다.",
    "SelectC_Commentary": "CloudFront는 전 세계 엣지 로케이션에서 콘텐츠를 캐싱하여 성능을 개선하지만, 근본적인 인프라 운영 오버헤드 문제 해결과 직접적인 연관이 적습니다.",
    "SelectD": "웹 애플리케이션과 PostgreSQL 데이터베이스 사이에 Amazon ElastiCache를 설정합니다.",
    "SelectD_Commentary": "ElastiCache는 데이터 접근 속도를 개선하나, DB 인프라 유지관리 및 용량 계획 오버헤드를 근본적으로 줄이는 데에는 제한적입니다.",
    "SelectE": "AWS Fargate와 Amazon Elastic Container Service(Amazon ECS)를 사용하여 웹 애플리케이션을 호스팅하도록 마이그레이션합니다.",
    "SelectE_Commentary": "AWS Fargate는 서버 관리가 필요 없어 애플리케이션 컨테이너 운영 부담을 최소화하고 자동 확장을 지원하기 때문에 운영 오버헤드를 크게 완화합니다."
  },
  {
    "Question_Number": "Q130",
    "Question_Description": "어떤 애플리케이션이 여러 Availability Zone에 걸쳐 Amazon EC2 인스턴스에서 실행되고 있으며, 이들 인스턴스는 한 Application Load Balancer 뒤에 위치한 Amazon EC2 Auto Scaling group 내에 있습니다. 애플리케이션은 EC2 인스턴스의 CPU 사용률이 약 40%일 때 최고의 성능을 발휘합니다. 솔루션스 아키텍트가 Auto Scaling group 내 모든 인스턴스에서 원하는 성능을 유지하기 위해서는 어떻게 해야 합니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86659-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 애플리케이션이 특정 CPU 사용률(약 40%)을 유지할 때 가장 효율적이라는 점에 주목하여, Auto Scaling group을 동적으로 조정하는 최적의 방법을 찾는 문제입니다. Target Tracking Scaling은 설정된 지표(여기서는 CPU 사용률)를 기준으로 자동으로 인스턴스 규모를 조정하므로, 정확한 목표 값을 유지하며 성능을 극대화할 수 있습니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.2"
    ],
    "Keywords": [
      "Auto Scaling",
      "CPU 사용률 40%",
      "Application Load Balancer",
      "Amazon EC2"
    ],
    "Terms": [
      "Amazon EC2",
      "Auto Scaling group",
      "Availability Zone",
      "Application Load Balancer",
      "CPU utilization",
      "AWS Lambda",
      "scheduled scaling"
    ],
    "SelectA": "simple scaling policy를 사용하여 Auto Scaling group을 동적으로 확장하도록 설정합니다.",
    "SelectA_Commentary": "simple scaling policy는 한 번의 지표 평가 후 단순 규칙로 확장하거나 축소하지만, CPU 사용률을 일정하게 유지하기에는 유연성이 부족합니다.",
    "SelectB": "target tracking policy를 사용하여 Auto Scaling group을 동적으로 확장하도록 설정합니다.",
    "SelectB_Commentary": "Target Tracking Scaling은 CPU 사용률 목표치를 40%로 설정하면 이를 자동으로 유지하도록 확장/축소를 수행하므로 문제 요구사항에 가장 적합합니다.",
    "SelectC": "AWS Lambda 함수를 사용하여 원하는 Auto Scaling group 용량을 업데이트합니다.",
    "SelectC_Commentary": "Lambda 함수를 통해 직접 용량을 조정하면 추가적인 로직과 관리가 필요하여 실시간 유연 조정에 비효율적입니다.",
    "SelectD": "scheduled scaling을 사용하여 정해진 시간에 Auto Scaling group을 확장 및 축소합니다.",
    "SelectD_Commentary": "스케줄 기반 확장은 CPU 사용률 변동에 실시간 대응이 어려워 필요 시점에 적절한 규모를 유지하기 어렵습니다."
  },
  {
    "Question_Number": "Q131",
    "Question_Description": "한 회사가 파일 공유 애플리케이션을 개발 중이며, 이 애플리케이션은 Amazon S3 버킷을 스토리지로 사용할 예정입니다. 회사는 모든 파일을 Amazon CloudFront distribution을 통해 제공하고자 합니다. 또한 S3 URL로 직접 접근하는 것을 허용하고 싶지 않습니다. 이러한 요구 사항을 충족하기 위해 솔루션스 아키텍트는 무엇을 해야 합니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85992-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 S3 객체를 직접 URL로 접근할 수 없도록 보호하면서도 CloudFront를 통해 파일을 제공하는 아키텍처를 설계하는 방법을 묻습니다. 가장 효율적이자 권장되는 방식은 CloudFront의 origin access identity(OAI)를 생성하고, 이를 통해서만 S3 버킷에 대한 읽기 권한을 부여하여 S3 URL에 직접 접근이 불가능하게 만드는 것입니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1",
      "1.3"
    ],
    "Keywords": [
      "파일 공유 애플리케이션",
      "Amazon S3 버킷",
      "Amazon CloudFront distribution",
      "S3 URL 직접 접근 차단",
      "origin access identity (OAI)"
    ],
    "Terms": [
      "Amazon S3",
      "Amazon CloudFront",
      "origin access identity (OAI)",
      "IAM",
      "S3 Bucket Policy",
      "Principal",
      "Amazon Resource Name (ARN)"
    ],
    "SelectA": "각 S3 버킷에 대해 개별 정책을 작성하고, CloudFront에만 읽기 권한을 부여한다.",
    "SelectA_Commentary": "CloudFront에서 버킷별로 정책을 따로 설정할 수 있지만, 버킷 정책이 복잡해지고 실수 가능성이 커 비효율적입니다.",
    "SelectB": "IAM 사용자를 생성하고, 해당 사용자에게 S3 버킷 객체에 대한 읽기 권한을 부여합니다. 그리고 CloudFront에 이 사용자를 할당합니다.",
    "SelectB_Commentary": "CloudFront가 IAM 사용자 자격 증명을 직접 사용하는 방식은 일반적이지 않으며 보안 및 관리 측면에서도 권장되지 않습니다.",
    "SelectC": "S3 버킷 정책에서 CloudFront distribution ID를 Principal로 지정하고 대상 S3 버킷을 ARN으로 지정합니다.",
    "SelectC_Commentary": "distribution ID만으로는 접근을 완전히 제어하기 어렵고, 추천되는 표준 접근 방식(Origin Access Identity)을 사용하지 않았습니다.",
    "SelectD": "origin access identity (OAI)를 생성하고, 이를 CloudFront distribution에 할당합니다. S3 버킷 권한을 OAI가 읽기 권한을 갖도록만 구성합니다.",
    "SelectD_Commentary": "OAI를 통해 CloudFront만이 S3 버킷에 접근하도록 제한할 수 있어, 직접 S3 URL 접근을 차단하고 안전하게 콘텐츠를 제공하는 최적의 방법입니다."
  },
  {
    "Question_Number": "Q132",
    "Question_Description": "회사의 웹사이트는 사용자에게 다운로드 가능한 과거 성능 보고서를 제공합니다. 이 웹사이트는 전 세계적으로 확장 가능해야 하며, 비용 효율적이고 인프라 리소스 프로비저닝을 최소화하면서 가능한 한 가장 빠른 응답 시간을 제공해야 합니다. 어떤 솔루션 조합을 사용하면 이 요구사항을 충족할 수 있습니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86654-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 전 세계 사용자에게 정적 파일을 빠르고 효율적으로 제공하기 위한 최적의 방법을 묻습니다. Amazon CloudFront와 Amazon S3를 조합하면 글로벌 Edge Location을 통한 빠른 전송과 무제한 확장이 가능하여 최소 인프라로도 요구사항을 충족할 수 있습니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.1",
      "3.4"
    ],
    "Keywords": [
      "글로벌 확장",
      "정적 콘텐츠",
      "비용 효율",
      "빠른 응답 시간",
      "Amazon CloudFront",
      "Amazon S3"
    ],
    "Terms": [
      "Amazon CloudFront",
      "Amazon S3",
      "AWS Lambda",
      "Amazon DynamoDB",
      "Application Load Balancer",
      "Amazon EC2 Auto Scaling",
      "Amazon Route 53",
      "Internal Application Load Balancer"
    ],
    "SelectA": "Amazon CloudFront와 Amazon S3를 사용합니다.",
    "SelectA_Commentary": "정적 콘텐츠를 S3에 저장하고, CloudFront Edge Location을 통해 글로벌 사용자에게 빠르게 제공할 수 있어 비용과 운영 복잡도를 모두 줄이면서 매우 빠른 응답 시간을 얻을 수 있습니다.",
    "SelectB": "AWS Lambda와 Amazon DynamoDB를 사용합니다.",
    "SelectB_Commentary": "Lambda+DynamoDB는 서버리스 환경이지만 정적 파일 전달에 적합하지 않으며, 대규모 다운로드 트래픽 처리에 불리합니다.",
    "SelectC": "Application Load Balancer와 Amazon EC2 Auto Scaling을 사용합니다.",
    "SelectC_Commentary": "EC2 인스턴스 기반 확장은 정적 콘텐츠 제공에는 불필요하게 복잡하며, S3+CloudFront 대비 비용 효율과 글로벌 가속 측면이 떨어집니다.",
    "SelectD": "Amazon Route 53과 내부 Application Load Balancers를 사용합니다.",
    "SelectD_Commentary": "내부 ALB는 내부 트래픽 처리용으로 적합하며, 전 세계 정적 파일 배포와는 직접적으로 연관이 없습니다."
  },
  {
    "Question_Number": "Q133",
    "Question_Description": "한 회사가 온프레미스에서 Oracle 데이터베이스를 운영하고 있습니다. 회사의 AWS 마이그레이션의 일환으로, 해당 데이터베이스를 최신 버전으로 업그레이드하려고 합니다. 또한 데이터베이스에 대한 재해 복구(DR)도 설정해야 합니다. 회사는 운영 오버헤드를 최소화하면서 데이터베이스 기반 운영 체제에 대한 액세스를 유지하고 싶어 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85423-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 온프레미스의 Oracle DB를 최신 버전으로 업그레이드함과 동시에 재해 복구 환경을 구축해야 합니다. 운영 오버헤드를 낮추고 OS 접근성을 유지하려면 Amazon RDS Custom for Oracle이 최적이며, 다른 리전에 Read Replica를 생성하면 DR을 쉽고 안정적으로 구성할 수 있습니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.2"
    ],
    "Keywords": [
      "Oracle 데이터베이스",
      "AWS 마이그레이션",
      "최신 버전 업그레이드",
      "DR",
      "운영 오버헤드",
      "OS 접근",
      "Amazon RDS Custom for Oracle"
    ],
    "Terms": [
      "Amazon EC2",
      "Amazon RDS for Oracle",
      "Amazon RDS Custom for Oracle",
      "OS 접근",
      "Read Replica",
      "Cross-Region Automated Backups",
      "Standby Database",
      "DR",
      "온프레미스"
    ],
    "SelectA": "Oracle 데이터베이스를 Amazon EC2 인스턴스로 마이그레이션하고, 다른 AWS 리전에 데이터베이스 복제를 설정합니다.",
    "SelectA_Commentary": "OS 접근이 가능하나, DR 구성과 패치·백업 등 전반적인 관리 부담이 커서 운영 오버헤드가 증가합니다.",
    "SelectB": "Oracle 데이터베이스를 Amazon RDS for Oracle로 마이그레이션하고, 자동화된 Cross-Region 백업을 활성화하여 다른 AWS 리전으로 스냅샷을 복제합니다.",
    "SelectB_Commentary": "RDS 제공 기능으로 관리 오버헤드는 낮지만, OS 접근 권한을 제공하지 않으므로 요구 사항을 충족하지 못합니다.",
    "SelectC": "Oracle 데이터베이스를 Amazon RDS Custom for Oracle로 마이그레이션하고, 다른 AWS 리전에 데이터베이스의 Read Replica를 생성합니다.",
    "SelectC_Commentary": "RDS Custom은 OS 접근 권한을 허용하며, DR을 위한 Read Replica 구성을 통해 운영 오버헤드를 낮추면서 재해 복구 요건도 충족합니다.",
    "SelectD": "Oracle 데이터베이스를 Amazon RDS for Oracle로 마이그레이션하고, 다른 가용 영역(AZ)에 대기(Standby) 데이터베이스를 생성합니다.",
    "SelectD_Commentary": "Multi-AZ 대기는 같은 리전에 속해 있으므로 광역 재해에 대한 대비가 부족하고, OS 접근 또한 불가능합니다."
  },
  {
    "Question_Number": "Q134",
    "Question_Description": "한 회사가 애플리케이션을 서버리스 솔루션으로 이전하려고 합니다. 서버리스 솔루션은 기존 및 신규 데이터를 SL을 사용하여 분석해야 합니다. 회사는 Amazon S3 버킷에 데이터를 저장하고 있으며, 이 데이터는 암호화를 필요로 하고, 다른 AWS Region으로 복제되어야 합니다. 운영 오버헤드를 최소화하면서 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85993-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제에서는 데이터를 Amazon S3에 두고, 암호화와 Cross-Region 복제를 동시에 충족해야 합니다. 또한 서버리스 방식으로 기존·신규 데이터를 분석하려면 관리 부담이 낮은 Amazon Athena 사용이 적합합니다. SSE-KMS는 추가적인 AWS KMS 설정이 필요해 오버헤드가 크지만, SSE-S3는 자동 관리가 가능해 운영 상 더 간편합니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.3"
    ],
    "Keywords": [
      "서버리스 솔루션",
      "기존 및 신규 데이터 분석",
      "암호화",
      "Amazon S3",
      "S3 Cross-Region Replication",
      "SSE-KMS",
      "SSE-S3",
      "Amazon Athena",
      "Amazon RDS"
    ],
    "Terms": [
      "Amazon S3",
      "S3 Cross-Region Replication (CRR)",
      "AWS KMS multi-Region keys (SSE-KMS)",
      "Amazon S3 managed encryption keys (SSE-S3)",
      "Amazon Athena",
      "Amazon RDS"
    ],
    "SelectA": "새로운 S3 버킷을 생성 후 데이터를 로드합니다. S3 Cross-Region Replication (CRR)을 사용하여 암호화된 객체를 다른 리전의 S3 버킷으로 복제합니다. 서버 사이드 암호화로 AWS KMS multi-Region keys (SSE-KMS)를 사용합니다. Amazon Athena로 데이터를 쿼리합니다.",
    "SelectA_Commentary": "SSE-KMS 구성은 유연하지만 추가 설정이 필요해 운영 오버헤드가 늘어날 수 있습니다. 버킷도 새로 생성해야 하므로 최소 오버헤드 요건에 부합하지 않습니다.",
    "SelectB": "새로운 S3 버킷을 생성 후 데이터를 로드합니다. S3 Cross-Region Replication (CRR)을 사용하여 암호화된 객체를 다른 리전의 S3 버킷으로 복제합니다. 서버 사이드 암호화로 AWS KMS multi-Region keys (SSE-KMS)를 사용합니다. Amazon RDS로 데이터를 쿼리합니다.",
    "SelectB_Commentary": "이 옵션 역시 SSE-KMS 구성이 필요하고, 분석용으로 RDS를 사용하여 서버 관리를 직접 해야 하므로 서버리스 목표와 어긋납니다.",
    "SelectC": "기존 S3 버킷에 데이터를 로드합니다. S3 Cross-Region Replication (CRR)을 사용하여 암호화된 객체를 다른 리전의 S3 버킷으로 복제합니다. 서버 사이드 암호화로 Amazon S3 managed encryption keys (SSE-S3)를 사용합니다. Amazon Athena로 데이터를 쿼리합니다.",
    "SelectC_Commentary": "SSE-S3 사용 시 자동 키 관리를 통해 오버헤드가 적으며, Athena는 서버리스로 운영 부담이 낮습니다. Cross-Region Replication도 바로 구성 가능해 요구 사항을 가장 간단히 충족합니다.",
    "SelectD": "기존 S3 버킷에 데이터를 로드합니다. S3 Cross-Region Replication (CRR)을 사용하여 암호화된 객체를 다른 리전의 S3 버킷으로 복제합니다. 서버 사이드 암호화로 Amazon S3 managed encryption keys (SSE-S3)를 사용합니다. Amazon RDS로 데이터를 쿼리합니다.",
    "SelectD_Commentary": "SSE-S3는 간편하지만 RDS는 서버리스가 아니기 때문에 관리 오버헤드가 증가하고, Athena만큼 손쉽게 데이터를 분석하기 어렵습니다."
  },
  {
    "Question_Number": "Q135",
    "Question_Description": "한 회사가 AWS에서 워크로드를 운영하고 있습니다. 외부 공급자의 service에 연결해야 합니다. 해당 service는 공급자의 VPC에 호스팅되어 있습니다. 회사 보안 팀은 연결이 사설로만 이루어져야 하고, 오직 target service에만 한정되어야 한다고 요구합니다. 또한 연결은 오직 회사의 VPC에서만 시작되어야 합니다. 이 요구사항을 만족하는 솔루션은 무엇입니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85994-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 외부 공급자의 VPC에 있는 target service에 대해 사설 연결을 구성해야 하는 상황입니다. VPC Peering은 광범위한 통신을 허용해 보안 요건을 만족하기 어렵고, NAT Gateway는 트래픽이 인터넷을 거쳐 이동하게 됩니다. 따라서 PrivateLink 기반의 VPC Endpoint를 사용하면 특정 service만 사설로 접근할 수 있고, 연결은 오직 회사 VPC에서 시작되므로 요구사항에 부합합니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1"
    ],
    "Keywords": [
      "사설 연결",
      "target service 제한",
      "AWS PrivateLink",
      "VPC endpoint",
      "VPC Peering",
      "NAT Gateway"
    ],
    "Terms": [
      "AWS PrivateLink",
      "VPC Peering",
      "NAT Gateway",
      "VPC Endpoint",
      "Virtual Private Gateway"
    ],
    "SelectA": "회사의 VPC와 공급자의 VPC 간에 VPC peering connection을 생성하고, 라우팅 테이블을 수정하여 target service로 연결합니다.",
    "SelectA_Commentary": "VPC Peering은 VPC 간의 양방향 연결을 제공합니다. target service만으로 트래픽을 한정하기 어려우며, 공급자 측에서의 접근 제한도 복잡해집니다.",
    "SelectB": "공급자에게 VPC 내에 virtual private gateway를 생성하도록 요청합니다. AWS PrivateLink를 사용하여 target service에 연결합니다.",
    "SelectB_Commentary": "Virtual private gateway는 일반적으로 site-to-site VPN 용도입니다. PrivateLink를 이용하려면 VPC Endpoint가 필요하며, 이 방식은 잘못된 구성입니다.",
    "SelectC": "회사의 VPC 퍼블릭 서브넷에 NAT Gateway를 생성하고, 라우팅 테이블을 수정하여 target service로 연결합니다.",
    "SelectC_Commentary": "NAT Gateway를 통한 트래픽은 인터넷을 경유하게 됩니다. 이는 사설 연결 및 특정 service만으로 제한한다는 요건을 충족하지 않습니다.",
    "SelectD": "공급자에게 target service에 대한 VPC endpoint를 생성하도록 요청합니다. AWS PrivateLink를 사용하여 target service에 연결합니다.",
    "SelectD_Commentary": "AWS PrivateLink 기반의 VPC Endpoint를 사용하면 오직 지정된 service로만 사설 연결이 이루어지고, 연결이 회사 VPC에서만 시작되도록 보장할 수 있습니다."
  },
  {
    "Question_Number": "Q136",
    "Question_Description": "한 회사가 온프레미스 PostgreSQL 데이터베이스를 Amazon Aurora PostgreSQL로 마이그레이션하려고 합니다. 마이그레이션 중에도 온프레미스 데이터베이스는 온라인 상태로 접근 가능해야 합니다. 또한 Aurora 데이터베이스는 온프레미스 데이터베이스와 동기화 상태를 유지해야 합니다. 이러한 요구 사항을 충족하기 위해 솔루션스 아키텍트가 수행해야 할 작업 조합은 무엇입니까? (정답은 두 개를 고르십시오.)",
    "Answer": "A,C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85438-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "AWS DMS를 사용하면 온프레미스 데이터베이스를 온라인 상태로 유지하면서 Aurora PostgreSQL과 실시간 동기화가 가능합니다. Replication Server를 생성하고 Ongoing Replication Task를 설정해 최소 다운타임으로 마이그레이션할 수 있습니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.2"
    ],
    "Keywords": [
      "온프레미스 PostgreSQL",
      "Amazon Aurora PostgreSQL",
      "AWS DMS",
      "온라인 상태",
      "동기화",
      "마이그레이션"
    ],
    "Terms": [
      "AWS Database Migration Service (AWS DMS)",
      "Ongoing Replication Task",
      "AWS DMS Replication Server",
      "AWS Schema Conversion Tool (AWS SCT)",
      "Amazon EventBridge"
    ],
    "SelectA": "Create an ongoing replication task.",
    "SelectA_Commentary": "DMS에서 지속적인 동기화를 수행하는 핵심 설정입니다. 온라인 상태로 데이터가 계속 복제되어 온프레미스와 Aurora 간 최신 상태를 유지합니다.",
    "SelectB": "Create a database backup of the on-premises database.",
    "SelectB_Commentary": "백업은 단발성 작업이므로 실시간 동기화에는 적합하지 않습니다. 온라인 동작을 위한 지속적 업데이트 기능이 제공되지 않습니다.",
    "SelectC": "Create an AWS Database Migration Service (AWS DMS) replication server.",
    "SelectC_Commentary": "DMS 동작에 필요한 컴퓨팅 리소스로, 원본·타깃 간 실시간 복제를 진행하려면 반드시 구성해야 하는 핵심 요소입니다.",
    "SelectD": "Convert the database schema by using the AWS Schema Conversion Tool (AWS SCT).",
    "SelectD_Commentary": "엔진 유형이 동일(동종 마이그레이션)인 경우 필수적이지 않을 수 있습니다. 필요 시 사용하지만 실시간 동기화 자체에는 영향이 적습니다.",
    "SelectE": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule to monitor the database synchronization.",
    "SelectE_Commentary": "EventBridge 규칙은 이벤트 알림용으로, 실시간 데이터 복제를 해주지는 않습니다. 동기화 작업 자체와는 직접 관련이 없습니다."
  },
  {
    "Question_Number": "Q137",
    "Question_Description": "한 회사는 AWS Organizations를 사용하여 각 사업부별로 전용 AWS account를 생성하여, 사업부 계정 요청 시 독립적으로 관리하고 있습니다. 어느 한 계정의 root user email 주소로 전송된 알림을 해당 root 사용자가 놓친 일이 있었고, 회사는 앞으로 모든 알림을 놓치지 않도록 하면서도 이러한 알림이 계정 관리자에게만 제한적으로 전달되기를 원합니다. 이 요구사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85997-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 root user email 주소로 갈 알림을 놓치지 않으면서도, 계정 관리자에게만 이메일이 전달되도록 모범적인 설정 방안을 묻습니다. 각 계정별로 distribution list를 두고 alternate contact를 설정하면, 중요한 알림이 제대로 전달되면서 오남용을 방지할 수 있어 요구사항을 만족합니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1"
    ],
    "Keywords": [
      "AWS Organizations",
      "AWS account",
      "root user email 주소",
      "account administrators",
      "distribution list",
      "alternate contact"
    ],
    "Terms": [
      "AWS Organizations",
      "AWS account",
      "root user email address",
      "account administrators",
      "alternate contacts",
      "distribution list",
      "management account root user"
    ],
    "SelectA": "회사 이메일 서버를 구성하여 AWS account root user email address로 전송되는 알림 이메일을 조직의 모든 사용자에게 전달하도록 설정합니다.",
    "SelectA_Commentary": "알림이 전체 조직에 전달되므로 계정 관리자에게만 제한된 전달이라는 요구사항을 충족하지 못합니다.",
    "SelectB": "모든 AWS account root user email 주소를 몇몇 관리자에게만 전달되는 distribution list로 구성하고, AWS Organizations 콘솔 또는 프로그래밍 방식으로 AWS account alternate contacts를 설정합니다.",
    "SelectB_Commentary": "소수 관리자에게만 알림이 전달되고 alternate contact를 통해 추가적으로 관리할 수 있어, 알림 누락 없이 계정 관리자만 확인하도록 제한할 수 있는 가장 적합한 방법입니다.",
    "SelectC": "모든 AWS account root user email 메시지를 한 명의 관리자에게만 보내도록 구성하고, 이 관리자가 알림을 모니터링하여 적절한 그룹에 전달하도록 합니다.",
    "SelectC_Commentary": "단일 관리자에게만 의존하면 알림 누락 위험이 높아지고, 적절한 담당자 구분이 복잡해집니다.",
    "SelectD": "모든 기존 AWS account와 새로 생성되는 계정이 동일한 root user email 주소를 사용하도록 설정합니다. 그리고 AWS Organizations 콘솔 또는 프로그래밍 방식을 통해 AWS account alternate contact를 구성합니다.",
    "SelectD_Commentary": "모든 계정이 동일한 이메일을 사용하면 관리 범위가 과도하게 넓어지고, 계정별 관리자 제한 전달이라는 조건을 충족하기 어렵습니다."
  },
  {
    "Question_Number": "Q138",
    "Question_Description": "한 회사는 AWS 상에서 전자상거래 애플리케이션을 운영하고 있습니다. 새로운 주문이 발생할 때마다 해당 주문은 단일 Availability Zone에서 실행 중인 Amazon EC2 인스턴스의 RabbitMQ queue에 메시지로 게시됩니다. 이 메시지들은 별도의 EC2 인스턴스에서 실행되는 다른 애플리케이션이 처리하며, 이 애플리케이션은 정보를 또 다른 EC2 인스턴스에 있는 PostgreSQL database에 저장합니다. 현재 모든 EC2 인스턴스는 동일한 Availability Zone에 있습니다. 회사는 높은 가용성과 최소한의 운영 오버헤드를 제공하도록 아키텍처를 재설계해야 합니다. 이러한 요구사항을 충족하기 위해 Solutions Architect는 무엇을 해야 합니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85999-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 RabbitMQ 메시지 처리와 PostgreSQL database를 사용하는 전자상거래 애플리케이션을 어떻게 고가용성과 낮은 운영 부담으로 재구성할지 묻습니다. Amazon MQ로 마이그레이션하면 Queue 운영 부담을 줄일 수 있고, Database를 Amazon RDS for PostgreSQL Multi-AZ로 이전하면 관리가 간소화되면서도 가용성을 높일 수 있습니다. 따라서 큐와 DB 모두 완전관리형 혹은 최소 관리로 전환해 운영 오버헤드를 최소화하는 것이 핵심입니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1",
      "2.2"
    ],
    "Keywords": [
      "RabbitMQ queue",
      "Amazon MQ",
      "Amazon RDS for PostgreSQL",
      "Multi-AZ",
      "Availability Zone",
      "운영 오버헤드",
      "고가용성",
      "Auto Scaling group",
      "PostgreSQL",
      "Amazon EC2"
    ],
    "Terms": [
      "Amazon MQ",
      "RabbitMQ",
      "Auto Scaling group",
      "Multi-AZ",
      "Amazon RDS for PostgreSQL",
      "Amazon EC2",
      "Availability Zone",
      "PostgreSQL"
    ],
    "SelectA": "Amazon MQ에서 RabbitMQ를 active/standby로 구성된 이중화 인스턴스로 마이그레이션합니다. 애플리케이션을 호스팅하는 EC2 인스턴스에 Multi-AZ Auto Scaling group을 생성합니다. PostgreSQL database를 호스팅하는 다른 EC2 인스턴스에도 Multi-AZ Auto Scaling group을 생성합니다.",
    "SelectA_Commentary": "Database를 직접 EC2에 Multi-AZ로 구성하면 운영 측면에서 여전히 부담이 높습니다.",
    "SelectB": "Amazon MQ에서 RabbitMQ를 active/standby로 구성된 이중화 인스턴스로 마이그레이션합니다. 애플리케이션을 호스팅하는 EC2 인스턴스에 Multi-AZ Auto Scaling group을 생성합니다. Database를 Multi-AZ Amazon RDS for PostgreSQL로 마이그레이션합니다.",
    "SelectB_Commentary": "큐와 DB 모두 완전관리형 또는 최소한의 관리로 구성해 고가용성과 낮은 운영 오버헤드를 확보하는 최적의 솔루션입니다.",
    "SelectC": "RabbitMQ queue를 호스팅하는 EC2 인스턴스에 Multi-AZ Auto Scaling group을 생성합니다. 애플리케이션을 호스팅하는 다른 EC2 인스턴스에도 Multi-AZ Auto Scaling group을 생성합니다. Database는 Multi-AZ Amazon RDS for PostgreSQL로 마이그레이션합니다.",
    "SelectC_Commentary": "RabbitMQ를 직접 EC2에서 Multi-AZ로 운영하면 관리가 복잡해지고, 오버헤드가 큽니다.",
    "SelectD": "RabbitMQ queue를 호스팅하는 EC2 인스턴스, 애플리케이션을 호스팅하는 EC2 인스턴스, PostgreSQL database를 호스팅하는 EC2 인스턴스 각각에 대해 모두 Multi-AZ Auto Scaling group을 생성합니다.",
    "SelectD_Commentary": "큐와 DB 모두 EC2 기반 Multi-AZ로 구성하면 운영 부담이 크고 관리가 복잡해집니다."
  },
  {
    "Question_Number": "Q139",
    "Question_Description": "보고 팀은 매일 Amazon S3 버킷에 업로드되는 파일들을 수령합니다. 현재는 보고 팀이 매일 같은 시각에 이 초기 S3 버킷에서 파일을 검토하고, Amazon QuickSight에 활용하기 위해 분석용 S3 버킷으로 수동 복사하고 있습니다. 하지만 추가 팀들이 더 많은 용량의 파일들을 초기 S3 버킷으로 전송하기 시작했습니다. 보고 팀은 파일이 초기 S3 버킷에 업로드되는 즉시 자동으로 분석용 S3 버킷에 복사되도록 원합니다. 또한 AWS Lambda 함수를 이용해 복사된 데이터에 대해 패턴 매칭 코드를 실행하고 싶으며, 파일들을 Amazon SageMaker Pipelines에도 전달하기를 원합니다. 최소한의 운영 오버헤드로 이를 만족하려면 어떤 솔루션을 구현해야 합니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85872-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "S3 Replication을 사용하면 초기 S3 버킷에서 분석용 S3 버킷으로 파일 복사를 자동화할 수 있어 운영 부담이 크게 줄어듭니다. 또한 분석용 버킷에서의 객체 생성 이벤트를 Amazon EventBridge와 연계하면 Lambda로 패턴 매칭 함수를 호출하고 SageMaker Pipelines로 데이터를 전달할 수 있어 가장 간단하고 확장성이 뛰어납니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1"
    ],
    "Keywords": [
      "분석용 S3 버킷 자동 복사",
      "AWS Lambda 패턴 매칭",
      "S3 Replication",
      "Amazon SageMaker Pipelines",
      "최소 운영 오버헤드"
    ],
    "Terms": [
      "Amazon S3",
      "S3 Replication",
      "AWS Lambda",
      "Amazon SageMaker Pipelines",
      "Amazon EventBridge (Amazon CloudWatch Events)",
      "S3 이벤트 알림",
      "s3:ObjectCreated:Put"
    ],
    "SelectA": "Lambda 함수를 생성하여 파일을 분석용 S3 버킷으로 복사합니다. 분석용 S3 버킷에 S3 이벤트 알림을 설정하고, Lambda와 SageMaker Pipelines를 이벤트 알림 대상으로 구성합니다. 이벤트 유형은 s3:ObjectCreated:Put으로 설정합니다.",
    "SelectA_Commentary": "Lambda로 수동 복사 과정을 대체하지만, 복제 로직을 직접 유지·관리해야 하므로 운영 오버헤드가 높아집니다.",
    "SelectB": "Lambda 함수를 만들어 파일을 분석용 S3 버킷으로 복사합니다. 분석용 S3 버킷에서 Amazon EventBridge(CloudWatch Events)로 이벤트 알림을 보내도록 설정합니다. EventBridge(CloudWatch Events)에 ObjectCreated 규칙을 생성하고, Lambda와 SageMaker Pipelines를 대상으로 구성합니다.",
    "SelectB_Commentary": "A와 마찬가지로 파일 복사를 Lambda가 직접 처리해야 하므로, 코드를 유지·관리하는 부담이 여전히 큽니다.",
    "SelectC": "두 S3 버킷 간에 S3 Replication을 구성합니다. 분석용 S3 버킷에 S3 이벤트 알림을 설정하여 Lambda와 SageMaker Pipelines를 이벤트 대상로 구성하고, 이벤트 유형은 s3:ObjectCreated:Put으로 설정합니다.",
    "SelectC_Commentary": "파일 복사는 자동화되지만 S3 이벤트 알림에서 SageMaker Pipelines를 직접 대상으로 삼기가 번거로울 수 있습니다. 다른 AWS 서비스 중개가 필요해 추가 설정이 늘어날 수 있습니다.",
    "SelectD": "두 S3 버킷 간에 S3 Replication을 구성합니다. 분석용 S3 버킷에서 Amazon EventBridge(CloudWatch Events)로 이벤트 알림을 보내도록 설정합니다. EventBridge(CloudWatch Events)에서 ObjectCreated 규칙을 생성하고, Lambda와 SageMaker Pipelines를 대상으로 구성합니다.",
    "SelectD_Commentary": "S3 Replication으로 파일 복사를 자동화하고, EventBridge로 이벤트를 중앙에서 처리하여 Lambda와 SageMaker Pipelines를 유연하게 호출할 수 있습니다. 운영 오버헤드가 가장 적고 확장성도 뛰어납니다."
  },
  {
    "Question_Number": "Q140",
    "Question_Description": "한 솔루션스 아키텍트가 AWS 상에서 애플리케이션을 실행하는 비용을 최적화하도록 회사에 조언해야 합니다. 이 애플리케이션은 Amazon EC2, AWS Fargate, AWS Lambda를 사용합니다. EC2 인스턴스는 데이터 수집 계층을 실행하며 사용량이 산발적이고 예측할 수 없습니다. 해당 워크로드는 언제든 중단될 수 있습니다. 애플리케이션의 프런트엔드는 Fargate에서 실행되고, Lambda는 API 계층을 제공합니다. 프런트엔드와 API 계층의 사용량은 향후 1년 동안 예측 가능합니다. 이 애플리케이션을 가장 비용 효율적으로 호스팅하기 위해 어떤 구매 옵션 조합을 사용해야 합니까? (2개를 선택하세요)",
    "Answer": "A,C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86083-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 산발적이고 예측 불가능한 워크로드와 예측 가능한 워크로드를 분리해 비용 최적화 전략을 찾는 것입니다. 데이터 수집 계층에는 Spot을 활용하여 비용을 절감하고, 예측 가능한 Fargate와 Lambda에는 Compute Savings Plan을 적용해 최대한 유연성을 확보하면서 비용 효율을 높일 수 있습니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.2"
    ],
    "Keywords": [
      "비용 최적화",
      "EC2",
      "Fargate",
      "Lambda",
      "Spot Instances",
      "Compute Savings Plan"
    ],
    "Terms": [
      "Amazon EC2",
      "AWS Fargate",
      "AWS Lambda",
      "Spot Instances",
      "On-Demand Instances",
      "Compute Savings Plan",
      "EC2 instance Savings Plan",
      "All Upfront Reserved Instances"
    ],
    "SelectA": "데이터 수집 계층에 Spot Instances를 사용합니다.",
    "SelectA_Commentary": "Spot Instances는 필요 시 언제든 중단될 수 있는 불규칙적 워크로드에 매우 저렴하게 적합합니다.",
    "SelectB": "데이터 수집 계층에 On-Demand Instances를 사용합니다.",
    "SelectB_Commentary": "On-Demand Instances는 유연하지만 비용이 더 높아, 산발적인 워크로드에는 비효율적입니다.",
    "SelectC": "프런트엔드와 API 계층에 1년 Compute Savings Plan을 구매합니다.",
    "SelectC_Commentary": "Compute Savings Plan은 Fargate, Lambda, 그리고 EC2 모든 서비스에 적용되므로 예측 가능한 워크로드에 효과적입니다.",
    "SelectD": "데이터 수집 계층에 1년 All Upfront Reserved Instances를 구매합니다.",
    "SelectD_Commentary": "Reserved Instances는 예측 불가능한 워크로드에 적합하지 않고, 중단 위험이 큰 환경에서 유연성이 떨어집니다.",
    "SelectE": "프런트엔드와 API 계층에 1년 EC2 instance Savings Plan을 구매합니다.",
    "SelectE_Commentary": "EC2 instance Savings Plan은 EC2 인스턴스에만 적용되어 Fargate와 Lambda에서는 혜택을 받지 못합니다."
  },
  {
    "Question_Number": "Q141",
    "Question_Description": "한 회사가 전 세계 사용자들에게 글로벌 속보, 지역 알림, 날씨 업데이트를 제공하는 웹 기반 포털을 운영합니다. 포털은 정적 콘텐츠와 동적 콘텐츠를 혼합해 각 사용자에게 맞춤화된 화면을 제공합니다. 콘텐츠는 Application Load Balancer(ALB) 뒤에서 동작하는 Amazon EC2 인스턴스 상의 API 서버를 통해 HTTPS로 제공됩니다. 회사는 전 세계 모든 사용자에게 가능한 한 지연 없이 빠른 콘텐츠 제공을 원합니다. 가장 낮은 지연 시간을 보장하기 위해 솔루션스 아키텍트는 애플리케이션을 어떻게 설계해야 합니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85439-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 전 세계 대상으로 개인화된 정적·동적 콘텐츠를 빠르게 제공해야 하는 상황에서, 지연 시간을 최소화하는 네트워크 아키텍처를 설계하는 방법을 묻습니다. Amazon CloudFront는 정적 및 동적 콘텐츠 전송 시 Edge Location을 통해 전 세계 곳곳에서 발생하는 요청에 대해 빠른 응답을 제공합니다. ALB를 직접 오리진으로 두면 동적 콘텐츠도 CloudFront 배포를 통해 캐시 이점과 네트워크 가속을 누릴 수 있어 최소 지연 응답이 가능합니다. 따라서 단일 Region에 배포하고 CloudFront를 활용하는 구성이 가장 효율적입니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.4"
    ],
    "Keywords": [
      "전 세계 사용자",
      "정적 콘텐츠",
      "동적 콘텐츠",
      "낮은 지연 시간",
      "Application Load Balancer",
      "Amazon CloudFront"
    ],
    "Terms": [
      "Amazon CloudFront",
      "Application Load Balancer (ALB)",
      "Amazon EC2",
      "Amazon Route 53",
      "latency routing policy",
      "geolocation routing policy",
      "AWS Region",
      "HTTPS"
    ],
    "SelectA": "AWS 리전 하나에 애플리케이션 스택을 배포하고, ALB를 origin으로 설정한 Amazon CloudFront를 통해 모든 정적 및 동적 콘텐츠를 서빙합니다.",
    "SelectA_Commentary": "전 세계 Edge Location을 활용해 모든 콘텐츠를 빠르고 일관되게 제공하는 최적의 솔루션입니다.",
    "SelectB": "애플리케이션 스택을 두 개의 AWS 리전에 배포하고, Amazon Route 53 latency routing policy를 사용해 가장 가까운 리전의 ALB에서 콘텐츠를 제공하도록 합니다.",
    "SelectB_Commentary": "복수 리전 배포로 글로벌 성능을 높일 수 있지만, 관리 복잡성이 증가하고 CloudFront의 광범위한 캐싱 및 가속 이점을 활용하지 못해 A보다 효율이 낮습니다.",
    "SelectC": "애플리케이션 스택을 단일 AWS 리전에 배포하고, 정적 콘텐츠는 Amazon CloudFront로, 동적 콘텐츠는 ALB에서 직접 서빙합니다.",
    "SelectC_Commentary": "동적 콘텐츠의 지연이 증가할 수 있어 최소 지연을 보장하기 어렵습니다.",
    "SelectD": "애플리케이션 스택을 두 개의 AWS 리전에 배포하고, Amazon Route 53 geolocation routing policy를 사용해 가장 가까운 리전의 ALB에서 모든 콘텐츠를 제공합니다.",
    "SelectD_Commentary": "지리적 정책은 특정 국가 또는 지역별 라우팅에 유용하지만, 네트워크 지연을 최적으로 줄이지 못해 A보다 성능이 떨어집니다."
  },
  {
    "Question_Number": "Q142",
    "Question_Description": "한 게임 회사가 고가용성 아키텍처를 설계하고 있습니다. 애플리케이션은 수정된 Linux 커널에서 동작하며 UDP 기반 트래픽만 지원합니다. 회사는 프론트엔드 계층이 가능한 최고의 사용자 경험을 제공하기를 원합니다. 이 계층은 지연 시간이 낮아야 하며, 가장 가까운 엣지 로케이션으로 트래픽을 라우팅해야 하고, 애플리케이션 엔드포인트에 진입하기 위한 고정 IP 주소를 제공해야 합니다. 이러한 요구사항을 충족하기 위해 Solutions Architect는 무엇을 해야 합니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86667-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 UDP 트래픽을 지원하는 게임 애플리케이션을 전 세계적으로 빠르고 안정적으로 서비스하기 위해 어떤 프론트엔드 구성을 사용해야 하는지를 묻습니다. AWS Global Accelerator는 UDP 같은 비 HTTP 트래픽에도 최적화되어 있고, 전용 엣지 네트워크를 통해 트래픽을 가장 가까운 리전으로 라우팅하며, 고정 IP 주소를 제공할 수 있어 요구사항을 가장 잘 충족합니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.2",
      "3.4"
    ],
    "Keywords": [
      "고가용성 아키텍처",
      "UDP 기반 트래픽",
      "지연 시간 최소화",
      "엣지 로케이션",
      "고정 IP 주소",
      "AWS Global Accelerator",
      "Network Load Balancer",
      "Amazon EC2",
      "EC2 Auto Scaling"
    ],
    "Terms": [
      "AWS Global Accelerator",
      "Amazon CloudFront",
      "Amazon EC2",
      "EC2 Auto Scaling",
      "Network Load Balancer",
      "Application Load Balancer",
      "Amazon Route 53",
      "AWS Lambda",
      "AWS Application Auto Scaling",
      "Amazon API Gateway"
    ],
    "SelectA": "Amazon Route 53을 구성하여 요청을 Application Load Balancer로 포워딩합니다. AWS Lambda를 AWS Application Auto Scaling에서 애플리케이션으로 사용합니다.",
    "SelectA_Commentary": "Application Load Balancer는 HTTP/HTTPS를 주로 처리하고 UDP 트래픽을 직접 지원하지 않으므로 적절하지 않습니다.",
    "SelectB": "Amazon CloudFront를 구성하여 요청을 Network Load Balancer로 포워딩합니다. AWS Lambda를 AWS Application Auto Scaling 그룹에서 애플리케이션으로 사용합니다.",
    "SelectB_Commentary": "CloudFront는 주로 HTTP 기반 콘텐츠 캐싱과 전달에 최적화되어 있어 UDP 트래픽 및 고정 IP 제공 측면에서 부적합합니다.",
    "SelectC": "AWS Global Accelerator를 구성하여 요청을 Network Load Balancer로 포워딩합니다. 애플리케이션에는 EC2 Auto Scaling 그룹의 Amazon EC2 인스턴스를 사용합니다.",
    "SelectC_Commentary": "AWS Global Accelerator가 UDP 트래픽, 고정 IP, 지연 시간 최소화를 모두 지원해 가장 적합한 솔루션입니다.",
    "SelectD": "Amazon API Gateway를 구성하여 요청을 Application Load Balancer로 포워딩합니다. 애플리케이션에는 EC2 Auto Scaling 그룹의 Amazon EC2 인스턴스를 사용합니다.",
    "SelectD_Commentary": "API Gateway는 REST 또는 WebSocket 등 HTTP 기반 API 호출에 최적화되어 있어 UDP 트래픽에는 적합하지 않습니다."
  },
  {
    "Question_Number": "Q143",
    "Question_Description": "한 회사가 기존의 온프레미스 모놀리식(monolithic) 애플리케이션을 AWS로 마이그레이션하려고 합니다. 이 회사는 기존 프론트엔드 코드와 백엔드 코드를 최대한 유지하면서, 애플리케이션을 더 작은 규모의 애플리케이션들로 분할하고자 합니다. 각 애플리케이션은 서로 다른 팀에서 관리할 예정입니다. 또한 높은 확장성을 갖추고 운영 오버헤드를 최소화하는 솔루션이 필요합니다. 이러한 요구사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86473-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 기존 모놀리식 애플리케이션을 분할해 각 팀이 관리 가능하도록 마이크로서비스 형태로 전환하면서, 높은 확장성과 낮은 운영 부담을 원하는 시나리오입니다. Amazon ECS를 사용하면 컨테이너로 각각의 서비스를 분리해 관리가 용이하며, Application Load Balancer와 결합해 탄력적인 확장이 가능합니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.2"
    ],
    "Keywords": [
      "모놀리식 애플리케이션",
      "AWS 마이그레이션",
      "Amazon ECS",
      "마이크로서비스",
      "운영 오버헤드 최소화"
    ],
    "Terms": [
      "AWS Lambda",
      "Amazon API Gateway",
      "AWS Amplify",
      "Amazon EC2",
      "Application Load Balancer",
      "Auto Scaling group",
      "Amazon Elastic Container Service (Amazon ECS)"
    ],
    "SelectA": "AWS Lambda에서 애플리케이션을 호스팅하고, Amazon API Gateway와 연동합니다.",
    "SelectA_Commentary": "서버리스 환경이지만, 기존 코드를 크게 수정해야 할 수 있고, 모놀리식 구조를 그대로 유지하기에는 Lambda로의 전체 마이그레이션이 복잡할 수 있습니다.",
    "SelectB": "AWS Amplify로 애플리케이션을 호스팅하고, Amazon API Gateway와 연동된 AWS Lambda에 연결합니다.",
    "SelectB_Commentary": "Amplify는 프론트엔드 호스팅에 적합하지만, 이미 완성된 백엔드를 크게 분할하기에는 제약이 많아 전체 요구사항을 만족하기 어렵습니다.",
    "SelectC": "Amazon EC2 인스턴스에서 애플리케이션을 호스팅합니다. Application Load Balancer와 Auto Scaling group으로 구성합니다.",
    "SelectC_Commentary": "EC2 기반 구성은 직접 서버를 관리해야 하므로 운영 오버헤드가 높으며, 마이크로서비스로 자유롭게 분할하기에도 관리 부담이 큽니다.",
    "SelectD": "Amazon Elastic Container Service(Amazon ECS)에서 애플리케이션을 호스팅합니다. Application Load Balancer를 Amazon ECS 대상 그룹으로 설정합니다.",
    "SelectD_Commentary": "컨테이너 기반으로 각 서비스를 분리해 팀별 관리가 가능하며, ECS가 인프라 관리를 상당 부분 자동화하여 확장성과 운영 편의성을 모두 확보할 수 있습니다."
  },
  {
    "Question_Number": "Q144",
    "Question_Description": "한 회사가 글로벌 eCommerce 애플리케이션의 데이터 스토어로 Amazon Aurora를 사용하기 시작했습니다. 대규모 리포트를 실행할 때 개발자들은 eCommerce 애플리케이션이 성능 저하를 겪고 있다고 보고했습니다. Amazon CloudWatch의 지표를 살펴본 결과, 월간 리포트가 실행될 때 ReadIOPS와 CPUUtilization 지표가 급증한다는 사실을 솔루션스 아키텍트가 확인했습니다. 가장 비용 효율적인 솔루션은 무엇입니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86781-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 월간 리포트 실행 시 발생하는 높은 읽기 부하로 인해 Aurora 메인 DB에 성능 저하가 일어나는 상황을 해결하면서도 비용 효율을 달성해야 하는 시나리오입니다. Aurora Replica를 활용하면 보고서 트래픽을 오프로딩해 메인 DB의 부하를 줄이면서 추가적인 인프라나 IOPS 비용을 과도하게 발생시키지 않아, 가장 합리적인 선택입니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.3"
    ],
    "Keywords": [
      "글로벌 eCommerce 애플리케이션",
      "Amazon Aurora",
      "월간 리포트",
      "성능 저하",
      "비용 효율",
      "Aurora Replica"
    ],
    "Terms": [
      "Amazon Aurora",
      "Amazon Redshift",
      "Aurora Replica",
      "Amazon CloudWatch",
      "ReadIOPS",
      "CPUUtilization",
      "Provisioned IOPS",
      "DB instance class"
    ],
    "SelectA": "월간 리포팅을 Amazon Redshift로 마이그레이션합니다.",
    "SelectA_Commentary": "데이터 웨어하우스 솔루션이지만 마이그레이션과 운영 복잡도가 높고, 규모에 따라 추가 비용이 발생할 수 있어 가장 비용 효율적이지 않습니다.",
    "SelectB": "월간 리포팅을 Aurora Replica로 마이그레이션합니다.",
    "SelectB_Commentary": "리포트 실행 시 읽기 부하를 Replica로 분산해 메인 DB 성능 저하를 최소화하고, 필요한 만큼만 용량을 확장해 비용 효율을 높입니다.",
    "SelectC": "Aurora 데이터베이스를 더 큰 인스턴스 클래스로 마이그레이션합니다.",
    "SelectC_Commentary": "즉각적인 성능 향상을 기대할 수 있으나, 인스턴스 비용이 지속해서 증가하여 장기적으로는 비용 효율적이지 않습니다.",
    "SelectD": "Aurora 인스턴스의 Provisioned IOPS를 증가시킵니다.",
    "SelectD_Commentary": "IO 성능은 향상되지만 CPUUtilization 문제는 여전히 남을 수 있으며, 프로비저닝된 IOPS 비용이 높아질 수 있습니다."
  },
  {
    "Question_Number": "Q145",
    "Question_Description": "한 회사가 웹사이트 분석 애플리케이션을 단일 Amazon EC2 On-Demand Instance에서 호스팅하고 있습니다. 이 분석 소프트웨어는 PHP로 작성되었으며, MySQL 데이터베이스를 사용합니다. PHP를 제공하는 웹 서버와 데이터베이스 서버 모두 EC2 인스턴스 안에 함께 배포되어 있습니다. 현재 애플리케이션은 트래픽이 많은 시간대에 성능 저하와 5xx 오류를 보이고 있으며, 회사는 애플리케이션이 끊김 없이 확장되도록 만들고자 합니다. 이 요구사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86474-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 단일 EC2 인스턴스에 모든 구성 요소가 모노리틱하게 배포되어 생기는 확장 한계를 해결하고, 트래픽 급증 시 발생하는 성능 저하와 5xx 오류를 방지하려는 상황입니다. 데이터베이스를 Amazon Aurora MySQL로 분리해 성능을 높이고, Auto Scaling group과 Spot Fleet을 활용하여 무중단으로 인스턴스를 확장·축소함으로써 비용 효율성과 확장성을 동시에 달성하는 것이 핵심입니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1",
      "2.2"
    ],
    "Keywords": [
      "EC2 On-Demand Instance",
      "Amazon Aurora MySQL DB instance",
      "AMI",
      "Auto Scaling group",
      "Spot Fleet",
      "Application Load Balancer",
      "비용 효율",
      "5xx 오류",
      "무중단 확장"
    ],
    "Terms": [
      "Amazon EC2 On-Demand Instance",
      "MySQL",
      "Amazon RDS for MySQL",
      "Amazon Aurora MySQL",
      "PHP",
      "Application Load Balancer",
      "AWS Lambda",
      "Amazon CloudWatch alarm",
      "AMIs",
      "Auto Scaling group",
      "Spot Fleet",
      "Amazon Route 53"
    ],
    "SelectA": "데이터베이스를 Amazon RDS for MySQL DB instance로 마이그레이션합니다. 웹 애플리케이션을 AMI로 생성한 뒤 두 번째 EC2 On-Demand Instance를 시작합니다. Application Load Balancer로 각 EC2 인스턴스에 부하를 분산합니다.",
    "SelectA_Commentary": "RDS로 DB를 이전하고 인스턴스를 2대로 늘려도 자동 확장이 어렵고 On-Demand만 이용해 비용 면에서 최적이 아닙니다.",
    "SelectB": "데이터베이스를 Amazon RDS for MySQL DB instance로 마이그레이션합니다. 웹 애플리케이션을 AMI로 생성한 뒤 두 번째 EC2 On-Demand Instance를 시작합니다. Amazon Route 53 가중 라우팅을 사용해 두 EC2 인스턴스 간 트래픽을 분산합니다.",
    "SelectB_Commentary": "가중 라우팅으로 부하를 분산하지만 Auto Scaling 지원이 없어 사용량 증감 시 유연한 확장이 어렵습니다.",
    "SelectC": "데이터베이스를 Amazon Aurora MySQL DB instance로 마이그레이션합니다. AWS Lambda 함수를 작성해 EC2 인스턴스를 정지 후 인스턴스 유형을 변경하도록 합니다. CPU 사용률이 75%를 초과하면 Amazon CloudWatch alarm으로 Lambda 함수를 호출합니다.",
    "SelectC_Commentary": "수동에 가까운 인스턴스 유형 변경 방식이라 실시간 트래픽 증가에 부하 대응이 원활하지 않고 운영 부담이 큽니다.",
    "SelectD": "데이터베이스를 Amazon Aurora MySQL DB instance로 마이그레이션합니다. 웹 애플리케이션 AMI를 생성한 뒤 Launch Template에 적용합니다. 해당 Launch Template로 Auto Scaling group을 구성하고 Spot Fleet을 사용하도록 설정합니다. Application Load Balancer를 Auto Scaling group에 연결합니다.",
    "SelectD_Commentary": "Aurora로 DB를 분리해 성능을 확보하고, Spot Fleet 및 Auto Scaling으로 무중단 확장과 비용 효율을 동시에 달성하는 최적의 방법입니다."
  },
  {
    "Question_Number": "Q146",
    "Question_Description": "회사는 Application Load Balancer 뒤에 Amazon EC2 On-Demand Instances 그룹에서 무상태 웹 애플리케이션을 프로덕션으로 운영하고 있습니다. 애플리케이션은 매 영업일에 8시간 동안 높은 사용량을 기록하고, 야간에는 보통 수준의 일정한 사용량을 유지하며, 주말에는 낮은 사용량을 보입니다. 회사는 애플리케이션 가용성에 영향을 주지 않으면서 EC2 비용을 최소화하고자 합니다. 이러한 요구사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86750-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 수요가 크게 변동하는 웹 애플리케이션 환경에서 비용을 절감하면서 가용성을 유지하는 방법을 묻습니다. 기본 사용량은 Reserved Instances로 확보하고, 추가 수요는 Spot Instances로 처리함으로써 예측 가능한 운영 비용 절감과 높은 가용성을 모두 달성할 수 있습니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.2"
    ],
    "Keywords": [
      "비용 최소화",
      "애플리케이션 가용성",
      "EC2",
      "Spot Instances",
      "Reserved Instances",
      "On-Demand Instances"
    ],
    "Terms": [
      "Amazon EC2 On-Demand Instances",
      "Reserved Instances",
      "Spot Instances",
      "Dedicated Instances",
      "Application Load Balancer",
      "Stateless Web Application"
    ],
    "SelectA": "전체 워크로드를 Spot Instances로 사용합니다.",
    "SelectA_Commentary": "Spot 용량은 회수될 수 있어 가용성을 보장하기 어렵고, 고정적으로 필요한 사용량을 덮기에는 위험도가 높아 적합하지 않습니다.",
    "SelectB": "기본적인 사용량 수준에 대해서는 Reserved Instances를 사용하고, 애플리케이션이 필요한 추가 용량에 대해서는 Spot Instances를 사용합니다.",
    "SelectB_Commentary": "기본적으로 일정량의 사용량을 안정적으로 확보하면서도 필요 시 Spot Instances로 유연하게 대처하여 비용 최적화와 가용성을 모두 달성할 수 있으므로 적합한 솔루션입니다.",
    "SelectC": "기본적인 사용량 수준에 대해서는 On-Demand Instances를 사용하고, 애플리케이션이 필요한 추가 용량에 대해서는 Spot Instances를 사용합니다.",
    "SelectC_Commentary": "On-Demand Instances만으로 기본 사용량을 처리하면 비용 절감이 제한적입니다. Reserved Instances보다 상대적으로 비용이 높아 최적의 방법은 아닙니다.",
    "SelectD": "기본적인 사용량 수준에 대해서는 Dedicated Instances를 사용하고, 애플리케이션이 필요한 추가 용량에 대해서는 On-Demand Instances를 사용합니다.",
    "SelectD_Commentary": "Dedicated Instances는 물리적 서버 전용 사용으로 비용이 높아 기본 사용량을 처리하기에 비효율적이며, 전반적인 비용 절감에 부적합합니다."
  },
  {
    "Question_Number": "Q147",
    "Question_Description": "한 회사에서 중요한 애플리케이션의 로그 파일을 10년 동안 보관해야 합니다. 이 애플리케이션 팀은 최근 1개월 이내의 로그는 자주 문제 해결을 위해 접근하지만, 1개월이 지난 로그는 거의 접근하지 않습니다. 애플리케이션은 매달 10TB 이상의 로그를 생성합니다. 비용 효율성을 최대화하면서 이러한 요구 사항을 충족시키는 스토리지 옵션은 무엇입니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86864-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 주로 월간 10TB 규모의 방대한 로그를 10년 동안 보존하는 동시에 최근 한 달간은 빈번히 조회되는 특성을 고려한 방법을 묻습니다. S3 Lifecycle policy를 활용하면 오래된 로그를 저비용 스토리지인 S3 Glacier Deep Archive로 자동 전환하여 운영 과정과 비용을 모두 효율화할 수 있습니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.1"
    ],
    "Keywords": [
      "로그 보관",
      "10년 보관",
      "코스트 최적화",
      "한 달 지난 로그",
      "저장 비용"
    ],
    "Terms": [
      "Amazon S3",
      "AWS Backup",
      "S3 Glacier Deep Archive",
      "S3 Lifecycle policy",
      "Amazon CloudWatch Logs"
    ],
    "SelectA": "Amazon S3에 로그를 저장하고, 1개월 지난 로그는 AWS Backup을 사용하여 S3 Glacier Deep Archive로 이동합니다.",
    "SelectA_Commentary": "AWS Backup은 S3 Glacier Deep Archive 직접 전환을 지원하지 않으므로 요구 사항을 충족하기 어렵습니다.",
    "SelectB": "Amazon S3에 로그를 저장하고, S3 Lifecycle policy를 통해 1개월 지난 로그를 S3 Glacier Deep Archive로 이동합니다.",
    "SelectB_Commentary": "S3 Lifecycle policy로 자동 전환을 설정하면 장기 보관 비용이 크게 절감되고 운영 부담도 최소화되어 가장 적합한 솔루션입니다.",
    "SelectC": "Amazon CloudWatch Logs에 로그를 저장하고, 1개월 지난 로그는 AWS Backup을 사용하여 S3 Glacier Deep Archive로 이동합니다.",
    "SelectC_Commentary": "AWS Backup이 CloudWatch Logs에 직접 적용되지 않으며, Glacier Deep Archive 전환을 지원하지 않아 비효율적입니다.",
    "SelectD": "Amazon CloudWatch Logs에 로그를 저장하고, Amazon S3 Lifecycle policy를 사용하여 1개월 지난 로그를 S3 Glacier Deep Archive로 이동합니다.",
    "SelectD_Commentary": "CloudWatch Logs에서 직접 S3 Lifecycle policy를 설정할 수 없으므로 요구 사항을 만족하지 못합니다."
  },
  {
    "Question_Number": "Q148",
    "Question_Description": "한 회사는 다음과 같은 컴포넌트로 구성된 데이터 수집 워크플로우를 사용하고 있습니다:\n1) 새로운 데이터가 도착했을 때 알림을 받는 Amazon Simple Notification Service(Amazon SNS) topic\n2) 데이터를 처리하고 저장하는 AWS Lambda function\n\n해당 워크플로우는 가끔 네트워크 연결 문제로 인해 실패가 발생합니다. 한 번 실패가 발생하면, 회사가 수동으로 작업을 재실행하지 않는 이상 데이터가 수집되지 않습니다. 모든 알림이 최종적으로 처리되도록 하려면 어떻게 해야 합니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85424-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 일시적인 네트워크 장애가 발생했을 때도 데이터가 유실되지 않도록 알림을 안정적으로 처리하는 방법에 대한 것입니다. SNS에서 Lambda로 직접 전달이 실패했을 때 재시도 또는 보조 경로가 필요합니다. Amazon SQS 큐를 on-failure 대상(DLQ)으로 구성하면 재처리를 자동화하여 모든 알림이 결국 처리되도록 보장할 수 있습니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1",
      "2.2"
    ],
    "Keywords": [
      "네트워크 연결 문제",
      "데이터 수집 실패",
      "모든 알림 최종 처리",
      "on-failure 대상",
      "SQS 큐"
    ],
    "Terms": [
      "Amazon SNS",
      "AWS Lambda",
      "Amazon SQS",
      "on-failure destination",
      "SNS topic’s retry strategy",
      "네트워크 연결 문제"
    ],
    "SelectA": "Lambda function을 여러 가용 영역에 배포하도록 구성합니다.",
    "SelectA_Commentary": "Lambda function 자체의 고가용성을 높이지만, 네트워크 문제로 인한 처리 누락을 막지는 못하므로 적절한 해법이 아닙니다.",
    "SelectB": "Lambda function 설정을 수정하여 CPU와 메모리 할당량을 늘립니다.",
    "SelectB_Commentary": "더 많은 리소스를 할당해도 네트워크 장애로 인해 호출 자체가 실패하면 데이터를 재처리할 방법이 없으므로 효과적이지 않습니다.",
    "SelectC": "SNS topic의 재시도 전략을 구성하여 재시도 횟수와 재시도 간 대기 시간을 늘립니다.",
    "SelectC_Commentary": "일정 횟수 이상 실패 시 장기 보관 및 추후 재처리가 어려울 수 있으므로, 근본적으로 알림을 모아두고 재처리할 보조 경로(DLQ)가 필요합니다.",
    "SelectD": "Amazon Simple Queue Service(Amazon SQS) 큐를 on-failure 대상으로 구성하고, Lambda function이 해당 큐에 있는 메시지를 처리하도록 수정합니다.",
    "SelectD_Commentary": "SNS에서 Lambda 호출이 실패하면 알림이 SQS 큐에 쌓여 재처리할 수 있으므로 결국 모든 알림을 놓치지 않고 처리할 수 있는 가장 안정적인 방법입니다."
  },
  {
    "Question_Number": "Q149",
    "Question_Description": "한 회사에는 이벤트 데이터를 생성하는 서비스가 있습니다. 이 회사는 수신되는 즉시 AWS를 통해 이벤트 데이터를 처리하고자 합니다. 데이터는 특정 순서로 작성되며, 이 순서를 처리 전 과정에서 반드시 유지해야 합니다. 회사는 운영 오버헤드를 최소화할 수 있는 솔루션을 구현하고자 합니다. 솔루션스 아키텍트는 이를 어떻게 달성해야 합니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86784-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 이벤트 데이터가 작성된 순서를 반드시 지켜야 하며 운영을 단순화해야 하는 상황입니다. Amazon SQS FIFO 큐는 메시지 순서를 보장하고, AWS Lambda를 사용하면 서버 관리 없이 메시지를 자동으로 처리할 수 있어 운영 오버헤드가 최소화됩니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.1"
    ],
    "Keywords": [
      "이벤트 데이터",
      "FIFO 큐",
      "메시지 순서 유지",
      "운영 오버헤드 최소화",
      "Lambda"
    ],
    "Terms": [
      "Amazon SQS FIFO queue",
      "Amazon SQS standard queue",
      "Amazon SNS topic",
      "AWS Lambda",
      "메시지 순서 보장"
    ],
    "SelectA": "Amazon Simple Queue Service(Amazon SQS) FIFO 큐를 생성해 메시지를 저장합니다. 그리고 AWS Lambda 함수를 설정해 해당 큐에서 메시지를 처리하도록 합니다.",
    "SelectA_Commentary": "FIFO 큐는 순서를 보장하며, Lambda로 자동 처리하면 운영이 간단해집니다. 정답입니다.",
    "SelectB": "Amazon Simple Notification Service(Amazon SNS) 토픽을 생성하여 처리할 페이로드를 포함하는 알림을 전송합니다. AWS Lambda 함수를 구독자로 설정합니다.",
    "SelectB_Commentary": "SNS는 메시지 순서 보장이 어렵습니다. 순서 유지가 필수이므로 적합하지 않습니다.",
    "SelectC": "Amazon Simple Queue Service(Amazon SQS) standard 큐를 생성해 메시지를 저장합니다. 그리고 AWS Lambda 함수를 설정해 해당 큐의 메시지를 독립적으로 처리하도록 합니다.",
    "SelectC_Commentary": "standard 큐는 순서를 보장하지 않으므로 이벤트 데이터 순서가 어긋날 수 있습니다.",
    "SelectD": "Amazon Simple Notification Service(Amazon SNS) 토픽을 생성하여 처리할 페이로드를 포함하는 알림을 전송합니다. 구독자로 Amazon Simple Queue Service(Amazon SQS) 큐를 설정합니다.",
    "SelectD_Commentary": "SNS와 standard 큐 조합은 순서를 강제할 수 없으므로 요구 사항을 충족하지 못합니다."
  },
  {
    "Question_Number": "Q150",
    "Question_Description": "한 회사가 온프레미스 서버에서 Amazon EC2 인스턴스로 애플리케이션을 마이그레이션 중입니다. 마이그레이션 설계 요구 사항으로, 솔루션스 아키텍트는 인프라 메트릭 알람을 구현해야 합니다. 이 회사는 CPU Utilization이 50%를 짧게 초과하는 상황에는 조치를 취할 필요가 없지만, CPU Utilization이 50%를 초과하고 동시에 디스크의 Read IOPS가 높아지는 경우 즉시 대응해야 합니다. 또한 오탐(거짓 경보)을 줄여야 합니다. 이러한 요구사항을 충족하기 위해서는 어떻게 해야 합니까?",
    "Answer": "A",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86034-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 여러 지표를 결합해 오탐을 줄이고 즉시 필요한 상황에만 알람을 받도록 구성하는 방법에 관한 것입니다. CPU가 50%를 넘고 Read IOPS가 동시에 높은 상황을 한 번에 감지해야 하므로, CloudWatch Composite Alarms를 사용해 조건을 결합하고 필요시 한 번에 알람을 보낼 수 있습니다.",
    "Domain": "복원력을 갖춘 아키텍처 설계",
    "Tasks": [
      "2.2"
    ],
    "Keywords": [
      "Amazon EC2",
      "CPU Utilization",
      "Read IOPS",
      "오탐 감소",
      "인프라 메트릭 알람"
    ],
    "Terms": [
      "Amazon EC2",
      "CloudWatch Composite Alarms",
      "CloudWatch Dashboards",
      "CloudWatch Synthetics",
      "CloudWatch Metric Alarms",
      "CPU Utilization",
      "read IOPS",
      "on-premises"
    ],
    "SelectA": "Amazon CloudWatch Composite Alarms를 이용해 가능한 곳에서 알람을 생성합니다.",
    "SelectA_Commentary": "Composite Alarms를 사용하면 여러 지표 알람을 결합해 CPU와 디스크 사용이 동시에 높을 때만 알람을 보내므로 오탐을 줄이고 즉각적인 대응이 가능합니다.",
    "SelectB": "Amazon CloudWatch Dashboards를 생성해 지표를 시각화하고 신속히 대응합니다.",
    "SelectB_Commentary": "대시보드를 통해 지표를 직관적으로 파악할 수 있지만, 필요한 알람 메커니즘을 줄여주지는 않아 조건 결합이나 오탐 감소 요구 사항을 충족하지 못합니다.",
    "SelectC": "Amazon CloudWatch Synthetics Canaries를 생성해 애플리케이션을 모니터링하고 알람을 발생시킵니다.",
    "SelectC_Commentary": "Synthetics는 주로 애플리케이션 엔드포인트를 모니터링하는 도구로, 인프라 지표인 CPU, IOPS에 대한 결합 알람에는 적합하지 않습니다.",
    "SelectD": "가능한 곳에서 다중 지표 임계값(single metric alarm)으로 Amazon CloudWatch 알람을 생성합니다.",
    "SelectD_Commentary": "단일 메트릭 알람은 특정 지표 한 가지에 대해서만 동작하므로, 여러 지표를 동시에 모니터링하고 조건이 모두 충족될 때만 알람을 발생시키는 요구사항에 부합하지 않습니다."
  },
  {
    "Question_Number": "Q151",
    "Question_Description": "한 회사가 온프레미스 데이터 센터를 AWS로 마이그레이션하려고 합니다. 회사의 컴플라이언스 요구사항에 따라, 오직 ap-northeast-3 Region만 사용할 수 있습니다. 또한 VPC를 인터넷에 연결하는 것은 허용되지 않습니다. 이 요구사항을 충족하는 솔루션은 어떤 것입니까? (두 개를 고르십시오.)",
    "Answer": "A,C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86475-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 특정 Region 사용과 인터넷 연결을 차단해야 하는 보안 정책 설정 요구사항을 다룹니다. 컴플라이언스 상 ap-northeast-3 Region만 사용할 수 있으며, VPC에서 인터넷 연결이 차단되어야 합니다. 따라서 조직 단위의 강력한 제어(예: SCP)를 통해 Region 접근 및 인터넷 액세스를 막는 방안이 핵심입니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1",
      "1.3"
    ],
    "Keywords": [
      "마이그레이션",
      "컴플라이언스",
      "ap-northeast-3 Region",
      "VPC",
      "인터넷 차단",
      "AWS Control Tower",
      "AWS Organizations"
    ],
    "Terms": [
      "AWS Control Tower",
      "AWS Organizations",
      "Service Control Policies (SCPs)",
      "VPC",
      "Internet Access",
      "ap-northeast-3 Region",
      "Network ACL",
      "IAM Policy",
      "AWS WAF",
      "AWS Config"
    ],
    "SelectA": "AWS Control Tower를 사용하여 data residency guardrails를 구현하고, 인터넷 액세스를 거부하며 ap-northeast-3 이외의 모든 AWS Region에 대한 액세스를 거부합니다.",
    "SelectA_Commentary": "AWS Control Tower를 통해 조직 전반에 걸쳐 Region 제한과 인터넷 차단 정책을 쉽게 구성할 수 있어 요구사항을 충족시키는 올바른 솔루션입니다.",
    "SelectB": "AWS WAF에서 인터넷 액세스를 차단하는 규칙을 사용합니다. AWS 계정 설정에서 ap-northeast-3를 제외한 모든 AWS Region에 대한 액세스를 차단합니다.",
    "SelectB_Commentary": "AWS WAF는 주로 웹 트래픽을 제어하며, Region 전체 사용 제한 기능을 직접 제공하지 않으므로 요구사항을 완전히 만족시키지 못합니다.",
    "SelectC": "AWS Organizations에서 SCP를 구성하여 VPC의 인터넷 액세스를 방지합니다. ap-northeast-3 외 모든 AWS Region에 대한 액세스를 거부합니다.",
    "SelectC_Commentary": "조직 단위의 SCP를 통해 특정 Region 외의 사용을 원천적으로 차단하고, VPC가 인터넷에 연결되지 않도록 통제하여 요구사항을 만족시킵니다.",
    "SelectD": "각 VPC에 대하여 Network ACL의 아웃바운드 규칙으로 0.0.0.0/0로부터의 모든 트래픽을 차단합니다. 각 사용자에 대해 ap-northeast-3 이외의 Region 사용을 막는 IAM Policy를 생성합니다.",
    "SelectD_Commentary": "부분적인 NACL 및 IAM 정책 작업이며, 조직 전체 차원의 강제력이 떨어져 관리가 복잡해질 수 있습니다.",
    "SelectE": "AWS Config를 사용하여 internet gateway를 탐지 및 경고하고, ap-northeast-3 밖에서 생성되는 새 리소스에 대해 탐지 및 경고하는 Managed Rules를 활성화합니다.",
    "SelectE_Commentary": "AWS Config 알림은 사후 감지에 가깝고, 근본적으로 Region 또는 인터넷 연결 자체를 막는 직접적인 제어 기능이 부족합니다."
  },
  {
    "Question_Number": "Q152",
    "Question_Description": "한 회사가 신규 직원 교육을 제공하는 3계층 웹 애플리케이션을 운영하고 있습니다. 이 애플리케이션은 하루에 12시간만 접근되며, Amazon RDS for MySQL DB instance를 사용하여 정보를 저장합니다. 회사는 비용을 최소화하고자 합니다. 이러한 요구 사항을 충족하려면 솔루션스 아키텍트는 무엇을 해야 합니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86046-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 사용 시간이 제한된 DB instance를 자동으로 중지하여 비용을 절감하는 방법을 찾는 것입니다. AWS Lambda와 EventBridge를 활용한 DB 시작/중지 스케줄링이 AWS 권장 방식이며 운영 오버헤드가 가장 낮습니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.3"
    ],
    "Keywords": [
      "Amazon RDS for MySQL",
      "비용 절감",
      "시간대별 접근",
      "DB 자동 시작/중지",
      "Lambda",
      "EventBridge"
    ],
    "Terms": [
      "Amazon RDS for MySQL",
      "AWS Systems Manager Session Manager",
      "Amazon ElastiCache for Redis",
      "Amazon EC2",
      "IAM role",
      "AWS Lambda",
      "Amazon EventBridge",
      "cron job"
    ],
    "SelectA": "AWS Systems Manager Session Manager에 대한 IAM 정책을 구성하고, 해당 정책을 가진 IAM role을 생성해 트러스트 관계를 업데이트합니다. 그리고 DB instance에 대한 자동 시작과 중지를 설정합니다.",
    "SelectA_Commentary": "Session Manager만으로는 DB 인스턴스 자동 스케줄링이 적합하지 않습니다. 자동화 기능을 제대로 지원하지 않아 요구사항을 충족하기 어렵습니다.",
    "SelectB": "Amazon ElastiCache for Redis 캐시 클러스터를 생성해 DB instance가 중지되어도 캐시에서 데이터를 제공하도록 합니다. DB instance가 시작되면 캐시를 무효화합니다.",
    "SelectB_Commentary": "캐시를 사용해도 DB instance 비용 절감을 위한 효율적인 자동 중지/시작 기능을 제공하지 못하며 ElastiCache 추가 비용이 발생합니다.",
    "SelectC": "Amazon EC2 인스턴스를 생성하고, Amazon RDS에 액세스 권한을 부여하는 IAM role을 부착합니다. cron job을 구성해 원하는 일정에 EC2 인스턴스를 통해 RDS를 시작 및 중지합니다.",
    "SelectC_Commentary": "EC2 인스턴스를 별도로 운영해야 하므로 오버헤드가 늘어나고, 단순 자동화에 비해 구성과 유지보수가 복잡합니다.",
    "SelectD": "AWS Lambda 함수를 생성해 DB instance를 시작 및 중지하도록 합니다. Amazon EventBridge(또는 Amazon CloudWatch Events)에서 예약 규칙을 생성해 Lambda 함수를 호출하고, 해당 Lambda 함수를 이벤트 대상로 설정합니다.",
    "SelectD_Commentary": "Lambda와 EventBridge 조합은 DB 인스턴스의 자동 시작/중지를 간단하게 스케줄링해 비용 절감을 최적화하는 AWS 모범 사례입니다."
  },
  {
    "Question_Number": "Q153",
    "Question_Description": "한 회사가 인기 있는 노래의 클립으로 만든 벨소리를 판매하고 있습니다. 이 벨소리 파일들은 Amazon S3 Standard에 저장되어 있으며, 각 파일은 최소 128KB 이상의 크기를 갖습니다. 회사에는 수백만 개의 파일이 있지만, 90일이 지난 벨소리들은 다운로드 빈도가 매우 낮습니다. 회사는 가장 자주 사용되는 파일은 사용자에게 즉시 제공하면서도, 스토리지 비용을 절감해야 합니다. 이러한 요구사항을 가장 비용 효율적으로 충족하기 위해 어떤 조치를 취해야 합니까?",
    "Answer": "D",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86933-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 벨소리 파일을 저장할 때 자주 액세스되는 파일은 빠르게 제공하고, 90일 이후에는 다운로드 빈도가 낮아지므로 S3 Standard-Infrequent Access (S3 Standard-IA) 같은 저비용 스토리지로 자동 전환하여 비용을 절감하는 방법에 대한 것입니다. 90일 이전에는 S3 Standard에 두어 자주 액세스되는 파일을 효율적으로 제공하고, 이후에는 S3 Lifecycle Policy를 통해 자동으로 S3 Standard-IA로 이동시켜서 스토리지 비용을 크게 절감할 수 있습니다. 이는 관리 오버헤드를 최소화하고, 자주 사용되는 파일들은 그대로 빠르게 접근 가능하도록 유지하는 데 적합한 솔루션입니다.",
    "Domain": "비용에 최적화된 아키텍처 설계",
    "Tasks": [
      "4.1"
    ],
    "Keywords": [
      "벨소리 파일",
      "스토리지 비용 절감",
      "S3 Standard",
      "S3 Standard-Infrequent Access (S3 Standard-IA)",
      "S3 Intelligent-Tiering",
      "S3 Lifecycle Policy",
      "90일 이후"
    ],
    "Terms": [
      "Amazon S3 Standard",
      "S3 Standard-Infrequent Access (S3 Standard-IA)",
      "S3 Intelligent-Tiering",
      "S3 Lifecycle Policy",
      "S3 inventory"
    ],
    "SelectA": "객체의 초기 스토리지 계층으로 S3 Standard-Infrequent Access(S3 Standard-IA)를 구성합니다.",
    "SelectA_Commentary": "파일이 처음부터 S3 Standard-IA에 있으면 자주 액세스되는 90일 미만 파일에 대한 비용이 더 많이 들 수 있으므로 적절치 않습니다.",
    "SelectB": "파일을 S3 Intelligent-Tiering으로 이동하고, 90일 후에 비용이 더 저렴한 스토리지 계층으로 옮기도록 구성합니다.",
    "SelectB_Commentary": "S3 Intelligent-Tiering 자체도 자동 최적화를 제공하지만, 90일 이후로 액세스가 현저히 낮아지는 패턴이 명확할 경우 S3 Lifecycle Policy가 더 단순하고 비용 면에서도 유리합니다.",
    "SelectC": "S3 inventory를 구성하여 객체를 관리하고, 90일 후에 S3 Standard-Infrequent Access(S3 Standard-IA)로 수동 이동합니다.",
    "SelectC_Commentary": "S3 inventory는 객체 목록을 제공하기 위한 도구이며, 이를 통해 수동으로 객체를 옮기는 것은 관리 작업이 더 많아집니다. 자동화된 Lifecycle Policy가 더 효율적입니다.",
    "SelectD": "S3 Lifecycle 정책을 사용해 90일 후 객체를 S3 Standard에서 S3 Standard-Infrequent Access(S3 Standard-IA)로 이동하도록 합니다.",
    "SelectD_Commentary": "자동으로 90일 이전에는 S3 Standard에 두어 빈번히 사용되는 파일에 빠른 액세스를 제공하고, 90일이 지나면 비용이 저렴한 S3 Standard-IA로 이동시키는 최적의 솔루션입니다."
  },
  {
    "Question_Number": "Q154",
    "Question_Description": "한 회사가 의료 시험 결과를 Amazon S3 저장소에 저장해야 합니다. 이 저장소에는 몇몇 과학자만 새 파일을 추가할 수 있어야 하고, 그 외 모든 사용자는 읽기 전용 권한만 가져야 합니다. 또한 누구도 이미 업로드된 파일을 수정하거나 삭제할 수 없어야 하며, 생성일로부터 최소 1년 동안 모든 파일이 보존되어야 합니다. 이 요구 사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "B",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86359-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 의료 시험 결과를 Amazon S3에 안전하게 보관하면서, 특정 사용자만 파일 추가가 가능하고 파일의 수정·삭제는 절대 허용되지 않도록 설계해야 합니다. 생성된 파일을 1년 이상 보관하려면 S3 Object Lock의 Compliance mode를 사용하면 됩니다. Compliance mode는 어떤 사용자든지 Lock을 무시하거나 파일을 삭제할 수 없게 보장하므로 가장 적절한 솔루션입니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.1",
      "1.3"
    ],
    "Keywords": [
      "Amazon S3",
      "1년 보관",
      "파일 수정 삭제 방지",
      "S3 Object Lock",
      "compliance mode",
      "의료 시험 결과",
      "읽기 전용 접근"
    ],
    "Terms": [
      "S3 Object Lock",
      "Governance mode",
      "Compliance mode",
      "Retention period",
      "Legal hold",
      "IAM role",
      "S3 bucket policy",
      "AWS Lambda"
    ],
    "SelectA": "S3 Object Lock을 governance mode로 설정하고 1년간 legal hold를 적용합니다.",
    "SelectA_Commentary": "Governance mode는 관리자 권한을 통해 해제할 수 있어 완전한 불변성을 보장하지 못합니다. 따라서 수정·삭제를 완전히 방지할 수 없습니다.",
    "SelectB": "S3 Object Lock을 compliance mode로 설정하고 보존 기간을 365일로 지정합니다.",
    "SelectB_Commentary": "Compliance mode에서는 어떤 사용자도 Lock을 해제할 수 없어, 파일 수정·삭제가 불가능하며 1년 보존 요구사항도 충족합니다.",
    "SelectC": "특정 IAM role을 사용해 S3 버킷에서 객체 삭제·변경을 제한하고, S3 버킷 정책으로 오직 이 IAM role만을 허용합니다.",
    "SelectC_Commentary": "IAM role과 버킷 정책만으로는 객체에 대한 절대적 보호(수정·삭제 불가)와 보존 기간 강제 기능을 완전히 보장하기 어렵습니다.",
    "SelectD": "S3 버킷에 객체가 추가될 때마다 AWS Lambda 함수를 호출해서 해시를 추적하고, 변경된 객체를 표시하도록 구성합니다.",
    "SelectD_Commentary": "객체 변경 여부만 표시할 뿐, 실제 수정·삭제를 방지하거나 1년 보존을 강제할 수 없기 때문에 요구 사항을 충족하지 못합니다."
  },
  {
    "Question_Number": "Q155",
    "Question_Description": "한 대형 미디어 회사가 AWS에서 웹 애플리케이션을 호스팅하고 있습니다. 회사는 전 세계 사용자가 기밀 미디어 파일을 안정적으로 액세스할 수 있도록 파일을 캐싱하고자 합니다. 컨텐츠는 Amazon S3 버킷에 저장되어 있으며, 지리적 위치에 상관없이 빠른 속도로 데이터를 제공해야 합니다. 이러한 요구사항을 충족하는 솔루션은 무엇입니까?",
    "Answer": "C",
    "Link": "https://www.examtopics.com/discussions/amazon/view/86795-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "글로벌 사용자에게 기밀 콘텐츠를 빠르게 전송하기 위해서는 네트워크 엣지에서 캐싱하여 지연을 최소화하는 것이 핵심입니다. Amazon CloudFront는 분산된 엣지 서버를 통해 전 세계적 저지연 접근을 보장하므로 적합한 솔루션입니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.4"
    ],
    "Keywords": [
      "기밀 미디어 파일",
      "글로벌 캐싱",
      "전 세계 사용자",
      "빠른 콘텐츠 전송"
    ],
    "Terms": [
      "Amazon S3",
      "AWS DataSync",
      "AWS Global Accelerator",
      "Amazon CloudFront",
      "Amazon Simple Queue Service (Amazon SQS)",
      "CloudFront edge server"
    ],
    "SelectA": "AWS DataSync를 사용하여 S3 버킷을 웹 애플리케이션과 연결합니다.",
    "SelectA_Commentary": "DataSync는 주로 파일 서버 간 대규모 파일 전송을 자동화하는 서비스로, 글로벌 지연을 줄이는 캐싱 솔루션으로는 적합하지 않습니다.",
    "SelectB": "AWS Global Accelerator를 배포하여 S3 버킷을 웹 애플리케이션과 연결합니다.",
    "SelectB_Commentary": "Global Accelerator는 주로 TCP/UDP 트래픽 성능 향상을 위한 서비스이며, 객체 캐싱 기능을 제공하지 않아 미디어 파일 캐시에 부적합합니다.",
    "SelectC": "Amazon CloudFront를 배포하여 S3 버킷을 CloudFront 엣지 서버와 연결합니다.",
    "SelectC_Commentary": "글로벌 엣지에서 파일을 캐싱하여 지연을 최소화하고, 사용자가 어디서 요청하든 빠른 미디어 제공이 가능합니다. 정답입니다.",
    "SelectD": "Amazon Simple Queue Service (Amazon SQS)를 사용하여 S3 버킷을 웹 애플리케이션과 연결합니다.",
    "SelectD_Commentary": "SQS는 메시지 큐 서비스로, 캐싱 기능이나 콘텐츠 전송 가속 기능을 제공하지 않습니다."
  },
  {
    "Question_Number": "Q156",
    "Question_Description": "한 회사는 여러 데이터베이스에서 생성되는 배치 데이터와 네트워크 센서 및 애플리케이션 API에서 오는 실시간 스트림 데이터를 모두 다룹니다. 회사는 이 모든 데이터를 하나의 위치에 통합하여 비즈니스 분석을 수행해야 합니다. 들어오는 데이터를 처리한 후 여러 Amazon S3 버킷에 단계별로 저장해야 하며, 이후 팀이 일회성 쿼리를 실행하고 해당 데이터를 비즈니스 인텔리전스 도구로 가져와 KPI(주요 성과 지표)를 시각화하려고 합니다. 가장 적은 운영 오버헤드로 이러한 요구 사항을 충족하려면 다음 중 어떤 조합을 선택해야 합니까? (두 가지를 고르시오.)",
    "Answer": "A,E",
    "Link": "https://www.examtopics.com/discussions/amazon/view/85770-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 다양한 소스에서 들어오는 배치 및 실시간 데이터를 효율적으로 처리하고 Amazon S3에 저장해 향후 분석에 활용하는 방법을 묻습니다. 정답 선택지들은 최소한의 관리 부담으로 데이터 레이크를 구성하고, 일회성 쿼리와 대시보드 생성을 손쉽게 지원하는 데 초점을 둡니다.",
    "Domain": "고성능 아키텍처 설계",
    "Tasks": [
      "3.1",
      "3.5"
    ],
    "Keywords": [
      "배치 데이터",
      "실시간 스트림 데이터",
      "비즈니스 분석",
      "Amazon S3",
      "일회성 쿼리",
      "비즈니스 인텔리전스(KPI)",
      "운영 오버헤드 최소화"
    ],
    "Terms": [
      "Amazon Athena",
      "Amazon QuickSight",
      "Kinesis Data Analytics",
      "AWS Lambda",
      "Amazon Redshift",
      "AWS Glue",
      "ETL",
      "JSON",
      "Amazon OpenSearch Service (Amazon Elasticsearch Service)",
      "AWS Lake Formation",
      "Crawler",
      "Apache Parquet",
      "Operational Overhead"
    ],
    "SelectA": "Amazon Athena로 일회성 쿼리를 실행하고, Amazon QuickSight로 KPI 대시보드를 생성합니다.",
    "SelectA_Commentary": "서버리스 방식의 Athena로 스키마 없는 쿼리를 수행하고, QuickSight를 통해 시각화할 수 있어 운영 오버헤드를 크게 줄일 수 있으므로 적합합니다.",
    "SelectB": "Amazon Kinesis Data Analytics로 일회성 쿼리를 실행하고, Amazon QuickSight로 KPI 대시보드를 생성합니다.",
    "SelectB_Commentary": "Kinesis Data Analytics는 스트리밍 데이터 실시간 분석에 최적화된 서비스로, 일회성 쿼리에 사용하기에는 오버헤드가 더 높아 맞지 않습니다.",
    "SelectC": "각 데이터베이스의 개별 레코드를 사용자 정의 AWS Lambda 함수로 가져와 Amazon Redshift 클러스터로 전송합니다.",
    "SelectC_Commentary": "Lambda 함수를 직접 작성해 Redshift로 매번 전송하면 관리 및 코드 유지가 복잡해져서 운영 오버헤드가 증가하므로 비효율적입니다.",
    "SelectD": "AWS Glue ETL 작업을 사용하여 데이터를 JSON 형식으로 변환한 뒤, 여러 Amazon OpenSearch Service(Amazon Elasticsearch Service) 클러스터에 로드합니다.",
    "SelectD_Commentary": "OpenSearch Service를 분석 기본 대상으로 사용하는 것은 로그 검색 등 특수 케이스에 적합하며, 여기서는 BI 도구로의 연계와 운영 간소화 측면에서 부적합합니다.",
    "SelectE": "AWS Lake Formation의 블루프린트를 사용해 가져올 수 있는 데이터를 식별하고, AWS Glue를 사용해 소스를 크롤링하여 데이터를 추출한 뒤 Apache Parquet 형식으로 Amazon S3에 적재합니다.",
    "SelectE_Commentary": "Lake Formation과 Glue를 이용해 데이터 레이크를 쉽게 구성하고, Parquet 형식으로 S3에 저장하여 쿼리와 시각화를 위한 기반을 마련하므로 운영 오버헤드가 낮습니다."
  },
  {
    "Question_Number": "Q157",
    "Question_Description": "한 회사가 Amazon Aurora PostgreSQL DB 클러스터에 데이터를 저장하고 있습니다. 이 회사는 모든 데이터를 5년 동안 보관해야 하며, 5년 후에는 모든 데이터를 삭제해야 합니다. 또한 데이터베이스 내에서 수행되는 작업에 대한 감사 로그(audit logs)를 무기한으로 보관해야 합니다. 현재 회사는 Aurora에 대해 자동 백업을 구성해 두었습니다. 이러한 요구사항을 충족하기 위해 솔루션스 아키텍트는 어떤 단계를 결합해서 수행해야 합니까? (2개를 선택하세요.)",
    "Answer": "D,E",
    "Link": "https://www.examtopics.com/discussions/amazon/view/87629-exam-aws-certified-solutions-architect-associate-saa-c03/",
    "AnswerDescription": "이 문제는 5년 동안의 데이터 보존과 만료 시 삭제, 그리고 감사 로그를 무기한 보관해야 하는 요구사항을 동시에 충족하는 방법을 묻습니다. Aurora의 자동 백업 보존 기간은 최대 35일로 제한되므로, 장기 보관을 위해서는 AWS Backup을 사용해야 합니다. 또한 감사 로그는 Amazon CloudWatch Logs로 내보내어 무기한으로 저장할 수 있습니다.",
    "Domain": "보안 아키텍처 설계",
    "Tasks": [
      "1.3"
    ],
    "Keywords": [
      "DB 클러스터 데이터 5년 보관",
      "무기한 감사 로그",
      "Amazon Aurora PostgreSQL",
      "AWS Backup",
      "CloudWatch Logs export"
    ],
    "Terms": [
      "Amazon Aurora PostgreSQL",
      "자동 백업(automated backups)",
      "manual snapshot",
      "AWS Backup",
      "Amazon CloudWatch Logs export",
      "lifecycle policy"
    ],
    "SelectA": "DB cluster에 대한 manual snapshot을 생성합니다.",
    "SelectA_Commentary": "수동 스냅샷은 필요 시점마다 따로 생성해야 하므로, 5년 동안 자동으로 보관·삭제를 관리하기 어렵습니다.",
    "SelectB": "자동 백업에 대해 lifecycle policy를 생성합니다.",
    "SelectB_Commentary": "Aurora 자동 백업의 최대 보존 기간은 35일이므로 5년 보관 요구사항을 충족할 수 없습니다.",
    "SelectC": "자동화된 백업 보존(backup retention)을 5년으로 설정합니다.",
    "SelectC_Commentary": "Aurora 자동 백업은 보존 기간 상한이 35일이므로 5년 보관 설정은 불가능합니다.",
    "SelectD": "DB cluster에 대한 Amazon CloudWatch Logs export를 구성합니다.",
    "SelectD_Commentary": "CloudWatch Logs를 통해 감사 로그를 무기한 보관할 수 있으므로 로그 보관 요구사항을 충족합니다.",
    "SelectE": "AWS Backup을 사용하여 백업을 수행하고, 5년 동안 해당 백업을 보관합니다.",
    "SelectE_Commentary": "AWS Backup을 이용하면 백업을 장기간(5년) 자동으로 보관하고, 만료 시 자동 삭제도 지원합니다."
  }
]